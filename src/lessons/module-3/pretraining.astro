---
// Module 3, Lesson 3.1: Pretraining - Building the Foundation
import KeyTakeaway from "../../components/KeyTakeaway.astro";
import PaperReference from "../../components/PaperReference.astro";
import MathBlock from "../../components/MathBlock.astro";
import Diagram from "../../components/Diagram.astro";
import Quiz from "../../components/Quiz.astro";
import GlossaryTooltip from "../../components/GlossaryTooltip.astro";
---

<h2>Learning Objectives</h2>
<p>After completing this lesson, you will be able to:</p>
<ul>
  <li>
    Understand the pretraining data pipeline: sourcing,
    filtering, deduplication, and tokenization
  </li>
  <li>
    Derive the next-token prediction objective and explain
    why it produces general-purpose representations
  </li>
  <li>
    Explain distributed training strategies: data
    parallelism, tensor parallelism, and pipeline
    parallelism
  </li>
  <li>
    Understand mixed-precision training and why it enables
    efficient large-scale training
  </li>
</ul>

<h2>The Pretraining Data Pipeline</h2>

<p>
  Pretraining is where an <GlossaryTooltip term="LLM" /> acquires its broad knowledge
  of language, facts, and reasoning patterns. Think of it as the foundation
  of a building: everything the model will later do (follow instructions,
  write code, answer questions) rests on what it absorbs here. The quality
  and scale of pretraining data fundamentally determine what
  the model can learn.
</p>

<h3>Data Sources</h3>
<p>
  You might be surprised by just how much text goes into training a modern
  LLM. We're talking trillions of tokens from diverse
  sources:
</p>
<ul>
  <li>
    <strong>Web crawls</strong>: Common Crawl provides
    petabytes of raw HTML. After filtering, this yields
    hundreds of billions of tokens. Projects like RefinedWeb
    and FineWeb apply aggressive quality filtering to Common
    Crawl data.
  </li>
  <li>
    <strong>Books and literature</strong>: Books3, Project
    Gutenberg, and curated book corpora provide long-form,
    high-quality text with coherent structure.
  </li>
  <li>
    <strong>Code</strong>: GitHub repositories, Stack
    Overflow. Code data significantly improves reasoning and
    structured thinking, even for non-code tasks.
  </li>
  <li>
    <strong>Scientific papers</strong>: ArXiv, PubMed,
    Semantic Scholar. Domain-specific technical knowledge.
  </li>
  <li>
    <strong>Curated datasets</strong>: Wikipedia,
    StackExchange, high-quality forums. These are heavily
    upweighted despite their small size relative to web
    data.
  </li>
</ul>

<h3>Data Quality Filtering</h3>
<p>
  Here's the thing: raw web text is incredibly noisy. Think spam, boilerplate,
  duplicated pages, and plenty of content you wouldn't want a model learning from.
  Filtering is critical and typically involves multiple stages:
</p>
<ul>
  <li>
    <strong>Heuristic filters</strong>: Remove pages with
    too many special characters, extreme length, low word
    count, or high perplexity under a reference language
    model.
  </li>
  <li>
    <strong>Deduplication</strong>: Near-duplicate removal
    using MinHash or exact n-gram matching. Deduplication
    reduces memorization and improves generalization.
    Studies show that deduplicating training data can reduce
    the required dataset size by 30-50% with no loss in
    quality.
  </li>
  <li>
    <strong>Toxicity filtering</strong>: Classifier-based
    removal of harmful, biased, or low-quality content.
  </li>
  <li>
    <strong>Language identification</strong>: FastText
    classifiers separate languages for appropriate mixing
    ratios.
  </li>
</ul>

<h3>Data Mixing</h3>
<p>
  Not all data sources are equal, and this is where things get interesting.
  Training mixes are carefully tuned, like a recipe where the proportions matter
  as much as the ingredients:
</p>
<ul>
  <li>
    High-quality sources (Wikipedia, books, code) are
    upsampled despite being a small fraction of total data.
  </li>
  <li>
    LLaMA trained on roughly: 67% web, 4.5% Wikipedia, 4.5%
    books, 6.5% code (GitHub), and various other sources.
  </li>
  <li>
    The optimal mix is an active research area. Models like
    <GlossaryTooltip term="GPT" />-5.2 and Claude use proprietary data mixtures that are
    closely guarded.
  </li>
</ul>

<h3>Tokenization</h3>
<p>
  Before a model can process text, it needs to break it into pieces. But how do
  you split text into units? Whole words? Individual characters? Modern LLMs use
  a clever middle ground called <strong>subword tokenization</strong>, which splits
  text into chunks that balance vocabulary size against sequence length:
</p>
<ul>
  <li>
    <strong>Byte Pair Encoding (<GlossaryTooltip
      term="BPE"
    />)</strong>: Iteratively merges the most frequent byte
    pairs. Used by <GlossaryTooltip term="GPT" /> models.
    Vocabulary sizes typically range from 32K to 100K+
    tokens.
  </li>
  <li>
    <strong>SentencePiece</strong>: Language-agnostic
    tokenizer that works directly on raw text (no
    pre-tokenization). Used by LLaMA, <GlossaryTooltip term="T5" />.
  </li>
  <li>
    <strong>Tokenizer-free approaches</strong>: Byte-level
    models operate directly on raw bytes, eliminating
    tokenization artifacts but requiring longer sequences.
  </li>
</ul>

<p>
  <strong>Tokenization matters</strong>: The choice of
  tokenizer affects the effective context length,
  multilingual performance, and computational cost. A
  tokenizer trained primarily on English text may represent
  non-English languages with 2-3x more tokens per word,
  effectively reducing context length for those languages.
</p>

<h2>The Pretraining Objective: Next-Token Prediction</h2>

<p>
  The core training objective is deceptively simple: given a
  sequence of tokens, predict the next one. That's it. No labeled data,
  no human annotations. Just billions of pages of text and one question
  over and over: "What word comes next?"
</p>

<p>
  More formally, given a sequence <MathBlock
    formula={"x_1, x_2, \\ldots, x_T"}
  />, the model learns to maximize the probability of each
  token given all preceding tokens. The loss function captures
  how surprised the model is by the actual next token:
</p>

<MathBlock
  formula={"\\mathcal{L}(\\theta) = -\\sum_{t=1}^{T} \\log P_\\theta(x_t \\mid x_1, \\ldots, x_{t-1})"}
  display={true}
/>

<p>
  Read this as: for each position in the sequence, measure how well the model
  predicted the actual next token (using log-probability), then sum up all those
  scores. The negative sign means a higher probability (better prediction) produces
  a lower loss. In other words, the model is penalized for being surprised,
  so it learns to predict as accurately as possible.
</p>

<p>
  This is called <strong>causal language modeling</strong> (also known as
  autoregressive modeling). The key trick: during training, the model processes
  the entire sequence in one forward pass using a causal attention mask that
  prevents each position from peeking at future tokens. It's like covering up
  the rest of a sentence and asking "what comes next?" at every position
  simultaneously. This enables efficient parallel training despite the
  sequential nature of generation.
</p>

<h3>Why Next-Token Prediction Works So Well</h3>

<p>
  You might wonder: how can something so simple produce such capable models?
  The answer is that predicting the next token in internet text requires an
  incredibly broad set of capabilities:
</p>
<ul>
  <li>
    <strong>Syntax and grammar</strong>: To predict the next
    word, the model must learn linguistic structure.
  </li>
  <li>
    <strong>Semantics</strong>: Predicting the right word
    requires understanding meaning and context.
  </li>
  <li>
    <strong>World knowledge</strong>: Completing factual
    text requires knowing facts about the world.
  </li>
  <li>
    <strong>Reasoning</strong>: Predicting the next step in
    a mathematical proof or logical argument requires
    reasoning.
  </li>
  <li>
    <strong>Style and pragmatics</strong>: The model must
    learn to match tone, register, and communicative intent.
  </li>
</ul>

<p>
  As Ilya Sutskever famously argued: prediction is
  compression, and compression requires understanding. A
  model that achieves very low perplexity on internet text
  has necessarily learned a rich world model.
</p>

<h3>Cross-Entropy Loss and Perplexity</h3>

<p>
  The training loss is the cross-entropy between the model's
  predicted distribution and the true next token. But raw loss numbers
  are hard to interpret. That's where <strong>perplexity</strong> comes in.
  It converts the loss into something more intuitive by exponentiating the
  average cross-entropy:
</p>

<MathBlock
  formula={"\\text{PPL} = \\exp\\left(-\\frac{1}{T}\\sum_{t=1}^{T} \\log P_\\theta(x_t \\mid x_{<t})\\right)"}
  display={true}
/>

<p>
  In plain English: perplexity tells you how many equally likely choices the
  model is effectively picking from at each step. If the perplexity is 10,
  it's as if the model is choosing between 10 equally likely options every time
  it predicts the next token.
</p>

<p>
  Lower is better. A perplexity of 1 would mean perfect prediction (the model
  always knows exactly what comes next). State-of-the-art LLMs achieve
  perplexities around 5-10 on typical web text, which is remarkably good
  considering the diversity of language.
</p>

<Diagram
  diagramId="pretraining-pipeline"
  title="Pretraining Pipeline Overview"
  autoplay={true}
  animationDuration={5000}
>
  <div
    class="bg-[hsl(var(--card))] p-4 rounded w-full"
  >
    <div class="flex flex-col gap-4 items-center">
      <!-- Stage 1: Data -->
      <div
        class="flex flex-wrap gap-3 items-center w-full justify-center"
        data-animate
        style="animation-delay: 0.3s"
      >
        <div
          class="px-3 py-2 bg-[hsl(var(--diagram-blue-bg))] rounded text-xs text-center font-medium"
        >
          Web Crawl
        </div>
        <div
          class="px-3 py-2 bg-[hsl(var(--diagram-blue-bg))] rounded text-xs text-center font-medium"
        >
          Books
        </div>
        <div
          class="px-3 py-2 bg-[hsl(var(--diagram-blue-bg))] rounded text-xs text-center font-medium"
        >
          Code
        </div>
        <div
          class="px-3 py-2 bg-[hsl(var(--diagram-blue-bg))] rounded text-xs text-center font-medium"
        >
          Papers
        </div>
      </div>
      <div
        class="text-[hsl(var(--muted-foreground))]"
        data-animate
        style="animation-delay: 0.8s"
      >
        &#8595;
      </div>

      <!-- Stage 2: Filtering -->
      <div
        class="px-4 py-2 bg-[hsl(var(--diagram-amber-bg))] rounded text-xs text-center font-medium w-full sm:w-64"
        data-animate
        style="animation-delay: 1.2s"
      >
        Filter + Deduplicate + Tokenize
      </div>
      <div
        class="text-[hsl(var(--muted-foreground))]"
        data-animate
        style="animation-delay: 1.6s"
      >
        &#8595;
      </div>

      <!-- Stage 3: Training -->
      <div
        class="px-4 py-2 bg-[hsl(var(--diagram-indigo-bg))] rounded text-xs text-center font-medium w-full sm:w-64"
        data-animate
        style="animation-delay: 2.0s"
      >
        Next-Token Prediction on Trillions of Tokens
      </div>
      <div
        class="text-[hsl(var(--muted-foreground))]"
        data-animate
        style="animation-delay: 2.4s"
      >
        &#8595;
      </div>

      <!-- Stage 4: Output -->
      <div
        class="px-4 py-3 bg-[hsl(var(--diagram-emerald-bg))] rounded text-sm text-center font-semibold w-full sm:w-64"
        data-animate
        style="animation-delay: 3.0s"
      >
        Pretrained Base Model
      </div>
    </div>

    <div
      class="mt-4 text-sm text-[hsl(var(--muted-foreground))] text-center"
      data-animate
      style="animation-delay: 4s"
    >
      Base models are powerful text completors but not yet
      aligned for instruction-following
    </div>
  </div>
</Diagram>

<h2>Distributed Training</h2>

<p>
  So far we've talked about <em>what</em> to train on and <em>what</em> the
  objective is. But there's a practical challenge: modern LLMs are enormous.
  Training a frontier model requires thousands of GPUs working
  in concert. A single GPU simply cannot hold the model parameters,
  optimizer states, gradients, and activations all at once. Distributed
  training strategies solve this by partitioning the work across many devices.
</p>

<h3>Data Parallelism (DP)</h3>

<p>
  Let's start with the simplest strategy. The idea: put a complete copy of the
  model on each GPU, give each GPU a different slice of the training batch, and
  average the gradients at the end. It's like giving each worker the same recipe
  but different ingredients to test.
</p>
<ul>
  <li>Each GPU processes a different mini-batch shard.</li>
  <li>
    After the backward pass, gradients are synchronized via <strong
      >all-reduce</strong
    > (each GPU gets the averaged gradient).
  </li>
  <li>
    <strong>Limitation</strong>: Every GPU must hold the
    full model. A 70B parameter model with Adam optimizer
    states requires ~280GB of memory per replica (parameters
    in FP16 + optimizer states in FP32), exceeding
    single-GPU capacity.
  </li>
</ul>

<h3>ZeRO: Zero Redundancy Optimizer</h3>

<p>
  Data parallelism's memory problem seems hard to avoid, but ZeRO (from DeepSpeed)
  offers an elegant solution. The core insight: why should every GPU store a copy
  of <em>everything</em>? Instead, ZeRO partitions optimizer states, gradients,
  and parameters across GPUs to eliminate redundancy:
</p>
<ul>
  <li>
    <strong>ZeRO Stage 1</strong>: Partition optimizer
    states. Reduces memory by ~4x.
  </li>
  <li>
    <strong>ZeRO Stage 2</strong>: Partition gradients as
    well. Additional ~2x reduction.
  </li>
  <li>
    <strong>ZeRO Stage 3</strong>: Partition parameters too.
    Each GPU holds only a shard of the full model.
    Parameters are gathered on-demand during
    forward/backward passes.
  </li>
</ul>

<p>
  ZeRO enables data-parallel training of much larger models
  by eliminating the requirement that each GPU hold a full
  copy of everything.
</p>

<div
  class="bg-[hsl(var(--diagram-indigo-bg))] p-4 rounded-lg my-4"
>
  <p
    class="font-semibold text-[hsl(var(--diagram-indigo-fg))]"
  >
    The "Pizza Party" Analogy
  </p>
  <p class="text-sm">
    Imagine a group of chefs (GPUs) trying to bake a massive
    pizza (train a model).
  </p>
  <ul class="list-disc list-inside text-sm mt-2 ml-2">
    <li>
      <strong>Standard Data Parallelism</strong>: Every chef
      must memorize the <em>entire</em> 1,000-page recipe book.
      This is redundant and wastes brainpower (memory).
    </li>
    <li>
      <strong>ZeRO</strong>: We split the recipe book. Chef
      A holds pages 1-100, Chef B holds 101-200, etc. When
      Chef A needs to check step 150, they ask Chef B for
      that page, use it, and return it.
    </li>
  </ul>
  <p class="text-sm mt-2">
    By sharing the recipe (parameters/optimizer states)
    instead of duplicating it, we can bake much bigger
    pizzas.
  </p>
</div>

<h3>Tensor Parallelism (TP)</h3>

<p>
  While data parallelism splits the <em>data</em>, tensor parallelism splits
  the <em>model's weight matrices themselves</em> across GPUs. Consider a linear
  layer <MathBlock formula="Y = XW" />. Instead of one GPU holding the entire
  weight matrix <MathBlock formula="W" />, we slice it column-wise across N GPUs:
</p>

<MathBlock
  formula={"W = [W_1 | W_2 | \\ldots | W_N]"}
  display={true}
/>

<p>
  Think of it this way: if you had a huge multiplication table and four friends,
  you could give each friend one quarter of the columns. Each friend computes their
  portion, and then you combine the results. That's exactly what happens here: each
  GPU computes <MathBlock formula="Y_i = XW_i" />, then
  the partial results are combined via all-reduce. This is commonly
  applied to the attention and FFN layers.
</p>

<p>
  <strong>Trade-off</strong>: Tensor parallelism requires
  high-bandwidth interconnect (NVLink) between GPUs because
  communication happens within every layer. Best used within
  a single node (8 GPUs).
</p>

<h3>Pipeline Parallelism (PP)</h3>

<p>
  The third strategy takes yet another approach: instead of splitting data or
  weight matrices, you split the model <em>vertically</em> by layers. GPU 0 runs
  layers 1-10, GPU 1 runs layers 11-20, and so on, like an assembly line where
  each station handles one stage of production.
</p>
<ul>
  <li>
    <strong>Naive approach</strong>: Only one GPU active at
    a time (the "bubble" problem).
  </li>
  <li>
    <strong>Micro-batching</strong> (GPipe): Split mini-batch
    into micro-batches, pipeline them through stages. Reduces
    the bubble but doesn't eliminate it.
  </li>
  <li>
    <strong>1F1B scheduling</strong>: Interleave forward and
    backward passes of different micro-batches to minimize
    pipeline bubbles.
  </li>
</ul>

<p>
  Pipeline parallelism works well across nodes since
  inter-layer communication (passing activations) requires
  less bandwidth than intra-layer communication (tensor
  parallelism).
</p>

<h3>3D Parallelism in Practice</h3>

<p>
  In practice, frontier models don't pick just one strategy. They combine all three,
  which is why this is called <strong>3D parallelism</strong>: TP within a node
  (where GPUs have fast interconnects), PP across nodes, and DP (with ZeRO) across
  replicas. For example, training a 175B model on 1024 GPUs might use:
</p>
<ul>
  <li>TP = 8 (within each 8-GPU node)</li>
  <li>PP = 16 (across 16 pipeline stages)</li>
  <li>DP = 8 (8 data-parallel replicas)</li>
  <li>Total: 8 x 16 x 8 = 1024 GPUs</li>
</ul>

<h2>Mixed-Precision Training</h2>

<p>
  Now let's talk about a different kind of efficiency. Every number in a neural
  network (weights, gradients, activations) takes up memory and compute time.
  Training in full FP32 precision (32 bits per number) works, but it's wasteful.
  Modern training uses <strong>mixed precision</strong>: FP16 or BF16 for most
  operations, FP32 only where it really matters.
</p>

<h3>FP16 vs BF16</h3>
<p>
  Both FP16 and BF16 use 16 bits, but they allocate those bits differently.
  This trade-off matters more than you might expect:
</p>
<ul>
  <li>
    <strong>FP16</strong> (half precision): 1 sign, 5 exponent,
    10 mantissa bits. Range: Â±65,504. High precision but limited
    range, requiring loss scaling to prevent underflow.
  </li>
  <li>
    <strong>BF16</strong> (bfloat16): 1 sign, 8 exponent, 7 mantissa
    bits. Same range as FP32 but lower precision. No loss scaling
    needed. Preferred for LLM training.
  </li>
</ul>

<h3>The Mixed-Precision Recipe</h3>

<p>
  So how do you actually use these reduced-precision formats without sacrificing
  model quality? The key insight: forward and backward passes can use
  BF16, but certain operations need FP32 for numerical stability.
  Here's the recipe:
</p>
<ul>
  <li>
    <strong>Master weights in FP32</strong>: Maintain a
    full-precision copy of parameters. Small gradient
    updates in FP16 would be rounded to zero (gradient
    underflow).
  </li>
  <li>
    <strong>Forward/backward in BF16</strong>: Activations
    and gradients computed in reduced precision. This halves
    memory for activations and doubles throughput on modern
    GPUs (Tensor Cores).
  </li>
  <li>
    <strong>Accumulations in FP32</strong>: Reduction
    operations (summing gradients, softmax denominators) use
    FP32 to prevent numerical errors from accumulating.
  </li>
</ul>

<p>
  The weight update rule captures how these precisions interact. The model
  keeps a "master copy" of the weights in full precision:
</p>

<MathBlock
  formula={"\\theta_{t+1}^{\\text{FP32}} = \\theta_t^{\\text{FP32}} - \\eta \\cdot g_t^{\\text{BF16} \\to \\text{FP32}}"}
  display={true}
/>

<p>
  Read this as: take the current FP32 weights, subtract the learning rate times
  the gradient (which was computed in BF16 and then cast back to FP32). The
  crucial point is that the master weights live in FP32, so tiny gradient
  updates that would vanish in half-precision are properly accumulated over time.
</p>

<p>
  <strong>Impact</strong>: Mixed precision reduces memory
  usage by ~50% and increases training throughput by 2-3x on
  modern hardware, with negligible impact on model quality.
</p>

<h3>Activation Checkpointing</h3>

<p>
  There's one more memory-saving trick worth knowing about. During the forward
  pass, the model has to store all intermediate activations (the outputs of
  every layer) because they're needed during the backward pass to compute
  gradients. For large models, this dominates memory usage.
  <strong>Activation checkpointing</strong> (also called gradient checkpointing)
  offers a clever trade-off, spending a bit more compute to save a lot of memory:
</p>
<ul>
  <li>
    Only store activations at checkpoint boundaries (e.g.,
    every N layers).
  </li>
  <li>
    During backward pass, recompute intermediate activations
    from the nearest checkpoint.
  </li>
  <li>
    Reduces activation memory from O(L) to O(sqrt(L)) at the
    cost of ~33% extra compute.
  </li>
</ul>

<Quiz
  question="Why is BF16 generally preferred over FP16 for LLM pretraining?"
  quizId="bf16-vs-fp16"
  options={[
    {
      id: "a",
      text: "BF16 has higher precision (more mantissa bits) than FP16",
      correct: false,
      explanation:
        "Actually, BF16 has fewer mantissa bits (7 vs 10) than FP16, giving it lower precision.",
    },
    {
      id: "b",
      text: "BF16 has the same dynamic range as FP32, eliminating the need for loss scaling",
      correct: true,
      explanation:
        "Correct! BF16 has 8 exponent bits (same as FP32), giving it a much larger dynamic range than FP16. This means gradients rarely underflow, eliminating the need for the loss scaling workaround required by FP16.",
    },
    {
      id: "c",
      text: "BF16 uses less memory than FP16",
      correct: false,
      explanation:
        "Both BF16 and FP16 use 16 bits (2 bytes) per value. They use the same amount of memory.",
    },
    {
      id: "d",
      text: "BF16 is faster to compute than FP16 on all hardware",
      correct: false,
      explanation:
        "Performance depends on hardware support. Both formats run at similar speed on modern GPUs with Tensor Core support.",
    },
  ]}
/>

<KeyTakeaway>
  <ul>
    <li>
      <strong>Pretraining data quality</strong> is paramount:
      careful filtering, deduplication, and mixing of diverse
      sources (web, books, code, papers) determines model capabilities
    </li>
    <li>
      <strong>Next-token prediction</strong> is a deceptively
      powerful objective: predicting internet text requires learning
      syntax, semantics, world knowledge, and reasoning
    </li>
    <li>
      <strong>Distributed training</strong> combines data parallelism
      (split batches), tensor parallelism (split weight matrices),
      and pipeline parallelism (split layers) to scale across
      thousands of GPUs
    </li>
    <li>
      <strong>Mixed precision</strong> (BF16 forward/backward,
      FP32 master weights) cuts memory by ~50% and doubles throughput
      with minimal quality loss
    </li>
    <li>
      <strong>Activation checkpointing</strong> trades ~33% extra
      compute for dramatically reduced memory, enabling training
      of larger models
    </li>
  </ul>
</KeyTakeaway>

<h2>References</h2>

<PaperReference
  title="Language Models are Few-Shot Learners (GPT-3)"
  authors="Brown et al."
  year="2020"
  url="https://arxiv.org/abs/2005.14165"
  type="paper"
/>

<PaperReference
  title="LLaMA: Open and Efficient Foundation Language Models"
  authors="Touvron et al."
  year="2023"
  url="https://arxiv.org/abs/2302.13971"
  type="paper"
/>

<PaperReference
  title="ZeRO: Memory Optimizations Toward Training Trillion Parameter Models"
  authors="Rajbhandari et al."
  year="2020"
  url="https://arxiv.org/abs/1910.02054"
  type="paper"
/>

<PaperReference
  title="Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism"
  authors="Shoeybi et al."
  year="2020"
  url="https://arxiv.org/abs/1909.08053"
  type="paper"
/>

<PaperReference
  title="Mixed Precision Training"
  authors="Micikevicius et al."
  year="2018"
  url="https://arxiv.org/abs/1710.03740"
  type="paper"
/>

<PaperReference
  title="Scaling Data-Constrained Language Models"
  authors="Muennighoff et al."
  year="2023"
  url="https://arxiv.org/abs/2305.16264"
  type="paper"
/>
