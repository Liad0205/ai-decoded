---
// Module 3, Lesson 3.3: RLHF and Alignment - Teaching Models Human Values
import KeyTakeaway from "../../components/KeyTakeaway.astro";
import PaperReference from "../../components/PaperReference.astro";
import MathBlock from "../../components/MathBlock.astro";
import Diagram from "../../components/Diagram.astro";
import Quiz from "../../components/Quiz.astro";
import GlossaryTooltip from "../../components/GlossaryTooltip.astro";
---

<h2>Learning Objectives</h2>
<p>After completing this lesson, you will be able to:</p>
<ul>
  <li>
    Understand why <GlossaryTooltip term="SFT" /> alone is insufficient
    and how <GlossaryTooltip term="RLHF" />
    addresses its limitations
  </li>
  <li>
    Explain the full RLHF pipeline:
    reward modeling, <GlossaryTooltip term="PPO" />
    optimization, and KL constraints
  </li>
  <li>
    Derive and compare Direct Preference Optimization (<GlossaryTooltip
      term="DPO"
    />) as a simpler alternative to PPO
  </li>
  <li>
    Identify failure modes: reward hacking, mode collapse,
    and alignment tax
  </li>
</ul>

<h2>
  Why RLHF? The Limits of
  Supervised Learning
</h2>

<p>
  In the previous lesson, you saw how <GlossaryTooltip term="SFT" /> teaches a model to imitate human
  demonstrations. But here's the thing: imitation has fundamental limitations.
</p>
<ul>
  <li>
    <strong>Demonstration ≠ preference</strong>: It's easier
    for humans to compare two responses than to write a
    perfect one. A labeler might write a mediocre response
    but easily identify which of two responses is better.
  </li>
  <li>
    <strong>Ceiling effect</strong>: SFT can only be as good
    as the demonstrations. The model can't surpass the
    quality of its training data. <GlossaryTooltip
      term="RLHF"
    /> can, by optimizing against a learned preference signal.
  </li>
  <li>
    <strong>Subtle quality dimensions</strong>: Helpfulness,
    harmlessness, and honesty are hard to demonstrate
    exhaustively but easier to evaluate comparatively.
  </li>
</ul>

<p>
  <GlossaryTooltip term="RLHF" /> (Reinforcement Learning from
  Human Feedback) addresses these limitations by flipping the script: instead
  of asking "what would a human write?", it asks "which response would a human
  <em>prefer</em>?" This turns out to be a much more powerful training signal.
</p>

<h2>The RLHF Pipeline</h2>

<p>
  The RLHF pipeline has three stages, each building on the previous one. Think of it as a
  three-step recipe: first you teach the model the basics (SFT), then you build a "taste tester"
  that can judge quality (reward model), and finally you let the model practice and improve
  using that taste tester's feedback (RL optimization).
</p>

<Diagram
  diagramId="rlhf-pipeline"
  title="Three Stages of RLHF"
  autoplay={true}
  animationDuration={5000}
>
  <div
    class="bg-[hsl(var(--card))] p-4 rounded w-full"
  >
    <div class="grid grid-cols-1 sm:grid-cols-3 gap-4">
      <!-- Stage 1 -->
      <div
        class="text-center p-3 rounded-lg border-2 border-[hsl(var(--diagram-blue-border))] bg-[hsl(var(--diagram-blue-bg))]"
        data-animate
        style="animation-delay: 0.3s"
      >
        <div class="font-bold text-[hsl(var(--diagram-blue-fg))] text-sm mb-2">
          Stage 1: SFT
        </div>
        <div class="text-xs text-[hsl(var(--diagram-blue-fg))]">
          <div>Base model</div>
          <div class="my-1">&#8595;</div>
          <div>Fine-tune on demonstrations</div>
          <div class="my-1">&#8595;</div>
          <div class="font-semibold">
            SFT model &#960;<sub>SFT</sub>
          </div>
        </div>
      </div>

      <!-- Stage 2 -->
      <div
        class="text-center p-3 rounded-lg border-2 border-[hsl(var(--diagram-amber-border))] bg-[hsl(var(--diagram-amber-bg))]"
        data-animate
        style="animation-delay: 1.5s"
      >
        <div class="font-bold text-[hsl(var(--diagram-amber-fg))] text-sm mb-2">
          Stage 2: Reward Model
        </div>
        <div class="text-xs text-[hsl(var(--diagram-amber-fg))]">
          <div>Collect comparison data</div>
          <div class="my-1">&#8595;</div>
          <div>Train reward model R(x, y)</div>
          <div class="my-1">&#8595;</div>
          <div class="font-semibold">
            Reward model R<sub>&#966;</sub>
          </div>
        </div>
      </div>

      <!-- Stage 3 -->
      <div
        class="text-center p-3 rounded-lg border-2 border-[hsl(var(--diagram-emerald-border))] bg-[hsl(var(--diagram-emerald-bg))]"
        data-animate
        style="animation-delay: 3.0s"
      >
        <div
          class="font-bold text-[hsl(var(--diagram-emerald-fg))] text-sm mb-2"
        >
          Stage 3: RL (PPO)
        </div>
        <div class="text-xs text-[hsl(var(--diagram-emerald-fg))]">
          <div>Optimize policy &#960;<sub>&#952;</sub></div>
          <div class="my-1">&#8595;</div>
          <div>
            Maximize R while staying near &#960;<sub
              >SFT</sub
            >
          </div>
          <div class="my-1">&#8595;</div>
          <div class="font-semibold">
            Aligned model &#960;<sub>RLHF</sub>
          </div>
        </div>
      </div>
    </div>
  </div>
</Diagram>

<h3>Stage 1: Supervised Fine-Tuning</h3>

<p>
  Start with a pretrained model and fine-tune on
  demonstration data (covered in Lesson 3.2). This produces
  the SFT model <MathBlock
    formula={"\\pi_{\\text{SFT}}"}
  />, which serves as the starting point and reference
  policy for RLHF.
</p>

<h3>Stage 2: Reward Modeling</h3>

<p>
  This is where things get interesting. You need a way to automatically score responses
  based on human preferences, so you don't need a human in the loop for every single
  training step. The solution: train a <strong>reward model</strong> that learns to predict
  which responses humans would prefer. Here's how it works:
</p>
<ol>
  <li>
    Sample prompts from a distribution of user inputs.
  </li>
  <li>
    Generate 2+ responses per prompt from the <GlossaryTooltip
      term="SFT"
    /> model.
  </li>
  <li>
    Human annotators rank the responses (e.g., response A >
    response B).
  </li>
  <li>Train a reward model on these preferences.</li>
</ol>

<h4>The Bradley-Terry Model</h4>

<p>
  You might wonder: how do you turn pairwise comparisons ("I prefer A over B") into
  a trainable objective? The answer is the <strong>Bradley-Terry model</strong>, a classic
  probabilistic framework. Given a prompt x and two responses, <MathBlock
    formula="y_w"
  /> (preferred, the "winner") and <MathBlock formula="y_l" /> (dispreferred, the "loser"),
  the reward model <MathBlock formula={"R_\\phi"} /> is trained
  to assign higher reward to the preferred response. The core
  idea is that the probability of preferring one response over
  another depends only on the <em>difference</em> in their reward
  scores, passed through a sigmoid to produce a probability:
</p>

<div class="equation-with-label">
  <p class="equation-label">
    Bradley-Terry preference model:
  </p>
  <MathBlock
    formula={"P(y_w \\succ y_l \\mid x) = \\sigma(R_\\phi(x, y_w) - R_\\phi(x, y_l))"}
    display={true}
  />
</div>

<p>
  Read this as: "the probability that response <MathBlock formula="y_w" /> beats
  response <MathBlock formula="y_l" /> equals the sigmoid of their reward difference."
  Here <MathBlock formula={"\\sigma"} /> is the sigmoid function, which squashes any
  number into a probability between 0 and 1. The larger the reward gap between
  two responses, the more confidently the model predicts the higher-scored one
  is preferred.
</p>

<p>
  <strong>Concrete example</strong>: If the reward model
  scores response A at 2.0 and response B at 1.0, the
  probability of preferring A is <MathBlock
    formula={"\\sigma(2.0 - 1.0) = \\sigma(1.0) \\approx 0.73"}
  />, or 73%. If the gap widens to 3.0, the preference
  probability rises to <MathBlock
    formula={"\\sigma(3.0) \\approx 0.95"}
  />, or 95%. This sigmoid mapping ensures small reward
  differences produce uncertain predictions while large
  differences produce confident ones.
</p>

<p>
  Now that you know how to model a single preference, you need a training objective
  that works across the entire dataset. The goal: maximize the log-likelihood of all
  observed preferences:
</p>

<div class="equation-with-label">
  <p class="equation-label">Reward model loss function:</p>
  <MathBlock
    formula={"\\mathcal{L}_{\\text{RM}}(\\phi) = -\\mathbb{E}_{(x, y_w, y_l)}\\left[\\log \\sigma(R_\\phi(x, y_w) - R_\\phi(x, y_l))\\right]"}
    display={true}
  />
</div>

<p>
  In words: this loss pushes the reward model to widen the
  gap between preferred and dispreferred response scores,
  penalizing cases where the model assigns a higher (or
  equal) reward to the dispreferred response.
</p>

<p>
  <strong>Architecture</strong>: The reward model is
  typically initialized from the <GlossaryTooltip
    term="SFT"
  /> model, with the language model head replaced by a scalar
  output head. This gives it strong language understanding from
  the start.
</p>

<h4>Reward Model Quality</h4>
<ul>
  <li>
    <strong>Inter-annotator agreement</strong>: Humans agree
    on preferences only ~70-80% of the time. This sets an
    upper bound on reward model accuracy.
  </li>
  <li>
    <strong>Scaling</strong>: InstructGPT collected ~33K
    comparisons. More data improves reward model accuracy,
    but with diminishing returns.
  </li>
  <li>
    <strong>Calibration</strong>: The absolute value of
    rewards is arbitrary; only relative differences matter.
    Reward models are often normalized to have zero mean and
    unit variance.
  </li>
</ul>

<h3>Stage 3: RL Optimization with PPO</h3>

<p>
  With a trained reward model in hand, you can now optimize the language
  model policy <MathBlock formula={"\\pi_\\theta"} /> to maximize
  expected reward. But there's a catch: if you simply told the model "maximize
  your reward score," it would quickly learn to game the reward model rather
  than genuinely improving. So you add a leash: a KL divergence penalty that
  keeps the policy close to the <GlossaryTooltip
    term="SFT"
  /> policy. The full objective balances these two forces:
</p>

<MathBlock
  formula={"\\max_{\\pi_\\theta} \\mathbb{E}_{x \\sim \\mathcal{D},\\, y \\sim \\pi_\\theta(\\cdot|x)} \\left[R_\\phi(x, y)\\right] - \\beta \\cdot D_{\\text{KL}}\\left[\\pi_\\theta(\\cdot|x) \\| \\pi_{\\text{SFT}}(\\cdot|x)\\right]"}
  display={true}
/>

<p>
  In plain English: "maximize the reward score of generated responses, but don't stray
  too far from the original SFT model." The first term pushes quality up; the
  second term, the KL divergence penalty <MathBlock
    formula={"\\beta \\cdot D_{\\text{KL}}"}
  />, acts as a tether back to the SFT policy. The hyperparameter <MathBlock
    formula={"\\beta"}
  /> controls the strength of this tether. Without it,
  the model would exploit the reward model rather than genuinely
  improving.
</p>

<h4>Why PPO?</h4>

<p>
  You might wonder: why not just use basic gradient descent? The problem is that
  standard policy gradient methods can make catastrophically
  large updates when a single batch happens to produce
  unusually large gradients. <strong
    >Proximal Policy Optimization (<GlossaryTooltip
      term="PPO"
    />)</strong
  > prevents this by clipping the probability ratio between the
  new and old policy: if the new policy strays too far from the
  old one on any given action, the gradient is zeroed out, acting
  like a guardrail on update size. Here's the formal objective:
</p>

<MathBlock
  formula={"\\mathcal{L}_{\\text{PPO}} = \\min\\left(r_t(\\theta) \\hat{A}_t,\\ \\text{clip}(r_t(\\theta), 1-\\epsilon, 1+\\epsilon) \\hat{A}_t\\right)"}
  display={true}
/>

<p>
  This looks dense, so let's break it down piece by piece. <MathBlock
    formula={"r_t(\\theta) = \\pi_\\theta(a_t|s_t) / \\pi_{\\text{old}}(a_t|s_t)"}
  /> is the probability ratio between the updated policy and the
  previous policy for action <MathBlock formula={"a_t"} /> in
  state <MathBlock formula={"s_t"} />. If this ratio is 1.0, the policy hasn't changed
  at all; if it's 2.0, the new policy is twice as likely to pick this action. <MathBlock
    formula={"\\hat{A}_t"}
  /> is the advantage estimate (how much better this action was
  than expected), and <MathBlock formula={"\\epsilon"} /> is the
  clipping threshold (typically 0.1-0.2). The min operator ensures
  that overly optimistic ratio changes are capped, preventing
  destructively large policy updates.
</p>

<div
  class="bg-[hsl(var(--diagram-emerald-bg))] p-4 rounded-lg my-4"
>
  <p
    class="font-semibold text-[hsl(var(--diagram-emerald-fg))]"
  >
    Intuition: The "Cautious Manager"
  </p>
  <p class="text-sm">
    Think of PPO as a cautious manager
    overseeing an employee (the model).
  </p>
  <p class="text-sm mt-2">
    When the employee finds a new strategy that works well
    (high advantage), the manager says: "Great, do more of
    that, <strong
      >but don't change your behavior by more than 20% (<MathBlock
        formula="\\epsilon"
      />) all at once.</strong
    >"
  </p>
  <p class="text-sm mt-2">
    This clipping prevents the model from "falling off a
    cliff" by overcommitting to a new strategy that might
    only look good due to noise in the data.
  </p>
</div>

<p>If you're wondering how RL concepts map to language generation, here's the translation for the <GlossaryTooltip term="LLM" /> context:</p>
<ul>
  <li>
    <strong>State</strong>: The prompt + tokens generated so
    far.
  </li>
  <li>
    <strong>Action</strong>: The next token to generate.
  </li>
  <li>
    <strong>Reward</strong>: The reward model score for the
    complete response (sparse reward at the end of
    generation), plus a per-token KL penalty.
  </li>
  <li>
    <strong>Value function</strong>: A separate critic
    network estimates the expected future reward from each
    state (used to compute advantages).
  </li>
</ul>

<h4>RLHF Training Loop</h4>
<ol>
  <li>Sample a batch of prompts from the dataset.</li>
  <li>
    Generate responses using the current policy <MathBlock
      formula={"\\pi_\\theta"}
    />.
  </li>
  <li>
    Score responses with the reward model <MathBlock
      formula={"R_\\phi"}
    />.
  </li>
  <li>
    Compute per-token KL penalties against <MathBlock
      formula={"\\pi_{\\text{SFT}}"}
    />.
  </li>
  <li>Compute advantages using the value function.</li>
  <li>
    Update policy and value networks with <GlossaryTooltip
      term="PPO"
    />.
  </li>
  <li>Repeat.</li>
</ol>

<h2>Direct Preference Optimization (DPO)</h2>

<p>
  If the RLHF pipeline above seems complicated, you're not wrong. It requires training a separate
  reward model, running RL optimization with a critic network,
  carefully tuning <GlossaryTooltip term="PPO" />
  hyperparameters, and managing four models in memory simultaneously
  (policy, reference, reward, critic). That's a lot of moving parts.
</p>

<p>
  <strong><GlossaryTooltip term="DPO" /></strong> offers an elegant shortcut: it eliminates the
  reward model and RL entirely by showing that the optimal policy
  can be derived in closed form directly from the preference data. No reward model,
  no RL loop, no critic network. Just supervised learning with a clever loss function.
</p>

<h3>The DPO Insight</h3>

<p>
  Here's the key mathematical insight. Starting from the RLHF objective you saw earlier,
  Rafailov et al. showed that if you solve for the optimal policy under KL-constrained
  reward maximization, you get a closed-form relationship between the reward
  function and the policy:
</p>

<div class="equation-with-label">
  <p class="equation-label">
    Optimal policy-reward relationship:
  </p>
  <MathBlock
    formula={"R(x, y) = \\beta \\log \\frac{\\pi^*(y|x)}{\\pi_{\\text{ref}}(y|x)} + \\beta \\log Z(x)"}
    display={true}
  />
</div>

<p>
  In plain English: the reward of a response is proportional to how much more likely the
  optimal policy is to generate it compared to the reference policy. This is powerful because
  it means you don't need to learn the reward explicitly; it's already captured in the
  policy itself!
</p>

<p>
  Substituting this into the Bradley-Terry preference model
  and canceling the partition function <MathBlock
    formula="Z(x)"
  />, you get the DPO loss. The key
  intuition: DPO is really a classification
  loss in disguise. It increases the log-probability of preferred
  responses and decreases the log-probability of dispreferred
  ones, with the reference model acting as a regularizer to prevent
  the policy from straying too far:
</p>

<div class="equation-with-label">
  <p class="equation-label">
    DPO loss function:
  </p>
  <MathBlock
    formula={"\\mathcal{L}_{\\text{DPO}}(\\theta) = -\\mathbb{E}_{(x, y_w, y_l)}\\left[\\log \\sigma\\left(\\beta \\log \\frac{\\pi_\\theta(y_w|x)}{\\pi_{\\text{ref}}(y_w|x)} - \\beta \\log \\frac{\\pi_\\theta(y_l|x)}{\\pi_{\\text{ref}}(y_l|x)}\\right)\\right]"}
    display={true}
  />
</div>

<p>
  where <MathBlock formula={"\\pi_\\theta"} /> is the policy being
  trained, <MathBlock formula={"\\pi_{\\text{ref}}"} /> is the
  frozen reference policy (typically the <GlossaryTooltip
    term="SFT"
  /> model), <MathBlock formula={"y_w"} /> and <MathBlock
    formula={"y_l"}
  /> are the preferred and dispreferred responses, and <MathBlock
    formula={"\\beta"}
  /> controls regularization strength. Each log-ratio term <MathBlock
    formula={"\\log \\frac{\\pi_\\theta(y|x)}{\\pi_{\\text{ref}}(y|x)}"}
  /> measures how much the policy has shifted relative to the
  reference for a given response (think of it as an "implicit
  reward").
</p>

<p>
  Think of it this way: the loss drives the policy to assign higher
  probability to preferred responses (relative to the
  reference) and lower probability to dispreferred ones. The gradient is strongest
  when the model's implicit ranking disagrees with the human
  preference data, so it focuses training effort where the model is most "wrong."
</p>

<h3>DPO vs. PPO: Trade-offs</h3>

<p>
  So which should you use? As with most things in engineering, it depends. Here's how
  the two approaches compare:
</p>

<ul>
  <li>
    <strong>Simplicity</strong>: <GlossaryTooltip
      term="DPO"
    /> is a single supervised fine-tuning step (no reward model,
    no critic, no RL loop). Far easier to implement and debug.
  </li>
  <li>
    <strong>Memory</strong>: DPO requires
    only two models (policy + reference). <GlossaryTooltip
      term="PPO"
    /> requires four (policy + reference + reward + critic).
  </li>
  <li>
    <strong>Stability</strong>: <GlossaryTooltip
      term="DPO"
    /> avoids the instabilities of RL training (reward hacking,
    mode collapse during optimization).
  </li>
  <li>
    <strong>Expressiveness</strong>: <GlossaryTooltip
      term="PPO"
    /> can explore beyond the training data distribution by generating
    novel responses. DPO is limited
    to reweighting responses in the preference dataset.
  </li>
  <li>
    <strong>Online vs. offline</strong>: <GlossaryTooltip
      term="PPO"
    /> generates new responses during training (on-policy). <GlossaryTooltip
      term="DPO"
    /> trains on a fixed preference dataset (off-policy), which
    can be suboptimal if the preference data doesn't cover the
    current policy's output distribution.
  </li>
</ul>

<h3>
  DPO in Practice (2025-2026)
</h3>
<p>
  By 2025-2026, <strong
    >DPO has become a widely adopted
    alternative to <GlossaryTooltip
      term="RLHF"
    />/PPO</strong
  > for alignment in production systems. The reasons:
</p>
<ul>
  <li>
    <strong>Empirical effectiveness</strong>: <GlossaryTooltip
      term="DPO"
    /> achieves comparable or better results than <GlossaryTooltip
      term="PPO"
    /> on most alignment benchmarks with dramatically simpler
    implementation
  </li>
  <li>
    <strong>Cost efficiency</strong>: Requiring only two
    models instead of four reduces training infrastructure
    costs significantly
  </li>
  <li>
    <strong>Reliability</strong>: The supervised learning
    formulation is more stable and predictable than RL
    optimization
  </li>
  <li>
    <strong>Iteration speed</strong>: Teams can iterate
    faster on alignment with <GlossaryTooltip
      term="DPO"
    />'s simpler debugging and experimentation workflow
  </li>
</ul>

<h3>
  Beyond DPO: Newer Variants
  (2025-2026)
</h3>
<p>
  The field continues to evolve beyond vanilla <GlossaryTooltip
    term="DPO"
  />. Here are some notable directions you'll encounter in recent papers:
</p>
<ul>
  <li>
    <strong>IPO (Identity Preference Optimization)</strong>:
    Addresses DPO's sensitivity
    to the reference policy by using an identity mapping,
    improving robustness
  </li>
  <li>
    <strong>RLTHF (RL from Targeted Human Feedback)</strong
    >: Focuses preference collection on specific failure
    modes, improving sample efficiency
  </li>
  <li>
    <strong>Verifier-driven RL</strong>: Uses learned
    verifiers (for code, math) as reward signals, enabling
    self-improvement loops
  </li>
  <li>
    <strong
      >GRPO (Group Relative Policy Optimization)</strong
    >: Eliminates the critic/value model by using
    group-based advantage estimation from multiple sampled
    outputs. More memory-efficient than <GlossaryTooltip
      term="PPO"
    />. Used in DeepSeek-R1 for reasoning (Shao et al. 2024)
  </li>
  <li>
    <strong>Agentic self-refinement</strong>: Models
    critique and revise their own outputs using
    constitutional principles
  </li>
</ul>

<PaperReference
  title="DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models"
  authors="Shao et al."
  year="2024"
  url="https://arxiv.org/abs/2402.03300"
/>

<Quiz
  question="What is the key insight that makes DPO possible?"
  quizId="dpo-insight"
  options={[
    {
      id: "a",
      text: "The reward function is unnecessary because the model already knows what's good",
      correct: false,
      explanation:
        "DPO doesn't assume the model knows what's good. It still learns from human preference data; it just does so without an explicit reward model.",
    },
    {
      id: "b",
      text: "The optimal reward function under KL-constrained optimization can be expressed analytically in terms of the optimal policy and reference policy",
      correct: true,
      explanation:
        "Correct! The crucial insight is that the reward function R(x,y) = beta * log(pi*(y|x) / pi_ref(y|x)) + const. This allows substituting the policy directly into the preference model, eliminating the need for an explicit reward model and RL optimization.",
    },
    {
      id: "c",
      text: "Reinforcement learning is always equivalent to supervised learning",
      correct: false,
      explanation:
        "This is not true in general. DPO exploits a specific mathematical property of the KL-constrained reward maximization objective. Other RL problems cannot be reduced to supervised learning.",
    },
    {
      id: "d",
      text: "Human preferences can be learned from a single forward pass",
      correct: false,
      explanation:
        "DPO still requires iterative training (multiple gradient steps). The insight is about eliminating the RL loop, not reducing training to a single step.",
    },
  ]}
/>

<h2>Failure Modes and Challenges</h2>

<p>
  RLHF and DPO are powerful, but they come with some tricky failure modes you should know about.
  Understanding these helps you evaluate alignment claims critically and appreciate
  why this remains an active area of research.
</p>

<h3>Reward Hacking</h3>

<p>
  Here's a frustrating reality: models are very good at finding loopholes. The reward model is an imperfect
  proxy for human preferences, and given enough optimization pressure, the policy will
  find its blind spots.
</p>
<ul>
  <li>
    <strong>Length gaming</strong>: Longer responses often
    receive higher reward scores. The model learns to be
    verbose rather than concise. Mitigated by
    length-normalized rewards or explicit length penalties.
  </li>
  <li>
    <strong>Sycophancy</strong>: The model learns to agree
    with the user's stated views or provide overly confident
    answers, because annotators preferred confident-sounding
    responses.
  </li>
  <li>
    <strong>Formatting tricks</strong>: The model may use
    bullet points, bold text, or other formatting that
    correlates with higher reward scores without improving
    substance.
  </li>
</ul>

<p>
  <strong>Goodhart's Law applies</strong>: "When a measure
  becomes a target, it ceases to be a good measure." The
  reward model measures a proxy of quality; optimizing hard
  against it degrades true quality.
</p>

<h3>Mode Collapse</h3>

<p>
  Over-optimization of the reward can cause the model to
  collapse to a narrow set of high-reward but repetitive
  responses. Imagine a model that always responds in the same "safe" template
  because it reliably scores well. The outputs are technically fine, but
  the diversity and creativity you'd expect are gone.
</p>

<p>
  The KL penalty mitigates this by keeping the policy close
  to the diverse SFT distribution,
  but it's a balancing act: too-low <MathBlock formula={"\\beta"} /> allows collapse,
  while too-high <MathBlock formula={"\\beta"} /> prevents meaningful
  improvement. Finding the right <MathBlock formula={"\\beta"} /> is often
  the most important hyperparameter decision in RLHF.
</p>

<h3>Alignment Tax</h3>

<p>
  RLHF can sometimes reduce raw capability on
  certain benchmarks. This "alignment tax" reflects a real trade-off
  between helpfulness and safety: a model that learns to refuse harmful
  requests may also become overly cautious and refuse ambiguous-but-benign requests.
  You've probably experienced this yourself if you've ever had a chatbot refuse a
  perfectly reasonable request. Ongoing research aims to minimize this tax through better
  reward modeling and more nuanced training.
</p>

<h3>
  Iterative RLHF and Constitutional
  AI
</h3>

<p>
  In practice, alignment isn't a one-shot process. Modern approaches iterate: train a model,
  collect new preference data from it, retrain. Anthropic's <strong
    >Constitutional AI (CAI)</strong
  > takes this further by replacing some human feedback with AI feedback, allowing the
  model to critique its own outputs according to a set of principles
  (a "constitution") and generate preference pairs for <GlossaryTooltip
    term="DPO"
  />/RLHF
  training. This makes alignment more scalable, since you don't need
  human annotators for every iteration.
</p>

<KeyTakeaway>
  <ul>
    <li>
      <strong
        >Preference optimization goes beyond imitation</strong
      >: It optimizes for human preferences rather than
      mimicking demonstrations, enabling models to surpass
      demo quality
    </li>
    <li>
      <strong
        >Historical RLHF stages</strong
      >: SFT
      (demonstrations) → Reward Model (comparisons) → <GlossaryTooltip
        term="PPO"
      />
      (optimization with KL constraint)
    </li>
    <li>
      <strong
        >DPO has become widely adopted</strong
      > (2025-2026), deriving a closed-form loss from preference
      data and eliminating the reward model and RL loop entirely.
      Its simplicity, stability, and effectiveness make it a popular
      alternative to <GlossaryTooltip
        term="RLHF"
      />/PPO
    </li>
    <li>
      <strong>Modern variants</strong>: IPO, RLTHF, and
      verifier-driven approaches continue to improve on
      vanilla DPO for specific use
      cases
    </li>
    <li>
      <strong>Reward hacking</strong> (Goodhart's Law) remains
      a challenge: models exploit imperfect reward proxies through
      length gaming, sycophancy, and formatting tricks
    </li>
    <li>
      <strong>The KL penalty</strong> is critical for preventing
      mode collapse and maintaining the diversity of the pretrained
      model
    </li>
  </ul>
</KeyTakeaway>

<h2>References</h2>

<PaperReference
  title="Training language models to follow instructions with human feedback (InstructGPT)"
  authors="Ouyang et al."
  year="2022"
  url="https://arxiv.org/abs/2203.02155"
  type="paper"
/>

<PaperReference
  title="Direct Preference Optimization: Your Language Model is Secretly a Reward Model"
  authors="Rafailov et al."
  year="2023"
  url="https://arxiv.org/abs/2305.18290"
  type="paper"
/>

<PaperReference
  title="Proximal Policy Optimization Algorithms"
  authors="Schulman et al."
  year="2017"
  url="https://arxiv.org/abs/1707.06347"
  type="paper"
/>

<PaperReference
  title="Constitutional AI: Harmlessness from AI Feedback"
  authors="Bai et al."
  year="2022"
  url="https://arxiv.org/abs/2212.08073"
  type="paper"
/>

<PaperReference
  title="Learning to summarize from human feedback"
  authors="Stiennon et al."
  year="2020"
  url="https://arxiv.org/abs/2009.01325"
  type="paper"
/>

<PaperReference
  title="Scaling Laws for Reward Model Overoptimization"
  authors="Gao et al."
  year="2023"
  url="https://arxiv.org/abs/2210.10760"
  type="paper"
/>
