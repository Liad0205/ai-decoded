---
// Module 3, Lesson 3.2: Supervised Fine-Tuning - From Base Model to Assistant
import KeyTakeaway from "../../components/KeyTakeaway.astro";
import PaperReference from "../../components/PaperReference.astro";
import MathBlock from "../../components/MathBlock.astro";
import RevealSection from "../../components/RevealSection.astro";
import Quiz from "../../components/Quiz.astro";
import GlossaryTooltip from "../../components/GlossaryTooltip.astro";
---

<h2>Learning Objectives</h2>
<p>After completing this lesson, you will be able to:</p>
<ul>
  <li>
    Understand why base models need supervised fine-tuning
    (<GlossaryTooltip term="SFT" />) to become useful assistants
  </li>
  <li>
    Learn how SFT datasets are constructed and what makes
    high-quality demonstration data
  </li>
  <li>
    Analyze catastrophic forgetting and strategies to
    mitigate it
  </li>
  <li>
    Compare SFT to instruction tuning and understand their
    relationship
  </li>
</ul>

<h2>
  Why Fine-Tune? The Gap Between Base Models and Assistants
</h2>

<p>
  Imagine you've just finished pretraining a massive language model. It's absorbed billions of words from books, websites, and code. It's a powerful text completor: give it "The capital of France is" and it will likely continue with "Paris." But try asking it "What is the capital of France?" as a question, and something odd happens. It might generate more questions, continue in a completely different style, or produce rambling, unstructured text. It doesn't know it's supposed to be an assistant.
</p>

<p>
  <strong>Here's the fundamental problem</strong>: the pretraining
  distribution is internet text, not helpful conversations.
  The model learned to mimic the statistical patterns of web
  pages, books, and code. Nobody told it to follow instructions
  or be helpful.
</p>

<p>
  Supervised fine-tuning (<GlossaryTooltip term="SFT" />) bridges this gap. You train the
  model on curated examples of desired input-output
  behavior, specifically (instruction, response) pairs that demonstrate
  how an ideal assistant should respond. Think of it as teaching a
  brilliant but unruly student how to behave in a conversation.
</p>

<h2>The SFT Process</h2>

<h3>Training Objective</h3>

<p>
  Here's a key insight that might surprise you: SFT uses the <em>same</em> next-token prediction objective as pretraining. The only difference? You only compute loss on the <strong>response tokens</strong>, not the prompt tokens. Given a prompt-response pair (x, y), the model is penalized only for how well it predicts the response:
</p>

<MathBlock
  formula={"\\mathcal{L}_{\\text{SFT}}(\\theta) = -\\sum_{t=1}^{|y|} \\log P_\\theta(y_t \\mid x, y_{<t})"}
  display={true}
/>

<p>
  Read this as: "Sum up how surprised the model is at each response token, given everything that came before it." The lower this number, the better the model predicts the demonstration responses. The prompt tokens still flow through the model (providing context), but their loss is masked. You don't want the model learning to generate prompts, only to generate appropriate responses.
</p>

<h3>Practical Training Details</h3>
<p>
  Compared to pretraining, SFT is a much lighter process. Here are the key hyperparameters you need to know:
</p>
<ul>
  <li>
    <strong>Learning rate</strong>: Typically 1-2 orders of
    magnitude smaller than pretraining (e.g., 1e-5 to 2e-5
    vs. 3e-4 for pretraining). You're making gentle adjustments, not rewriting the model's knowledge.
  </li>
  <li>
    <strong>Epochs</strong>: Usually 2-5 epochs over the SFT dataset. Too many epochs causes overfitting to the
    small dataset, and the model starts memorizing responses rather than learning the style.
  </li>
  <li>
    <strong>Batch size</strong>: Smaller than pretraining,
    since fine-tuning is less compute-intensive.
  </li>
  <li>
    <strong>Duration</strong>: SFT is fast compared to
    pretraining: typically hours to days on a few GPUs,
    versus weeks to months on thousands of GPUs for
    pretraining.
  </li>
</ul>

<h2>Constructing SFT Datasets</h2>

<p>
  If there's one thing the research community agrees on, it's this: the quality of SFT data matters far more than the quantity. A small number of high-quality examples consistently outperforms a large number of sloppy ones. So where does this data come from?
</p>

<h3>Data Sources</h3>
<ul>
  <li>
    <strong>Human-written demonstrations</strong>: Expert
    annotators write ideal responses to diverse prompts.
    This is the gold standard but expensive. InstructGPT
    used ~13,000 human-written demonstrations.
  </li>
  <li>
    <strong>Distillation from stronger models</strong>: Use
    a frontier model (like <GlossaryTooltip term="GPT" />-5.2 or Claude) to generate responses,
    then fine-tune a smaller model on these outputs. As an early example, Alpaca
    used 52K examples generated by <GlossaryTooltip term="GPT" />-3.5. This approach is cheaper
    but raises concerns about data contamination and ceiling
    effects.
  </li>
  <li>
    <strong>Community-curated datasets</strong>: Open-source
    efforts like OpenAssistant, Dolly, and ShareGPT collect
    human conversations. Quality varies significantly.
  </li>
</ul>

<h3>What Makes Good SFT Data?</h3>

<RevealSection
  revealId="sft-data-quality"
  title="Principles of High-Quality SFT Data"
>
  <div data-reveal-step>
    <h4>Principle 1: Diversity of Tasks</h4>
    <p>
      Cover a wide range of tasks: question answering,
      summarization, creative writing, code generation,
      math, reasoning, multilingual tasks. The FLAN
      collection demonstrated that task diversity during
      instruction tuning significantly improves
      generalization to unseen tasks.
    </p>
    <button
      type="button"
      class="mt-2 px-3 py-1 bg-[hsl(var(--primary))] text-[hsl(var(--primary-foreground))] rounded text-sm"
      data-reveal-button="1">Next Principle</button
    >
  </div>

  <div data-reveal-step>
    <h4>Principle 2: Response Quality Over Quantity</h4>
    <p>
      LIMA ("Less Is More for Alignment") showed that
      fine-tuning on just 1,000 carefully curated examples
      can produce a model competitive with models trained on
      52K+ examples. Each response should be
      well-structured, accurate, and complete. Poor-quality
      responses actively harm the model.
    </p>
    <button
      type="button"
      class="mt-2 px-3 py-1 bg-[hsl(var(--primary))] text-[hsl(var(--primary-foreground))] rounded text-sm"
      data-reveal-button="2">Next Principle</button
    >
  </div>

  <div data-reveal-step>
    <h4>Principle 3: Format Consistency</h4>
    <p>
      Use a consistent prompt template (chat format) so the
      model learns when to generate and when to stop. Common
      templates include ChatML, Llama chat format, and
      Alpaca format. Inconsistent formatting confuses the
      model and leads to generation artifacts.
    </p>
    <button
      type="button"
      class="mt-2 px-3 py-1 bg-[hsl(var(--primary))] text-[hsl(var(--primary-foreground))] rounded text-sm"
      data-reveal-button="3">Next Principle</button
    >
  </div>

  <div data-reveal-step>
    <h4>Principle 4: Difficulty Distribution</h4>
    <p>
      Include examples across difficulty levels: simple
      factual questions, multi-step reasoning, creative
      tasks, and edge cases. Over-representing easy examples
      wastes training signal. Over-representing hard
      examples can destabilize training. A balanced
      distribution teaches the model to calibrate its
      effort.
    </p>
    <button
      type="button"
      class="mt-2 px-3 py-1 bg-[hsl(var(--primary))] text-[hsl(var(--primary-foreground))] rounded text-sm"
      data-reveal-button="4">Next Principle</button
    >
  </div>

  <div data-reveal-step>
    <h4>Principle 5: Safety Examples</h4>
    <p>
      Include examples that demonstrate appropriate refusal
      of harmful requests, honest acknowledgment of
      limitations, and avoidance of hallucination. Without
      explicit safety examples, the model may be overly
      compliant with harmful instructions.
    </p>
  </div>
</RevealSection>

<h3>Dataset Scale: How Much Data?</h3>

<p>You might be wondering: how many examples do you actually need? The answer depends on what you're trying to achieve:</p>
<ul>
  <li>
    <strong>Instruction following</strong>: As few as
    1,000-10,000 high-quality examples can teach a
    pretrained model to follow instructions (LIMA result).
  </li>
  <li>
    <strong>Broad capability</strong>: 50,000-500,000
    examples covering diverse tasks. This is the range used
    by most production systems.
  </li>
  <li>
    <strong>Domain specialization</strong>: For specialized
    domains (medical, legal, coding), smaller datasets of
    5,000-50,000 domain-specific examples on top of general
    SFT data.
  </li>
</ul>

<h2>Instruction Tuning vs. SFT</h2>

<p>
  You'll see these terms used interchangeably in blog posts and papers, but there's a
  subtle distinction worth knowing:
</p>
<ul>
  <li>
    <strong>Instruction tuning</strong> specifically refers to
    training on (instruction, response) pairs, often focusing
    on teaching the model to follow diverse instructions. FLAN
    and T0 pioneered this approach by converting existing <GlossaryTooltip
      term="NLP"
    /> datasets into instruction format.
  </li>
  <li>
    <strong>Supervised fine-tuning</strong> is the broader term
    for any supervised training after pretraining, including single-task
    fine-tuning (e.g., fine-tuning <GlossaryTooltip
      term="BERT"
    /> for sentiment classification).
  </li>
</ul>

<p>
  In practice, when people in the <GlossaryTooltip term="LLM" /> world say "<GlossaryTooltip
    term="SFT"
  />," they almost always mean multi-task instruction tuning on
  demonstration data. This is the first stage of the alignment
  pipeline, followed by preference optimization (<GlossaryTooltip
    term="DPO"
  /> or <GlossaryTooltip term="RLHF" />/<GlossaryTooltip
    term="PPO"
  />). You'll see this full pipeline in the next lesson.
</p>

<h2>Catastrophic Forgetting</h2>

<p>
  Here's the catch with fine-tuning: when you train on a narrow dataset, the model can <em>lose</em>
  capabilities it acquired during pretraining. This phenomenon is called <strong>catastrophic forgetting</strong>, and it's exactly as dramatic as it sounds. The model overwrites pretrained representations to fit
  the fine-tuning distribution, sacrificing breadth for specialization.
</p>

<h3>Why It Happens</h3>

<p>
  Think of it this way: neural networks learn by adjusting weights to minimize
  loss on whatever data you're currently training on. When you fine-tune on a small,
  narrow dataset, you're pulling the weights toward a new optimum that's great for your SFT data but may be far from where pretraining left them. The fine-tuned
  parameters <MathBlock formula={"\\theta_{\\text{SFT}}"} /> drift
  away from the pretrained parameters <MathBlock
    formula={"\\theta_{\\text{PT}}"}
  /> in the direction that minimizes SFT loss. Here's what that looks like as a single gradient step:
</p>

<MathBlock
  formula={"\\theta_{\\text{SFT}} = \\theta_{\\text{PT}} - \\eta \\nabla_\\theta \\mathcal{L}_{\\text{SFT}}(\\theta_{\\text{PT}})"}
  display={true}
/>

<p>
  In plain English: "Start from the pretrained weights, then take a step (scaled by learning rate <MathBlock formula={"\\eta"} />) in the direction that reduces the SFT loss." The problem? If that direction conflicts with what was learned during pretraining, some of those hard-won capabilities degrade. It's like retraining a multilingual translator exclusively on English conversations; they might get better at English chat but start forgetting their French.
</p>

<h3>Symptoms of Catastrophic Forgetting</h3>
<p>How do you know it's happening? Watch for these warning signs:</p>
<ul>
  <li>
    Degraded performance on tasks not covered by SFT data
    (e.g., coding ability drops after fine-tuning on
    conversation data)
  </li>
  <li>
    Loss of multilingual capabilities when fine-tuning on
    English-only data
  </li>
  <li>
    Reduced factual knowledge or increased hallucination
  </li>
  <li>
    Repetitive or formulaic outputs (the model overfits to
    SFT response patterns)
  </li>
</ul>

<h3>Mitigation Strategies</h3>
<p>Fortunately, there are several proven techniques to keep forgetting in check:</p>
<ul>
  <li>
    <strong>Low learning rate</strong>: A small learning
    rate keeps parameters close to the pretrained optimum,
    reducing forgetting at the cost of slower adaptation.
  </li>
  <li>
    <strong>Data mixing</strong>: Mix SFT data with a
    fraction of pretraining data during fine-tuning. This
    regularizes against forgetting by maintaining exposure
    to the pretrained distribution.
  </li>
  <li>
    <strong>Early stopping</strong>: Monitor validation
    performance on a diverse held-out set. Stop training
    before the model overfits to the SFT distribution.
  </li>
  <li>
    <strong>Parameter-efficient fine-tuning (<GlossaryTooltip
      term="PEFT"
    />)</strong>: Methods like <GlossaryTooltip
      term="LoRA"
    /> (covered in Module 4) freeze most pretrained weights
    and only train small adapter layers.
    This dramatically reduces forgetting by preserving the
    majority of pretrained representations.
  </li>
  <li>
    <strong>Elastic Weight Consolidation (EWC)</strong>: This is a more sophisticated approach. The idea is to add
    a regularization term that penalizes changes to
    parameters that were important for pretraining. Think of it like putting rubber bands on the most critical weights, pulling them back toward their pretrained values:
  </li>
</ul>

<MathBlock
  formula={"\\mathcal{L}_{\\text{total}} = \\mathcal{L}_{\\text{SFT}} + \\frac{\\lambda}{2} \\sum_i F_i (\\theta_i - \\theta_{\\text{PT},i})^2"}
  display={true}
/>

<p>
  Read this as: "The total loss is the SFT loss plus a penalty for moving each weight away from its pretrained value." The penalty is weighted by <MathBlock formula="F_i" /> (the Fisher information for parameter i), which measures how important that parameter was for the pretrained task. Parameters with high Fisher information are anchored more strongly to their pretrained values. Don't worry about memorizing the formula; the core idea is that you let less-important weights change freely while protecting the crucial ones.
</p>

<Quiz
  question="LIMA demonstrated that fine-tuning on just 1,000 carefully curated examples can be competitive with much larger datasets. What is the primary implication of this finding?"
  quizId="lima-insight"
  options={[
    {
      id: "a",
      text: "Pretraining is unnecessary: small datasets are sufficient for all tasks",
      correct: false,
      explanation:
        "LIMA's result depends entirely on a strong pretrained base model. The 1,000 examples work because the model already has broad knowledge from pretraining. Without pretraining, 1,000 examples would be insufficient.",
    },
    {
      id: "b",
      text: "SFT primarily teaches the model a response format/style, not new knowledge; the knowledge comes from pretraining",
      correct: true,
      explanation:
        "Correct! LIMA's key insight is the 'Superficial Alignment Hypothesis': alignment is mainly about teaching the model the right output style and format. The actual knowledge and capabilities come from pretraining. This explains why a small number of high-quality demonstrations suffices.",
    },
    {
      id: "c",
      text: "Data quality doesn't matter as long as the dataset is small enough to avoid overfitting",
      correct: false,
      explanation:
        "LIMA specifically emphasized quality: the 1,000 examples were meticulously curated by researchers. Random or low-quality examples, even in small quantities, would not achieve the same result.",
    },
    {
      id: "d",
      text: "RLHF is unnecessary because SFT alone produces aligned models",
      correct: false,
      explanation:
        "While LIMA showed strong SFT-only results, preference optimization (DPO/IPO) provides additional benefits for safety, reducing harmful outputs, and better calibrating model behavior. Most production systems use preference optimization after SFT.",
    },
  ]}
/>

<h2>Multi-Turn SFT and Chat Formatting</h2>

<p>
  So far, we've talked about single-turn examples: one instruction, one response. But real conversations don't work that way. Production chat models are fine-tuned on multi-turn
  conversations, which teaches the model to:
</p>
<ul>
  <li>Maintain context across multiple exchanges</li>
  <li>Handle follow-up questions and clarifications</li>
  <li>Refer back to earlier parts of the conversation</li>
  <li>
    Distinguish between system instructions, user messages,
    and assistant responses
  </li>
</ul>

<p>Here's what a typical training example looks like in ChatML format:</p>

<pre
  class="bg-[hsl(var(--muted))] p-4 rounded-lg text-sm overflow-x-auto my-4"><code>&lt;|system|&gt;You are a helpful assistant.&lt;|end|&gt;
&lt;|user|&gt;What is photosynthesis?&lt;|end|&gt;
&lt;|assistant|&gt;Photosynthesis is the process by which plants...&lt;|end|&gt;
&lt;|user|&gt;How efficient is it compared to solar panels?&lt;|end|&gt;
&lt;|assistant|&gt;Natural photosynthesis converts about 1-2% of...&lt;|end|&gt;</code></pre>

<p>
  Notice that loss is computed only on the assistant response tokens (the parts after <code>&lt;|assistant|&gt;</code>). The system and user tokens provide context but are not training targets. This way, the model learns <em>how to respond</em>, not how to write user messages.
</p>

<KeyTakeaway>
  <ul>
    <li>
      <strong>SFT bridges the gap</strong> between a base model
      (text completor) and an assistant by training on curated
      (instruction, response) pairs
    </li>
    <li>
      <strong
        >Loss is computed only on response tokens</strong
      >, teaching the model to generate appropriate
      responses, not to reproduce prompts
    </li>
    <li>
      <strong>Quality over quantity</strong>: LIMA showed
      1,000 high-quality examples can rival 52K+ low-quality
      ones, supporting the Superficial Alignment Hypothesis
    </li>
    <li>
      <strong>Catastrophic forgetting</strong> is the primary
      risk, mitigated by low learning rates, data mixing, early
      stopping, and parameter-efficient methods (<GlossaryTooltip
        term="LoRA"
      />)
    </li>
    <li>
      <strong>SFT is typically stage 1</strong> of the alignment
      pipeline, followed by preference optimization (<GlossaryTooltip
        term="DPO"
      />/IPO, which are popular alternatives to <GlossaryTooltip
        term="RLHF"
      />) for further refinement of model behavior
    </li>
  </ul>
</KeyTakeaway>

<h2>References</h2>

<PaperReference
  title="Training language models to follow instructions with human feedback (InstructGPT)"
  authors="Ouyang et al."
  year="2022"
  url="https://arxiv.org/abs/2203.02155"
  type="paper"
/>

<PaperReference
  title="LIMA: Less Is More for Alignment"
  authors="Zhou et al."
  year="2023"
  url="https://arxiv.org/abs/2305.11206"
  type="paper"
/>

<PaperReference
  title="Scaling Instruction-Finetuned Language Models (FLAN-T5/PaLM)"
  authors="Chung et al."
  year="2022"
  url="https://arxiv.org/abs/2210.11416"
  type="paper"
/>

<PaperReference
  title="Self-Instruct: Aligning Language Models with Self-Generated Instructions"
  authors="Wang et al."
  year="2023"
  url="https://arxiv.org/abs/2212.10560"
  type="paper"
/>

<PaperReference
  title="Alpaca: A Strong, Replicable Instruction-Following Model"
  authors="Taori et al."
  year="2023"
  url="https://crfm.stanford.edu/2023/03/13/alpaca.html"
  type="blog"
/>

<PaperReference
  title="Overcoming catastrophic forgetting in neural networks"
  authors="Kirkpatrick et al."
  year="2017"
  url="https://arxiv.org/abs/1612.00796"
  type="paper"
/>
