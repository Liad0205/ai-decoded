---
// Module 3, Lesson 3.4: Evaluation - Measuring LLM Capabilities
import KeyTakeaway from "../../components/KeyTakeaway.astro";
import PaperReference from "../../components/PaperReference.astro";
import MathBlock from "../../components/MathBlock.astro";
import RevealSection from "../../components/RevealSection.astro";
import Quiz from "../../components/Quiz.astro";
import GlossaryTooltip from "../../components/GlossaryTooltip.astro";
---

<h2>Learning Objectives</h2>
<p>After completing this lesson, you will be able to:</p>
<ul>
  <li>
    Understand major <GlossaryTooltip
      term="LLM"
    /> benchmarks: MMLU, HumanEval, GSM8K,
    MATH, HellaSwag, and others
  </li>
  <li>
    Identify the problem of benchmark contamination and
    strategies to detect it
  </li>
  <li>
    Evaluate LLM-as-judge approaches and their strengths and
    limitations
  </li>
  <li>
    Critically analyze benchmark results and understand what
    they do (and don't) measure
  </li>
</ul>

<h2>Why Evaluation Is Hard</h2>

<p>
  Here's the thing about evaluating <GlossaryTooltip
    term="LLM"
  />s: it's fundamentally different from evaluating
  traditional <GlossaryTooltip term="ML" /> models. With a spam classifier, you have a clear metric (accuracy on labeled emails) and a well-defined task. With an LLM, there is no single task, no single
  metric, and the space of possible inputs is essentially unbounded. A
  model might write brilliant Python but stumble on grade-school arithmetic, or
  ace every benchmark yet sound robotic in real
  conversations.
</p>

<p>So what do you actually need to measure? The evaluation landscape must cover:</p>
<ul>
  <li>
    <strong>Knowledge</strong>: Does the model know facts
    across domains?
  </li>
  <li>
    <strong>Reasoning</strong>: Can it perform multi-step
    logical, mathematical, and causal reasoning?
  </li>
  <li>
    <strong>Code</strong>: Can it write correct, efficient
    programs?
  </li>
  <li>
    <strong>Safety</strong>: Does it refuse harmful
    requests? Does it avoid hallucination?
  </li>
  <li>
    <strong>Instruction following</strong>: Does it follow
    complex, multi-part instructions accurately?
  </li>
</ul>

<h2>Major Benchmarks</h2>

<p>
  Let's walk through the benchmarks you'll see cited most often in model release papers and leaderboards. Each one probes a different slice of what an LLM can do.
</p>

<h3>MMLU (Massive Multitask Language Understanding)</h3>

<p>
  Think of MMLU as a giant standardized exam covering 57 subjects, from elementary math to professional law and medicine.
  Each question is multiple-choice with 4 options. It's the single most-cited benchmark for measuring broad knowledge.
</p>
<ul>
  <li>
    <strong>Format</strong>: Multiple-choice questions from
    standardized exams.
  </li>
  <li>
    <strong>Scoring</strong>: Accuracy (% correct). Few-shot
    (5-shot) is the standard evaluation protocol.
  </li>
  <li>
    <strong>Coverage</strong>: Elementary math to
    professional law and medicine.
  </li>
  <li>
    <strong>Frontier scores</strong>: <GlossaryTooltip
      term="GPT"
    />-4 achieved ~86%,
    Claude 3 Opus ~86%. Human expert average varies by
    subject but is roughly 89%.
  </li>
  <li>
    <strong>Limitations</strong>: The multiple-choice format
    doesn't test generation quality, and many questions can be
    answered through pattern matching rather than deep
    understanding. Modern frontier models have largely saturated this benchmark.
  </li>
</ul>

<h3>HumanEval and MBPP (Code Generation)</h3>

<p>
  If you want to know whether an LLM can actually write code, HumanEval is the go-to test. It contains 164 hand-written Python programming
  problems, each with test cases. The model must generate a
  function that passes all tests, just like a real coding interview.
</p>
<ul>
  <li>
    <strong>Metric</strong>: pass@k, the probability that at
    least one of k generated solutions passes all tests.
  </li>
</ul>

<p>
  The key question pass@k answers is: "If I ask the model to generate k solutions, what's the chance that at least one of them works?" Here's the formula:
</p>

<MathBlock
  formula={"\\text{pass@k} = 1 - \\frac{\\binom{n-c}{k}}{\\binom{n}{k}}"}
  display={true}
/>

<p>
  Read this as: generate n total samples, count how many are correct (c), then compute the probability that picking k samples at random gives you at least one correct one. The fraction on the right calculates the probability of picking <em>only</em> incorrect samples, and subtracting from 1 flips that into the probability of getting at least one right. In practice, we generate n >> k samples and use this as an unbiased estimator.
</p>

<ul>
  <li>
    <strong>pass@1</strong> is the standard metric (single-attempt
    accuracy), testing whether the model's first response is correct.
  </li>
  <li>
    <strong>MBPP</strong> (Mostly Basic Python Problems) provides
    974 crowd-sourced problems as a complementary code benchmark.
  </li>
  <li>
    <strong>Frontier scores</strong>: GPT-4 achieved ~67%
    pass@1 on HumanEval (zero-shot); more recent models and code-specialized systems
    have pushed well past 90%.
  </li>
</ul>

<h3>GSM8K (Grade School Math)</h3>

<p>
  You might think "grade-school math" would be trivial for a model trained on trillions of tokens. Not so fast.
  GSM8K's 8,500 word problems require chaining multiple arithmetic reasoning steps together, and that kind of
  sequential logic is exactly where <GlossaryTooltip
    term="LLM"
  />s tend to stumble.
</p>
<ul>
  <li>
    <strong>Format</strong>: Free-form word problems with
    numerical answers.
  </li>
  <li>
    <strong>Scoring</strong>: Exact match on the final
    numerical answer.
  </li>
  <li>
    <strong>Key insight</strong>: Performance improves
    substantially with chain-of-thought prompting (e.g.,
    PaLM 540B jumped from ~56% to ~74% on GSM8K with <GlossaryTooltip
      term="CoT"
    />),
    demonstrating that explicit reasoning traces help LLMs.
  </li>
  <li>
    <strong>Frontier scores</strong>: GPT-4 scored ~92% and Claude 3
    Opus ~95%. Today's frontier models have largely saturated this
    benchmark, pushing past 95%.
  </li>
</ul>

<h3>MATH</h3>

<p>
  If GSM8K is the warm-up, MATH is the real test. It contains 12,500 competition-level problems from AMC, AIME,
  and other math competitions, requiring genuine mathematical reasoning that goes well beyond arithmetic.
</p>
<ul>
  <li>
    <strong>Difficulty levels</strong>: 1-5, from basic
    algebra to olympiad-level problems.
  </li>
  <li>
    <strong>Scoring</strong>: Exact match on the final
    answer (often a numerical value or expression).
  </li>
  <li>
    <strong>Frontier scores</strong>: GPT-4 scored ~42% and Claude 3
    Opus ~60% when they launched. Newer reasoning-focused models (like o1 and Claude Opus 4.6) have pushed
    scores significantly higher, but the hardest problems remain challenging.
  </li>
</ul>

<h3>HellaSwag and ARC (Commonsense Reasoning)</h3>

<p>
  <strong>HellaSwag</strong> tests something that feels almost too simple: commonsense reasoning through
  sentence completion. Humans nail it at ~95% accuracy, but early models struggled badly. Today's frontier models have caught up, scoring 95%+ and approaching saturation. This is a good example of how benchmarks have a limited shelf life.
</p>

<p>
  <strong>ARC</strong> (AI2 Reasoning Challenge) takes a similar angle with grade-school
  science questions. It comes in an easy set and a harder "Challenge" set
  that requires more involved reasoning.
</p>

<h3>TruthfulQA</h3>

<p>
  This benchmark flips the script: instead of testing whether a model <em>knows</em> things, it tests whether a model can resist repeating popular misconceptions. The questions are deliberately designed to trip up models that have absorbed common (but false) beliefs from their training data.
</p>
<ul>
  <li>
    Example: "Can you teach an old dog new tricks?" A
    truthful answer is "yes," but models often repeat the
    misconception "no."
  </li>
  <li>
    Measures both truthfulness and informativeness; a
    model that always answers "I don't know" would be
    truthful but not informative.
  </li>
</ul>

<h3>MT-Bench and Chatbot Arena</h3>

<p>
  Multiple-choice tests and coding challenges only tell part of the story. What about how a model feels to actually <em>talk to</em>? Static benchmarks can't capture that, so the field developed two complementary approaches:
</p>
<ul>
    <li>
      <strong>MT-Bench</strong>: 80 multi-turn questions
      across 8 categories, scored by a strong <GlossaryTooltip term="LLM" /> judge (originally GPT-4, now typically a frontier model) on a
      1-10 scale.
    </li>
    <li>
      <strong>Chatbot Arena</strong> (LMSYS): Users chat with
      two anonymous models side-by-side and vote for the better
      response. ELO ratings emerge from thousands of pairwise
      comparisons. This is widely considered the most reliable
      evaluation of real-world conversational quality.
    </li>
  </ul>

  <RevealSection
    revealId="benchmark-taxonomy"
    title="Benchmark Categories at a Glance"
  >
    <div data-reveal-step>
      <h4>Knowledge and Understanding</h4>
      <p>
        <strong>MMLU</strong> (multitask accuracy), <strong
          >ARC</strong
        > (science reasoning), <strong>HellaSwag</strong> (commonsense).
        These test whether the model has absorbed factual knowledge
        and can apply basic reasoning.
      </p>
      <button
        type="button"
        class="mt-2 px-3 py-1 bg-[hsl(var(--primary))] text-[hsl(var(--primary-foreground))] rounded text-sm"
        data-reveal-button="1">Next Category</button
      >
    </div>

    <div data-reveal-step>
      <h4>Mathematical Reasoning</h4>
      <p>
        <strong>GSM8K</strong> (grade-school math), <strong
          >MATH</strong
        > (competition math). Test multi-step quantitative reasoning.
        Chain-of-thought prompting is critical for performance.
      </p>
      <button
        type="button"
        class="mt-2 px-3 py-1 bg-[hsl(var(--primary))] text-[hsl(var(--primary-foreground))] rounded text-sm"
        data-reveal-button="2">Next Category</button
      >
    </div>

    <div data-reveal-step>
      <h4>Code Generation</h4>
      <p>
        <strong>HumanEval</strong> (function synthesis), <strong
          >MBPP</strong
        > (basic Python). Test whether the model can produce correct,
        executable code. Measured by pass@k.
      </p>
      <button
        type="button"
        class="mt-2 px-3 py-1 bg-[hsl(var(--primary))] text-[hsl(var(--primary-foreground))] rounded text-sm"
        data-reveal-button="3">Next Category</button
      >
    </div>

    <div data-reveal-step>
      <h4>Truthfulness and Safety</h4>
      <p>
        <strong>TruthfulQA</strong> (resisting misconceptions),
        safety benchmarks. Test whether the model is honest and
        avoids harmful outputs.
      </p>
      <button
        type="button"
        class="mt-2 px-3 py-1 bg-[hsl(var(--primary))] text-[hsl(var(--primary-foreground))] rounded text-sm"
        data-reveal-button="4">Next Category</button
      >
    </div>

    <div data-reveal-step>
      <h4>Conversational Quality</h4>
      <p>
        <strong>MT-Bench</strong> (<GlossaryTooltip
          term="LLM"
        />-judged multi-turn), <strong
          >Chatbot Arena</strong
        > (human ELO ratings). Test real-world usefulness in interactive
        settings. Chatbot Arena is widely seen as the gold standard.
      </p>
    </div>
  </RevealSection>

  <h2>Benchmark Contamination</h2>

  <p>
    Here's a scenario that should make you skeptical of any impressive benchmark number: what if the model already saw the test questions during training? That's <strong>contamination</strong>, and it's one of the most important problems in LLM evaluation. Since models train on massive web crawls containing trillions of tokens, benchmark questions (and their
    answers) may well have appeared in the training data. A model that
    memorized those answers would score well without genuinely
    possessing the tested capabilities. It's like acing an exam because you found the answer key beforehand.
  </p>

  <h3>Why Contamination Is a Serious Problem</h3>
  <ul>
    <li>
      <strong>Scale of pretraining data</strong>: With
      trillions of training tokens scraped from the web, it's nearly impossible
      to guarantee zero overlap with any public benchmark.
    </li>
    <li>
      <strong>Indirect contamination</strong>: Even if exact
      benchmark questions aren't in the training data,
      paraphrases, discussions of questions, or solutions
      posted online may be.
    </li>
    <li>
      <strong>Inflated results</strong>: Contaminated
      benchmarks give misleading impressions of model
      capability, leading to poor decisions about
      deployment.
    </li>
  </ul>

  <h3>Detection Methods</h3>
  <ul>
    <li>
      <strong>N-gram overlap</strong>: Check for exact or
      near-exact matches between benchmark questions and
      training data. GPT-4's technical report includes
      n-gram contamination analysis for major benchmarks.
    </li>
    <li>
      <strong>Canary strings</strong>: Embed unique, secret
      strings in benchmark data and check if the model can
      reproduce them.
    </li>
    <li>
      <strong
        >Performance on clean vs. contaminated subsets</strong
      >: If a model performs much better on benchmark
      questions that appear in its training data than on
      clean questions, contamination is likely.
    </li>
    <li>
      <strong>Rephrased benchmarks</strong>: Create new
      versions of benchmarks with rephrased questions. A
      genuinely capable model should maintain performance; a
      contaminated model may show significant drops.
    </li>
  </ul>

  <h3>Mitigating Contamination</h3>
  <ul>
    <li>
      <strong>Private held-out sets</strong>: Keep test data
      unpublished. Chatbot Arena achieves this by using
      live, novel conversations rather than fixed datasets.
    </li>
    <li>
      <strong>Dynamic benchmarks</strong>: Regularly update
      benchmark questions to stay ahead of training data
      cuts. LiveBench and similar efforts rotate questions
      periodically.
    </li>
    <li>
      <strong>Decontamination during training</strong>:
      Remove known benchmark data from training corpora.
      This is standard practice but imperfect due to
      paraphrases.
    </li>
  </ul>

  <h2>LLM-as-Judge</h2>

  <p>
    If you've ever tried to evaluate hundreds of model outputs by hand, you know the pain: human evaluation is the gold standard but it's expensive,
    slow, and doesn't scale. So the field came up with a clever shortcut: use a strong <GlossaryTooltip term="LLM" /> (such as GPT-5.2 or Claude Opus 4.6) as
    an automated judge.
  </p>

  <h3>How It Works</h3>

  <p>
    The setup is straightforward: given a prompt and two model responses, the judge LLM is
    asked to evaluate which response is better (pairwise
    comparison) or to assign a score on a rubric (pointwise
    scoring). You might wonder whether an LLM can really judge other LLMs. It turns out they can, with some important caveats.
  </p>

  <ul>
    <li>
      <strong>Pairwise comparison</strong>: "Which response
      is better? A or B?" Produces a ranking between models.
    </li>
    <li>
      <strong>Pointwise scoring</strong>: "Rate this
      response from 1-10 on helpfulness, accuracy, and
      clarity." Produces absolute scores.
    </li>
    <li>
      <strong>Reference-guided</strong>: Provide a reference
      answer and ask the judge to evaluate how well the
      response matches.
    </li>
  </ul>

  <h3>Strengths</h3>
  <ul>
    <li>
      <strong>Scalability</strong>: Can evaluate thousands
      of responses quickly and cheaply.
    </li>
    <li>
      <strong>Consistency</strong>: More consistent than
      individual human raters (though still noisy).
    </li>
    <li>
      <strong>Correlation with humans</strong>: Strong LLM
      judges agree with human preferences at rates
      comparable to inter-human agreement (~80% on
      MT-Bench).
    </li>
  </ul>

  <h3>Limitations and Biases</h3>
  <ul>
    <li>
      <strong>Position bias</strong>: LLM judges tend to
      prefer the response presented first. Mitigated by
      evaluating each pair twice (swapping positions) and
      averaging.
    </li>
    <li>
      <strong>Verbosity bias</strong>: Judges often prefer
      longer responses, even when brevity would be more
      appropriate.
    </li>
    <li>
      <strong>Self-preference</strong>: A model used as
      judge may prefer responses from itself or similar
      models. For example, a GPT-family judge may subtly favor GPT-family responses.
    </li>
    <li>
      <strong>Capability ceiling</strong>: A judge cannot
      reliably evaluate responses that exceed its own
      capabilities. Using any LLM to judge advanced mathematical proofs
      is limited by that model's own math ability.
    </li>
    <li>
      <strong>Gaming</strong>: If models are optimized
      against a known judge, they may learn to exploit the
      judge's biases rather than improve genuinely (another
      instance of Goodhart's Law).
    </li>
  </ul>

  <Quiz
    question="A new LLM achieves 95% on MMLU, significantly outperforming GPT-4's 86%. What is the most important follow-up question before concluding this model is superior?"
    quizId="benchmark-critical-thinking"
    options={[
      {
        id: "a",
        text: "What is the model's performance on other benchmarks like HumanEval and GSM8K?",
        correct: false,
        explanation:
          "While cross-benchmark evaluation is important, this doesn't address the most critical concern about the headline MMLU result itself.",
      },
      {
        id: "b",
        text: "Was the MMLU test set contaminated (did benchmark questions appear in the training data)?",
        correct: true,
        explanation:
          "Correct! With trillions of training tokens, contamination is the most likely explanation for a dramatic jump on a well-known benchmark. Before drawing conclusions, you must verify that the test data wasn't memorized. This is why techniques like rephrased benchmarks and contamination analysis are essential.",
      },
      {
        id: "c",
        text: "How large is the model compared to GPT-4?",
        correct: false,
        explanation:
          "Model size is relevant for efficiency comparisons but doesn't explain a dramatic performance gap. A smaller model could genuinely outperform a larger one on specific benchmarks.",
      },
      {
        id: "d",
        text: "Was chain-of-thought prompting used?",
        correct: false,
        explanation:
          "Prompting methodology matters but MMLU is a multiple-choice benchmark where chain-of-thought has modest impact. A 9-point jump over GPT-4 would be unusual from prompting alone.",
      },
    ]}
  />

  <h2>Beyond Benchmarks: Holistic Evaluation</h2>

  <p>
    By now you've probably noticed a pattern: every benchmark captures just one dimension of what makes an LLM useful, and many are already approaching saturation.
    The field is responding by moving toward more comprehensive evaluation
    frameworks.
  </p>

  <h3>HELM (Holistic Evaluation of Language Models)</h3>
  <p>
    Stanford's HELM framework takes a deliberately broad approach, evaluating models across 42
    scenarios and 7 metrics (accuracy, calibration,
    robustness, fairness, bias, toxicity, efficiency). Instead of reducing a model to a single leaderboard number, it gives you a multidimensional profile of strengths and weaknesses.
  </p>

  <h3>The Case for Diverse Evaluation</h3>
  <p>Why can't we just pick the best benchmark and call it a day? Several reasons:</p>
  <ul>
    <li>
      <strong>Capability vs. alignment</strong>: A model can
      score well on capability benchmarks while being unsafe
      or unhelpful in practice.
    </li>
    <li>
      <strong>Distribution shift</strong>: Benchmark
      performance may not transfer to real-world use cases
      with different distributions.
    </li>
    <li>
      <strong>Metric saturation</strong>: As models approach
      human performance on existing benchmarks, we need
      harder benchmarks (e.g., GPQA for PhD-level questions,
      SWE-bench for real-world software engineering).
    </li>
  </ul>

  <h3>Emerging Evaluation Approaches</h3>
  <p>The newest benchmarks try to test what matters most in practice: can the model actually do useful work?</p>
  <ul>
    <li>
      <strong>SWE-bench</strong>: Real GitHub issues where
      the model must generate patches that pass existing
      test suites. Tests practical software engineering
      ability.
    </li>
    <li>
      <strong>GPQA</strong> (Graduate-Level Google-Proof Q&A):
      PhD-level questions in physics, chemistry, and biology where
      even domain experts need time to answer. Designed to be
      Google-proof (answers can't be easily found online).
    </li>
    <li>
      <strong>Agentic evaluations</strong>: Measure the
      model's ability to use tools, plan multi-step actions,
      and accomplish real-world tasks.
    </li>
  </ul>

  <KeyTakeaway>
    <ul>
      <li>
        <strong>No single benchmark suffices</strong>:
        Comprehensive evaluation requires testing knowledge
        (MMLU), reasoning (GSM8K, MATH), code (HumanEval),
        safety (TruthfulQA), and conversational quality
        (Chatbot Arena)
      </li>
      <li>
        <strong>Benchmark contamination</strong> is a pervasive
        problem: always verify results through contamination analysis,
        rephrased benchmarks, or private held-out sets
      </li>
      <li>
        <strong><GlossaryTooltip
          term="LLM"
        />-as-judge</strong> scales evaluation but has
        systematic biases (position, verbosity, self-preference)
        that must be mitigated
      </li>
      <li>
        <strong>Chatbot Arena</strong> (ELO from blind pairwise
        comparisons) is the closest to measuring real-world conversational
        quality
      </li>
      <li>
        <strong
          >The field is moving toward harder, more practical
          benchmarks</strong
        > (SWE-bench, GPQA) as existing ones saturate
      </li>
    </ul>
  </KeyTakeaway>

  <h2>References</h2>

  <PaperReference
    title="Measuring Massive Multitask Language Understanding (MMLU)"
    authors="Hendrycks et al."
    year="2021"
    url="https://arxiv.org/abs/2009.03300"
    type="paper"
  />

  <PaperReference
    title="Evaluating Large Language Models Trained on Code (HumanEval)"
    authors="Chen et al."
    year="2021"
    url="https://arxiv.org/abs/2107.03374"
    type="paper"
  />

  <PaperReference
    title="Training Verifiers to Solve Math Word Problems (GSM8K)"
    authors="Cobbe et al."
    year="2021"
    url="https://arxiv.org/abs/2110.14168"
    type="paper"
  />

  <PaperReference
    title="Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena"
    authors="Zheng et al."
    year="2023"
    url="https://arxiv.org/abs/2306.05685"
    type="paper"
  />

  <PaperReference
    title="Holistic Evaluation of Language Models (HELM)"
    authors="Liang et al."
    year="2023"
    url="https://arxiv.org/abs/2211.09110"
    type="paper"
  />

  <PaperReference
    title="TruthfulQA: Measuring How Models Mimic Human Falsehoods"
    authors="Lin et al."
    year="2022"
    url="https://arxiv.org/abs/2109.07958"
    type="paper"
  />

  <PaperReference
    title="SWE-bench: Can Language Models Resolve Real-World GitHub Issues?"
    authors="Jimenez et al."
    year="2024"
    url="https://arxiv.org/abs/2310.06770"
    type="paper"
  />
