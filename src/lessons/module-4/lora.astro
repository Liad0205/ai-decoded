---
// Module 4, Lesson 4.2: LoRA: Low-Rank Adaptation of Large Language Models
import KeyTakeaway from "../../components/KeyTakeaway.astro";
import PaperReference from "../../components/PaperReference.astro";
import MathBlock from "../../components/MathBlock.astro";
import Quiz from "../../components/Quiz.astro";
import RevealSection from "../../components/RevealSection.astro";
import Diagram from "../../components/Diagram.astro";
import GlossaryTooltip from "../../components/GlossaryTooltip.astro";
---

<section>
  <h2>Learning Objectives</h2>
  <p>After completing this lesson, you will be able to:</p>
  <ul>
    <li>
      Understand the low-rank hypothesis: why weight updates
      during fine-tuning occupy a low-dimensional subspace
    </li>
    <li>
      Derive the <GlossaryTooltip term="LoRA" /> reparameterization <MathBlock
        formula="W' = W + BA"
      /> from first principles
    </li>
    <li>
      Analyze rank selection and its impact on model
      capacity and efficiency
    </li>
    <li>
      Understand how LoRA adapters can be merged into base
      weights for zero-overhead inference
    </li>
  </ul>
</section>

<section>
  <h2>The Low-Rank Hypothesis</h2>

  <p>
    Here is the question that motivates everything in this lesson:
    if full fine-tuning works, why bother with anything else? The
    answer starts with a surprising empirical observation: <strong
      >the weight changes during fine-tuning have low
      intrinsic rank</strong
    >. When you compare pretrained weights to fine-tuned
    weights, the difference <MathBlock
      formula={"\\Delta W = W_{\\text{fine-tuned}} - W_{\\text{pretrained}}"}
    /> can be well-approximated by a low-rank matrix.
  </p>

  <p>
    Why is this remarkable? A weight matrix in a 7B model might
    have dimensions <MathBlock
      formula={"4096 \\times 4096"}
    /> with rank up to 4096. But the <em>change</em> needed for
    task adaptation might have an effective rank of just 4, 8,
    or 16. That means most of the "space" in the weight update is
    redundant.
  </p>

  <h3>Why Low Rank?</h3>
  <p>
    You might wonder: why would the update be so simple?
    Because pretrained models already encode rich representations.
    Fine-tuning for a specific task only needs to make a
    small, structured adjustment, like rotating the
    representation space slightly to align with
    task-specific features. Such structured adjustments
    naturally have low rank.
  </p>

  <p>
    Aghajanyan et al. (2020) formalized this as <strong
      >intrinsic dimensionality</strong
    >: the minimum number of parameters needed to reach 90%
    of full fine-tuning performance. For many <GlossaryTooltip term="NLP" /> tasks,
    this intrinsic dimensionality is orders of magnitude
    smaller than the full parameter count.
  </p>

  <div
    class="bg-[hsl(var(--diagram-purple-bg))] p-4 rounded-lg my-4"
  >
    <p
      class="font-semibold text-[hsl(var(--diagram-purple-fg))]"
    >
      Analogy: The Stencil
    </p>
    <p class="text-sm">
      Imagine you want to paint a complex mural (adapt the
      model) on a wall (the pretrained weights).
    </p>
    <ul class="list-disc list-inside text-sm mt-2 ml-2">
      <li>
        <strong>Full Fine-Tuning</strong>: You repaint the
        entire wall pixel by pixel. Precise, but slow and
        expensive.
      </li>
      <li>
        <strong>LoRA</strong>: You use a <strong
          >stencil</strong
        >. You only need to define a few simple shapes (low
        rank matrices <MathBlock formula="A" /> and <MathBlock
          formula="B"
        />) to guide the spray paint. You can create a
        complex-looking result by combining these simple
        stencils with the existing wall color, without
        repainting everything.
      </li>
    </ul>
  </div>
</section>

<section>
  <h2>LoRA: The Core Idea</h2>

  <p>
    Now that you understand <em>why</em> the weight update is
    low-rank, here is the trick that exploits it. Instead of
    learning <MathBlock
      formula={"\\Delta W \\in \\mathbb{R}^{d \\times d}"}
    /> directly (which has <MathBlock formula="d^2" /> parameters),
    LoRA decomposes it into two smaller matrices:
  </p>

  <MathBlock formula={"\\Delta W = BA"} display={true} />

  <p>
    Read this as: "the weight update equals the product of a tall,
    thin matrix <MathBlock
      formula={"B \\in \\mathbb{R}^{d \\times r}"}
    /> and a short, wide matrix <MathBlock
      formula={"A \\in \\mathbb{R}^{r \\times d}"}
    />, where the bottleneck dimension <MathBlock formula={"r \\ll d"} />
    is much smaller than the original."
  </p>

  <p>
    Think of it this way: instead of updating the full weight matrix
    (which can have millions of parameters), you factor
    the update into two small matrices whose product
    approximates the full update. The rank <MathBlock
      formula="r"
    /> controls the expressiveness vs. efficiency tradeoff.
    A rank-16 decomposition of a 4096 x 4096 matrix replaces 16.7M
    parameters with just 131K.
  </p>

  <p>
    To put the savings in concrete terms: for a weight
    matrix of size 4096 x 4096 (16.7M parameters), a rank-16
    LoRA adapter uses only 4096 x 16 + 16 x 4096 = 131K
    parameters (less than 1% of the original). Across all
    attention layers in a 7B model, this typically means
    training ~10M parameters instead of 7B.
  </p>

  <p>
    So what happens during a forward pass? The adapted layer
    computes:
  </p>

  <MathBlock
    formula={"h = W'x = Wx + \\Delta Wx = Wx + BAx"}
    display={true}
  />
  <p>
    In plain English: you take the original output (<MathBlock
      formula="Wx" />) and add a small correction from the
    LoRA path (<MathBlock formula="BAx" />). The base weights
    <MathBlock formula="W" /> stay frozen; only <MathBlock
      formula="B"
    /> and <MathBlock formula="A" /> are trained. This means
    you get all the benefits of adaptation without touching
    the original model.
  </p>

  <Diagram
    diagramId="lora-decomposition"
    title="LoRA Weight Decomposition"
    autoplay={true}
    animationDuration={4000}
  >
    <div
      class="bg-[hsl(var(--card))] p-4 rounded w-full"
    >
      <div
        class="flex items-center justify-center gap-4 flex-wrap"
      >
        <!-- Original weight -->
        <div
          class="flex flex-col items-center"
          data-animate
          style="animation-delay: 0.2s"
        >
          <div
            class="w-24 h-24 bg-[hsl(var(--muted))] border-2 border-[hsl(var(--border))] rounded flex items-center justify-center"
          >
            <span class="text-sm font-mono font-bold"
              >W</span
            >
          </div>
          <span class="text-xs mt-1 text-[hsl(var(--muted-foreground))]"
            >d x d</span
          >
          <span class="text-xs text-[hsl(var(--muted-foreground))]">frozen</span>
        </div>

        <div
          class="text-2xl font-bold text-[hsl(var(--muted-foreground))]"
          data-animate
          style="animation-delay: 0.5s"
        >
          +
        </div>

        <!-- B matrix -->
        <div
          class="flex flex-col items-center"
          data-animate
          style="animation-delay: 1.0s"
        >
          <div
            class="w-8 h-24 bg-[hsl(var(--diagram-indigo-solid))] border-2 border-[hsl(var(--diagram-indigo-border))] rounded flex items-center justify-center"
          >
            <span
              class="text-xs font-mono font-bold text-white"
              >B</span
            >
          </div>
          <span class="text-xs mt-1 text-[hsl(var(--diagram-indigo-fg))]"
            >d x r</span
          >
          <span class="text-xs text-[hsl(var(--diagram-indigo-fg))]"
            >trainable</span
          >
        </div>

        <div
          class="text-lg font-bold text-[hsl(var(--muted-foreground))]"
          data-animate
          style="animation-delay: 1.2s"
        >
          x
        </div>

        <!-- A matrix -->
        <div
          class="flex flex-col items-center"
          data-animate
          style="animation-delay: 1.4s"
        >
          <div
            class="w-24 h-8 bg-[hsl(var(--diagram-emerald-solid))] border-2 border-[hsl(var(--diagram-emerald-border))] rounded flex items-center justify-center"
          >
            <span
              class="text-xs font-mono font-bold text-white"
              >A</span
            >
          </div>
          <span class="text-xs mt-1 text-[hsl(var(--diagram-emerald-fg))]"
            >r x d</span
          >
          <span class="text-xs text-[hsl(var(--diagram-emerald-fg))]"
            >trainable</span
          >
        </div>

        <div
          class="text-2xl font-bold text-[hsl(var(--muted-foreground))]"
          data-animate
          style="animation-delay: 1.6s"
        >
          =
        </div>

        <!-- Result -->
        <div
          class="flex flex-col items-center"
          data-animate
          style="animation-delay: 2.0s"
        >
          <div
            class="w-24 h-24 bg-[hsl(var(--diagram-purple-border))] border-2 border-[hsl(var(--diagram-purple-border))] rounded flex items-center justify-center"
          >
            <span class="text-sm font-mono font-bold"
              >W'</span
            >
          </div>
          <span class="text-xs mt-1 text-[hsl(var(--diagram-purple-fg))]"
            >d x d</span
          >
          <span class="text-xs text-[hsl(var(--diagram-purple-fg))]"
            >adapted</span
          >
        </div>
      </div>

      <div
        class="mt-4 text-sm text-[hsl(var(--muted-foreground))] text-center"
        data-animate
        style="animation-delay: 3.0s"
      >
        With d=4096 and r=8: trainable params = 2 x 4096 x 8
        = 65,536 vs. 16,777,216 for full matrix (0.4%)
      </div>
    </div>
  </Diagram>

  <h3>Parameter Savings</h3>
  <p>
    Let's quantify exactly how much you save. The total number
    of trainable parameters in the <GlossaryTooltip
      term="LoRA"
    />
    decomposition is:
  </p>
  <div class="equation-with-label">
    <MathBlock
      formula={"\\text{LoRA params} = d \\times r + r \\times d = 2dr"}
      display={true}
    />
    <div class="text-sm font-medium text-[hsl(var(--foreground))]">
      Trainable parameters
    </div>
  </div>
  <p>
    In other words, you only train two thin matrices (one
    tall, one wide), each with <MathBlock
      formula="d \times r"
    /> entries. Compared to the full matrix:
  </p>
  <div class="equation-with-label">
    <MathBlock
      formula={"\\text{Ratio} = \\frac{2dr}{d^2} = \\frac{2r}{d}"}
      display={true}
    />
    <div class="text-sm font-medium text-[hsl(var(--foreground))]">
      Parameter efficiency ratio
    </div>
  </div>
  <p>
    This ratio tells you what fraction of the original
    parameters you actually train. It scales linearly with
    rank and inversely with dimension, so larger models
    get even better efficiency from LoRA.
  </p>
  <p>
    For <MathBlock formula="d = 4096" /> and <MathBlock
      formula="r = 8"
    />: ratio = <MathBlock
      formula={"2 \\times 8 / 4096 = 0.004"}
    />, meaning <strong
      >only 0.4% of the parameters per adapted matrix</strong
    >.
  </p>
</section>

<section>
  <h2>Full LoRA Derivation</h2>

  <p>
    Now that you have the intuition, let's walk through the math
    step by step. Each step builds on the previous one, so take
    your time.
  </p>

  <RevealSection
    revealId="lora-derivation"
    title="Step-by-Step LoRA Math"
  >
    <div data-reveal-step>
      <h4>Step 1: The Standard Linear Layer</h4>
      <p>A pretrained linear layer computes:</p>
      <MathBlock formula="h = Wx" display={true} />
      <p>
        where <MathBlock
          formula={"W \\in \\mathbb{R}^{d_{\\text{out}} \\times d_{\\text{in}}}"}
        /> and <MathBlock
          formula={"x \\in \\mathbb{R}^{d_{\\text{in}}}"}
        />. During full fine-tuning, we learn <MathBlock
          formula={"W + \\Delta W"}
        />.
      </p>
      <button
        type="button"
        class="mt-2 px-3 py-1 bg-[hsl(var(--primary))] text-[hsl(var(--primary-foreground))] rounded text-sm"
        data-reveal-button="1"
      >
        Next Step →
      </button>
    </div>

    <div data-reveal-step>
      <h4>Step 2: Low-Rank Constraint</h4>
      <p>
        LoRA constrains <MathBlock formula={"\\Delta W"} /> to
        have rank at most <MathBlock formula="r" />:
      </p>
      <MathBlock
        formula={"\\Delta W = BA, \\quad B \\in \\mathbb{R}^{d_{\\text{out}} \\times r}, \\quad A \\in \\mathbb{R}^{r \\times d_{\\text{in}}}"}
        display={true}
      />
      <p>
        Read this as: the weight update is forced to pass through
        a bottleneck of dimension <MathBlock formula="r" />.
        Matrix <MathBlock formula="A" /> projects the input down
        to this small dimension, and <MathBlock formula="B" />
        projects it back up. If you have worked with
        autoencoders, this is the same compress-then-expand idea.
      </p>
      <button
        type="button"
        class="mt-2 px-3 py-1 bg-[hsl(var(--primary))] text-[hsl(var(--primary-foreground))] rounded text-sm"
        data-reveal-button="2"
      >
        Next Step →
      </button>
    </div>

    <div data-reveal-step>
      <h4>Step 3: Initialization</h4>
      <p>
        This is a clever detail. At the start of fine-tuning,
        you want <MathBlock formula={"\\Delta W"} /> to be zero
        so the model starts from its pretrained behavior
        (not from some random perturbation):
      </p>
      <MathBlock
        formula={"A \\sim \\mathcal{N}(0, \\sigma^2), \\quad B = 0"}
        display={true}
      />
      <p>
        Read this as: <MathBlock formula="B" /> starts as all
        zeros, which guarantees <MathBlock formula="BA = 0" /> at
        the beginning of training. <MathBlock formula="A" /> uses
        a random Gaussian initialization (like Kaiming) to break
        symmetry. The result is that training begins from
        exactly the pretrained model's behavior.
      </p>
      <button
        type="button"
        class="mt-2 px-3 py-1 bg-[hsl(var(--primary))] text-[hsl(var(--primary-foreground))] rounded text-sm"
        data-reveal-button="3"
      >
        Next Step →
      </button>
    </div>

    <div data-reveal-step>
      <h4>Step 4: Scaling Factor</h4>
      <p>
        LoRA introduces a scaling constant <MathBlock
          formula={"\\alpha"}
        /> to control the magnitude of the adaptation. Without
        it, doubling the rank would roughly double the update
        magnitude, making hyperparameter tuning rank-dependent.
        Let's make this precise:
      </p>
      <MathBlock
        formula={"h = Wx + \\frac{\\alpha}{r} BAx"}
        display={true}
      />
      <p>
        Read this as: the output is the frozen pretrained result
        plus a scaled low-rank correction. The factor <MathBlock
          formula={"\\alpha / r"}
        /> normalizes the update so its magnitude stays roughly
        constant when you change the rank. Think of it like a
        learning rate specific to the LoRA path. Typical
        defaults are <MathBlock formula={"\\alpha = r"} /> (so the
        scaling factor is 1) or <MathBlock
          formula={"\\alpha = 2r"}
        />.
      </p>
      <button
        type="button"
        class="mt-2 px-3 py-1 bg-[hsl(var(--primary))] text-[hsl(var(--primary-foreground))] rounded text-sm"
        data-reveal-button="4"
      >
        Next Step →
      </button>
    </div>

    <div data-reveal-step>
      <h4>Step 5: Which Matrices to Adapt?</h4>
      <p>
        LoRA can be applied to any linear layer. In
        practice, the attention projection matrices are the
        primary targets:
      </p>
      <ul class="mt-2 space-y-1">
        <li>
          <MathBlock formula="W_Q" /> and <MathBlock
            formula="W_V"
          /> (the original LoRA paper's recommendation)
        </li>
        <li>
          <MathBlock formula="W_Q, W_K, W_V, W_O" />, all attention
          projections (common in practice)
        </li>
        <li>
          <GlossaryTooltip term="MLP" /> layers (<MathBlock
            formula={"W_{\\text{up}}, W_{\\text{down}}, W_{\\text{gate}}"}
          />), sometimes included for higher capacity
        </li>
      </ul>
      <p
        class="mt-2 text-[hsl(var(--foreground))] font-medium"
      >
        More target matrices = more trainable parameters but
        also more capacity for complex adaptations.
      </p>
    </div>
  </RevealSection>
</section>

<section>
  <h2>Rank Selection: The Key Hyperparameter</h2>

  <p>
    When you use LoRA in practice, the single most important
    decision you will make is choosing the rank <MathBlock
      formula="r" />. This is the main <GlossaryTooltip
      term="LoRA"
    />
    hyperparameter, and it controls the expressiveness vs.
    efficiency tradeoff:
  </p>

  <ul>
    <li>
      <strong>r = 1-4</strong>: Ultra-efficient. Good for
      simple classification tasks where the adaptation is a
      small rotation of the representation space.
    </li>
    <li>
      <strong>r = 8-16</strong>: The sweet spot for most
      instruction-tuning and domain adaptation tasks.
      Captures enough task structure with minimal overhead.
    </li>
    <li>
      <strong>r = 32-64</strong>: Higher capacity. Useful
      for complex tasks (e.g., multilingual adaptation,
      significant domain shifts like code-to-medical).
    </li>
    <li>
      <strong>r = 128+</strong>: Approaches full fine-tuning
      expressiveness. Diminishing returns; at this point,
      consider whether full fine-tuning is warranted.
    </li>
  </ul>

  <p>
    The original LoRA paper found that <strong
      >rank 4 already captures most of the task-specific
      signal</strong
    > for many <GlossaryTooltip term="NLP" /> benchmarks, with diminishing returns beyond
    rank 8. This confirms the low-rank hypothesis.
  </p>

  <h3>Memory Savings with LoRA</h3>
  <p>
    The parameter savings translate directly into memory savings,
    which is often the real bottleneck. Because only <MathBlock
      formula="B" /> and <MathBlock formula="A" /> are trained,
    the optimizer states and gradients shrink dramatically:
  </p>
  <div class="equation-with-label">
    <MathBlock
      formula={"\\text{LoRA trainable params} = 2 \\times r \\times d \\times \\text{(number of adapted layers)}"}
      display={true}
    />
    <div class="text-sm font-medium text-[hsl(var(--foreground))]">
      Total trainable parameters
    </div>
  </div>
  <p>
    Each adapted layer contributes two matrices (B and A),
    each with <MathBlock formula="r \times d" /> entries. The
    total across the model is simply this per-layer count times
    the number of layers you apply LoRA to.
  </p>
  <p>
    For LLaMA-2 7B with <MathBlock formula="r = 8" /> applied
    to all attention projections (Q, K, V, O across 32 layers):
  </p>
  <div class="equation-with-label">
    <MathBlock
      formula={"\\text{Trainable} = 2 \\times 8 \\times 4096 \\times 4 \\times 32 = 8{,}388{,}608 \\approx 8.4\\text{M params}"}
      display={true}
    />
    <div class="text-sm font-medium text-[hsl(var(--foreground))]">
      LLaMA-2 7B calculation
    </div>
  </div>
  <p>
    Breaking that down: 2 (matrices per layer) x 8 (rank) x
    4096 (hidden dim) x 4 (Q, K, V, O projections) x 32
    (transformer layers) = 8.4M trainable parameters. This is
    <strong>0.12% of the 6.74B total parameters</strong>.
    In practical terms, the optimizer states for 8.4M params in
    FP32 require only ~67 MB instead of ~54 GB. That is the
    difference between fitting on a single consumer GPU and
    needing a multi-GPU cluster.
  </p>
</section>

<section>
  <h2>Merging: Zero-Overhead Inference</h2>

  <p>
    Here is one of LoRA's most elegant properties, and one that
    makes it especially practical for production. Since the
    forward pass is just two additive terms, <strong
      >you can merge the adapter back into the base weights
      before deployment</strong
    >:
  </p>
  <MathBlock
    formula="h = Wx + BAx = (W + BA)x"
    display={true}
  />
  <p>
    In plain English: by the distributive property, the base
    computation plus the LoRA correction can be factored into a
    single matrix multiply. You precompute <MathBlock
      formula="W' = W + BA"
    /> and replace <MathBlock formula="W" /> entirely. The result:
  </p>

  <ul>
    <li>
      <strong>No extra latency</strong>: The merged model
      has the exact same architecture and inference speed as
      the base model
    </li>
    <li>
      <strong>No extra memory</strong>: No adapter matrices
      need to be stored at inference time
    </li>
    <li>
      <strong>Easy deployment</strong>: Swap adapters by
      loading different <MathBlock formula="B, A" /> matrices
      (each only a few MB) and remerging
    </li>
  </ul>

  <p>
    For <strong>multi-task serving</strong>, you can keep
    the base model in GPU memory and swap lightweight <GlossaryTooltip
      term="LoRA"
    />
    adapters per request. This is far more efficient than
    loading separate full models.
  </p>

  <h3>Unmerging for Continued Training</h3>
  <p>
    What if you need to keep fine-tuning after merging? You can
    always "unmerge" by subtracting: <MathBlock
      formula="W = W' - BA" />. This allows you to iteratively
    refine adapters without retraining from scratch.
  </p>
</section>

<Quiz
  question="If a weight matrix W has dimensions 4096 x 4096 and you apply LoRA with rank r=16, how many trainable parameters does the LoRA adapter add for this single matrix?"
  quizId="lora-params"
  options={[
    {
      id: "a",
      text: "65,536 (4096 x 16)",
      correct: false,
      explanation:
        "This is only one of the two matrices. LoRA uses both A (r x d) and B (d x r).",
    },
    {
      id: "b",
      text: "131,072 (2 x 4096 x 16)",
      correct: true,
      explanation:
        "Correct! LoRA adds B (4096 x 16) and A (16 x 4096), totaling 2 x 4096 x 16 = 131,072 parameters. This is only 0.78% of the original 16,777,216 parameters in W.",
    },
    {
      id: "c",
      text: "256 (16 x 16)",
      correct: false,
      explanation:
        "This would be the size of the bottleneck layer itself. LoRA projects from the full dimension d to rank r and back.",
    },
    {
      id: "d",
      text: "16,777,216 (4096 x 4096)",
      correct: false,
      explanation:
        "That's the full weight matrix size. LoRA's entire point is to avoid training all these parameters.",
    },
  ]}
/>

<KeyTakeaway>
  <ul>
    <li>
      <strong>The low-rank hypothesis</strong>: weight
      changes during fine-tuning occupy a low-dimensional
      subspace, often rank 4-16 out of thousands
    </li>
    <li>
      <strong>LoRA decomposes</strong> the weight update as <MathBlock
        formula={"\\Delta W = BA"}
      />, where B is zero-initialized and A is random,
      ensuring training starts from pretrained behavior
    </li>
    <li>
      <strong>Parameter savings</strong> are dramatic: 0.1-1%
      of parameters for most configurations, reducing optimizer
      memory from tens of GB to tens of MB
    </li>
    <li>
      <strong>Rank selection</strong>: r=8-16 is the sweet
      spot for most tasks; the LoRA paper showed diminishing
      returns beyond r=4 for many benchmarks
    </li>
    <li>
      <strong>Merged inference</strong>: <MathBlock
        formula="W' = W + BA"
      /> gives zero-overhead inference, so the adapted model runs
      at the same speed as the base model
    </li>
  </ul>
</KeyTakeaway>

<section>
  <h2>References</h2>

  <PaperReference
    title="LoRA: Low-Rank Adaptation of Large Language Models"
    authors="Hu, Shen, Wallis, Allen-Zhu, Li, Wang, Wang, Chen"
    year="2021"
    url="https://arxiv.org/abs/2106.09685"
    type="paper"
  />

  <PaperReference
    title="Intrinsic Dimensionality Explains the Effectiveness of Language Model Fine-Tuning"
    authors="Aghajanyan, Gupta, Zettlemoyer"
    year="2020"
    url="https://arxiv.org/abs/2012.13255"
    type="paper"
  />

  <PaperReference
    title="DoRA: Weight-Decomposed Low-Rank Adaptation"
    authors="Liu, Wang, Yin, Molchanov, Wang, Cheng, Chen"
    year="2024"
    url="https://arxiv.org/abs/2402.09353"
    type="paper"
  />

  <PaperReference
    title="Practical Tips for Finetuning LLMs Using LoRA"
    authors="Sebastian Raschka"
    year="2023"
    url="https://magazine.sebastianraschka.com/p/practical-tips-for-finetuning-llms"
    type="blog"
  />
</section>
