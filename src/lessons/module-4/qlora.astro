---
// Module 4, Lesson 4.3: QLoRA: Quantized Low-Rank Adaptation
import KeyTakeaway from "../../components/KeyTakeaway.astro";
import PaperReference from "../../components/PaperReference.astro";
import MathBlock from "../../components/MathBlock.astro";
import Quiz from "../../components/Quiz.astro";
import RevealSection from "../../components/RevealSection.astro";
import Diagram from "../../components/Diagram.astro";
import GlossaryTooltip from "../../components/GlossaryTooltip.astro";
---

<section>
  <h2>Learning Objectives</h2>
  <p>After completing this lesson, you will be able to:</p>
  <ul>
    <li>
      Understand quantization: how floating-point numbers
      are compressed to lower precision (FP32, FP16, INT8,
      INT4)
    </li>
    <li>
      Explain NormalFloat4 (NF4) quantization and why it is
      optimal for normally-distributed weights
    </li>
    <li>
      Derive the memory savings of double quantization
    </li>
    <li>
      Calculate the total memory footprint of <GlossaryTooltip
        term="QLoRA"
      /> vs. full
      fine-tuning and standard <GlossaryTooltip term="LoRA" />
    </li>
  </ul>
</section>

<section>
  <h2>The Quantization Landscape</h2>

  <p>
    In the previous lesson, you saw how <GlossaryTooltip term="LoRA" /> slashes the number of
    trainable parameters. But even with LoRA, you still need to load the
    full base model into GPU memory, and for a 7B+ model that can be
    14 GB or more. What if you could shrink those weights dramatically
    before they even hit the GPU?
  </p>

  <p>
    That is what <strong>quantization</strong> does. Think of it like
    converting a high-resolution photo to a smaller JPEG: you lose some
    fine detail, but the file takes up far less space. In the context of
    <GlossaryTooltip term="LLM" />s, we quantize model weights from their native precision (FP32 or
    BF16) to lower-bit formats, trading a small amount of accuracy for
    dramatic memory savings.
  </p>

  <h3>Number Formats in Deep Learning</h3>

  <p>
    Before diving into the quantization math, it helps to see what
    you are compressing <em>from</em> and <em>to</em>. Each number
    format allocates a different number of bits to represent values.
    Fewer bits means less memory, but also less precision:
  </p>

  <Diagram
    diagramId="precision-formats"
    title="Numerical Precision Formats"
    autoplay={true}
    animationDuration={3000}
  >
    <div
      class="bg-[hsl(var(--card))] p-4 rounded w-full"
    >
      <div class="space-y-2">
        <div
          class="flex items-center gap-3"
          data-animate
          style="animation-delay: 0.2s"
        >
          <div
            class="w-16 text-right text-sm font-mono font-bold"
          >
            FP32
          </div>
          <div class="flex-1 flex gap-0.5">
            <div
              class="h-6 bg-[hsl(var(--diagram-red-solid))] rounded-l"
              style="width: 3.125%"
              title="sign"
            >
            </div>
            <div
              class="h-6 bg-[hsl(var(--diagram-amber-solid))]"
              style="width: 25%"
              title="exponent (8 bits)"
            >
            </div>
            <div
              class="h-6 bg-[hsl(var(--diagram-blue-solid))] rounded-r"
              style="width: 71.875%"
              title="mantissa (23 bits)"
            >
            </div>
          </div>
          <div class="w-20 text-xs text-[hsl(var(--muted-foreground))]">
            32 bits / 4 bytes
          </div>
        </div>
        <div
          class="flex items-center gap-3"
          data-animate
          style="animation-delay: 0.6s"
        >
          <div
            class="w-16 text-right text-sm font-mono font-bold"
          >
            BF16
          </div>
          <div class="flex-1 flex gap-0.5">
            <div
              class="h-6 bg-[hsl(var(--diagram-red-solid))] rounded-l"
              style="width: 6.25%"
              title="sign"
            >
            </div>
            <div
              class="h-6 bg-[hsl(var(--diagram-amber-solid))]"
              style="width: 50%"
              title="exponent (8 bits)"
            >
            </div>
            <div
              class="h-6 bg-[hsl(var(--diagram-blue-solid))] rounded-r"
              style="width: 43.75%"
              title="mantissa (7 bits)"
            >
            </div>
          </div>
          <div class="w-20 text-xs text-[hsl(var(--muted-foreground))]">
            16 bits / 2 bytes
          </div>
        </div>
        <div
          class="flex items-center gap-3"
          data-animate
          style="animation-delay: 1.0s"
        >
          <div
            class="w-16 text-right text-sm font-mono font-bold"
          >
            FP16
          </div>
          <div class="flex-1 flex gap-0.5">
            <div
              class="h-6 bg-[hsl(var(--diagram-red-solid))] rounded-l"
              style="width: 6.25%"
              title="sign"
            >
            </div>
            <div
              class="h-6 bg-[hsl(var(--diagram-amber-solid))]"
              style="width: 31.25%"
              title="exponent (5 bits)"
            >
            </div>
            <div
              class="h-6 bg-[hsl(var(--diagram-blue-solid))] rounded-r"
              style="width: 62.5%"
              title="mantissa (10 bits)"
            >
            </div>
          </div>
          <div class="w-20 text-xs text-[hsl(var(--muted-foreground))]">
            16 bits / 2 bytes
          </div>
        </div>
        <div
          class="flex items-center gap-3"
          data-animate
          style="animation-delay: 1.4s"
        >
          <div
            class="w-16 text-right text-sm font-mono font-bold"
          >
            INT8
          </div>
          <div class="flex-1 flex gap-0.5">
            <div
              class="h-6 bg-[hsl(var(--diagram-emerald-solid))] rounded"
              style="width: 50%"
              title="integer (8 bits)"
            >
            </div>
          </div>
          <div class="w-20 text-xs text-[hsl(var(--muted-foreground))]">
            8 bits / 1 byte
          </div>
        </div>
        <div
          class="flex items-center gap-3"
          data-animate
          style="animation-delay: 1.8s"
        >
          <div
            class="w-16 text-right text-sm font-mono font-bold"
          >
            NF4
          </div>
          <div class="flex-1 flex gap-0.5">
            <div
              class="h-6 bg-[hsl(var(--diagram-purple-solid))] rounded"
              style="width: 25%"
              title="4-bit quantized"
            >
            </div>
          </div>
          <div class="w-20 text-xs text-[hsl(var(--muted-foreground))]">
            4 bits / 0.5 bytes
          </div>
        </div>
      </div>
      <div
        class="mt-3 flex flex-wrap gap-4 text-xs text-[hsl(var(--muted-foreground))] justify-center"
        data-animate
        style="animation-delay: 2.4s"
      >
        <span class="flex items-center gap-1"
          ><span class="w-3 h-3 bg-[hsl(var(--diagram-red-solid))] rounded"></span> sign</span
        >
        <span class="flex items-center gap-1"
          ><span class="w-3 h-3 bg-[hsl(var(--diagram-amber-solid))] rounded"
          ></span> exponent</span
        >
        <span class="flex items-center gap-1"
          ><span class="w-3 h-3 bg-[hsl(var(--diagram-blue-solid))] rounded"></span> mantissa</span
        >
        <span class="flex items-center gap-1"
          ><span class="w-3 h-3 bg-[hsl(var(--diagram-emerald-solid))] rounded"
          ></span> integer</span
        >
        <span class="flex items-center gap-1"
          ><span class="w-3 h-3 bg-[hsl(var(--diagram-purple-solid))] rounded"
          ></span> quantized</span
        >
      </div>
    </div>
  </Diagram>

  <h3>How Quantization Works</h3>
  <p>
    The core idea is straightforward: take a range of floating-point
    values and map them onto a small, fixed set of integer levels.
    Imagine you have a ruler with 256 possible marks (8-bit) or
    just 16 marks (4-bit). You stretch the ruler across the range
    of your values, then snap each value to the nearest mark.
    For <MathBlock formula="b" />-bit quantization, the formula is:
  </p>
  <div class="equation-with-label">
    <MathBlock
      formula={"x_q = \\text{round}\\left(\\frac{x - \\text{min}(X)}{s}\\right), \\quad s = \\frac{\\text{max}(X) - \\text{min}(X)}{2^b - 1}"}
      display={true}
    />
    <div class="text-sm font-medium text-[hsl(var(--foreground))]">
      Quantization formula
    </div>
  </div>
  <p>
    Read this as: shift each value so the minimum maps to 0,
    divide by the step size <MathBlock formula="s" />, and
    round to the nearest integer. The scale factor <MathBlock
      formula="s"
    /> divides the full value range into <MathBlock
      formula={"2^b - 1"}
    /> equal steps (for example, 15 steps with 4-bit, 255 steps
    with 8-bit). The resulting integer <MathBlock formula="x_q" />
    is what actually gets stored in memory. To get back to
    a usable floating-point value, you dequantize:
  </p>
  <div class="equation-with-label">
    <MathBlock
      formula={"\\hat{x} = s \\cdot x_q + \\text{min}(X)"}
      display={true}
    />
    <div class="text-sm font-medium text-[hsl(var(--foreground))]">
      Dequantization formula
    </div>
  </div>
  <p>
    In plain English: multiply the stored integer by the step size
    and add back the offset you subtracted earlier. You get back an
    approximation of the original value, not an exact copy. The
    maximum error is half a step size, which is usually small enough
    that the model barely notices.
  </p>
  <p>
    This introduces <strong>quantization error</strong>
    <MathBlock formula={"|x - \\hat{x}|"} />, which depends
    on the bit width and the distribution of values being
    quantized.
  </p>
</section>

<section>
  <h2>NF4: Quantization Designed for Neural Networks</h2>

  <p>
    The quantization formula above spaces its levels evenly across the
    value range. That works fine if your values are spread evenly too.
    But here is the thing: neural network weights are <em>not</em>
    evenly spread. They follow a roughly <strong>normal
    distribution</strong> centered around zero, meaning most weights
    cluster near zero, with only a few large outliers in the tails.
    Equal spacing wastes precious quantization levels on the sparse
    tails while under-serving the crowded center.
  </p>

  <p>
    <strong>NormalFloat4 (NF4)</strong>, introduced in the
    <GlossaryTooltip term="QLoRA" /> paper, fixes this mismatch.
    Instead of spacing levels evenly, it places them at the
    <strong>quantiles of the standard normal distribution</strong>.
    Think of slicing the bell curve into 16 equal-area slices and
    putting one quantization level in the center of each slice:
  </p>

  <MathBlock
    formula={"q_i = \\Phi^{-1}\\left(\\frac{i + 0.5}{2^b}\\right), \\quad i = 0, 1, \\ldots, 2^b - 1"}
    display={true}
  />

  <p>
    Read this as: for each of the <MathBlock formula={"2^b"} />
    levels, find the point on the bell curve where a fraction
    <MathBlock formula={"(i + 0.5) / 2^b"} /> of the total area
    falls to the left. <MathBlock formula={"\\Phi^{-1}"} /> is the
    inverse CDF (probit function) of the standard normal
    (it converts a uniform probability into the corresponding
    z-score). For 4-bit (16 levels), the result is that levels
    cluster densely near zero, where most weights live, and spread
    out in the tails where values are rare.
  </p>

  <p>
    In short, NF4 is an information-theoretically optimal
    4-bit data type for normally distributed weights. It
    spaces quantization levels to match the bell curve of
    neural network weights, minimizing quantization error
    where it matters most. Empirically, it introduces less
    error than uniform INT4 quantization for transformer
    weights.
  </p>

  <RevealSection
    revealId="nf4-levels"
    title="NF4 Quantization Levels"
  >
    <div data-reveal-step>
      <h4>The 16 NF4 Levels</h4>
      <p>
        For 4-bit precision with zero as a quantization
        level, the actual NF4 values (normalized to [-1, 1])
        are:
      </p>
      <pre
        class="bg-[hsl(var(--muted))] p-3 rounded text-xs font-mono overflow-x-auto mt-2">[-1.0, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911, 0.0,
  0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230, 1.0]</pre>
      <p class="mt-2">
        Notice the denser spacing near zero: the gap between
        -0.0911 and 0.0 is 0.091, while the gap between
        0.7230 and 1.0 is 0.277. This matches the normal
        distribution's density.
      </p>
      <button
        type="button"
        class="mt-2 px-3 py-1 bg-[hsl(var(--primary))] text-[hsl(var(--primary-foreground))] rounded text-sm"
        data-reveal-button="1"
      >
        Next Step →
      </button>
    </div>

    <div data-reveal-step>
      <h4>Block-wise Quantization</h4>
      <p>
        Weights are quantized in blocks of 64 values. Each
        block has its own scale factor:
      </p>
      <MathBlock
        formula={"s = \\frac{\\text{absmax}(X_{\\text{block}})}{q_{\\text{max}}}"}
        display={true}
      />
      <p>
        The scale factor is simply the block's largest
        absolute value divided by the maximum quantization
        level. This maps the block's range onto the NF4
        levels. Per-block scaling handles layers with
        different weight magnitudes, reducing quantization
        error compared to per-tensor scaling.
      </p>
      <p class="mt-2 text-[hsl(var(--diagram-emerald-fg))] font-medium">
        Each block of 64 weights requires one FP32 scale
        factor (4 bytes overhead per 64 weights = 0.5 bits
        per weight).
      </p>
    </div>
  </RevealSection>
</section>

<section>
  <h2>Double Quantization</h2>

  <p>
    You might wonder: if each block of 64 weights needs its own FP32
    scale factor, how much overhead does that add? For a 7B model,
    that is roughly <MathBlock
      formula={"7 \\times 10^9 / 64 \\approx 109\\text{M}"}
    /> FP32 scale factors, consuming ~437 MB. That is a lot of
    bookkeeping, partially eating into the savings you just worked
    so hard to get.
  </p>

  <p>
    <strong>Double quantization</strong> solves this with a clever
    trick: quantize the scale factors themselves. The quantization
    constants from the first round are gathered into groups of 256,
    and each group is compressed to 8-bit precision with a single
    FP32 scale factor. It is quantization all the way down:
  </p>

  <div class="equation-sequence">
    <MathBlock
      formula={"\\text{Memory per weight} = 4\\text{ bits (NF4 weight)} + \\frac{8\\text{ bits (INT8 scale)}}{64} + \\frac{32\\text{ bits (FP32 second-level scale)}}{64 \\times 256}"}
      display={true}
    />

    <MathBlock
      formula={"= 4 + 0.125 + 0.002 \\approx 4.127 \\text{ bits per weight}"}
      display={true}
    />

    <p class="mt-2">
      In plain English: each weight costs 4 bits for its
      quantized NF4 value. On top of that, you pay a tiny
      fraction for the bookkeeping: the first-level INT8 scale
      is shared across 64 weights (adding 0.125 bits each),
      and the second-level FP32 scale is shared across
      64 x 256 = 16,384 weights (essentially free). Compare
      this to single quantization, where you pay much more
      overhead:
    </p>
    <MathBlock
      formula={"\\text{Single quantization} = 4 + \\frac{32}{64} = 4.5 \\text{ bits per weight}"}
      display={true}
    />
  </div>

  <p>
    With single quantization, each FP32 scale factor costs
    0.5 bits per weight, a significant overhead on top of
    the 4-bit weight itself. Double quantization saves <MathBlock
      formula="0.373"
    /> bits per parameter. For a 7B model: <MathBlock
      formula={"7 \\times 10^9 \\times 0.373 / 8 \\approx 326\\text{ MB}"}
    /> saved. This adds up.
  </p>
</section>

<section>
  <h2>QLoRA: Putting It All Together</h2>

  <p>
    Now you have all the pieces. <strong><GlossaryTooltip term="QLoRA" /></strong>
    stacks three innovations together, and the combination is
    what makes it possible to fine-tune a 65B parameter model
    on a single 48 GB GPU:
  </p>

  <ol>
    <li>
      <strong>4-bit NF4 quantization</strong> of the base model
      weights
    </li>
    <li>
      <strong>Double quantization</strong> of the quantization
      constants
    </li>
    <li>
      <strong>LoRA</strong> adapters trained in BF16 on top of
      the quantized base
    </li>
  </ol>

  <p>
    Here is how the forward pass works. The base model's frozen
    weights live in 4-bit NF4 format, while the small
    <GlossaryTooltip term="LoRA" /> adapter matrices stay in
    full BF16 precision. During each forward pass, the two
    contributions are summed:
  </p>
  <MathBlock
    formula={"h = \\text{dequant}(W_{\\text{NF4}}) \\cdot x + \\frac{\\alpha}{r} BAx"}
    display={true}
  />

  <p>
    Read this as: the output <MathBlock formula="h" /> has two
    parts. First, the NF4 weights are dequantized back to BF16
    on-the-fly and multiplied by the input <MathBlock formula="x" />.
    Second, the LoRA adapter computes its low-rank correction
    <MathBlock formula="BAx" />, scaled by <MathBlock
      formula={"\\alpha / r"}
    />. Only the LoRA matrices <MathBlock formula="B" /> and
    <MathBlock formula="A" /> receive gradient updates during
    training. The quantized base weights are never modified.
  </p>

  <h3>Paged Optimizers</h3>
  <p>
    QLoRA also introduces <strong>paged optimizers</strong>,
    which use NVIDIA unified memory to automatically page
    optimizer states between GPU and CPU memory. When GPU
    memory runs low during a gradient spike (e.g., a
    particularly long sequence), optimizer states are
    evicted to CPU RAM and paged back when needed. This
    prevents out-of-memory crashes at the cost of a small
    speed reduction.
  </p>
  <p
    class="text-sm text-[hsl(var(--diagram-amber-fg))] mt-2"
  >
    <strong>Trade-off</strong>: While this prevents OOM
    errors, moving data over the PCIe bus (between GPU and
    CPU) is much slower than internal GPU memory access.
    Training will be slightly slower when paging is active,
    but it enables training larger models on consumer
    hardware.
  </p>
</section>

<section>
  <h2>Memory Comparison</h2>

  <p>
    All of these techniques sound promising in theory, but do
    the numbers actually work out? Let's put concrete figures
    on it. A 7B parameter model in FP16 requires 14 GB of GPU
    memory just to hold the weights. With QLoRA's 4-bit
    quantization, the base model shrinks to ~3.5 GB, plus
    ~100 MB for the LoRA adapters and optimizer, fitting
    comfortably on a single 8 GB consumer GPU. The following
    breakdown shows exactly where memory goes under each
    approach.
  </p>

  <RevealSection
    revealId="memory-comparison"
    title="LLaMA-2 7B: Memory Across Methods"
  >
    <div data-reveal-step>
      <h4>Full Fine-Tuning (FP16 + FP32 optimizer)</h4>
      <p>
        Every parameter needs storage for weights, optimizer
        state, and gradients:
      </p>
      <MathBlock
        formula={"\\text{Weights: } 6.74\\text{B} \\times 6 = 40.4\\text{ GB}"}
        display={true}
      />
      <MathBlock
        formula={"\\text{Optimizer: } 6.74\\text{B} \\times 8 = 53.9\\text{ GB}"}
        display={true}
      />
      <MathBlock
        formula={"\\text{Gradients: } 6.74\\text{B} \\times 2 = 13.5\\text{ GB}"}
        display={true}
      />
      <MathBlock
        formula={"\\textbf{Total: } \\sim 108\\text{ GB}"}
        display={true}
      />
      <p>
        The total is 6 + 8 + 2 = 16 bytes per parameter.
        Optimizer states alone (Adam's momentum and
        variance) consume more memory than the weights
        themselves.
      </p>
      <p
        class="text-[hsl(var(--foreground))] font-medium"
      >
        Requires: 2x A100 80GB
      </p>
      <button
        type="button"
        class="mt-2 px-3 py-1 bg-[hsl(var(--primary))] text-[hsl(var(--primary-foreground))] rounded text-sm"
        data-reveal-button="1"
      >
        Next: LoRA →
      </button>
    </div>

    <div data-reveal-step>
      <h4>LoRA (FP16 base + FP32 adapter optimizer)</h4>
      <p>
        The base model is frozen in FP16, so no optimizer
        states or gradients are needed for it. Only the tiny
        LoRA adapters incur optimizer overhead:
      </p>
      <MathBlock
        formula={"\\text{Base weights (frozen, FP16): } 6.74\\text{B} \\times 2 = 13.5\\text{ GB}"}
        display={true}
      />
      <MathBlock
        formula={"\\text{LoRA params (r=8, all attn): } \\sim 8.4\\text{M}"}
        display={true}
      />
      <MathBlock
        formula={"\\text{LoRA optimizer: } 8.4\\text{M} \\times 12 = 0.1\\text{ GB}"}
        display={true}
      />
      <MathBlock
        formula={"\\textbf{Total: } \\sim 14\\text{ GB}"}
        display={true}
      />
      <p>
        The 12 bytes per LoRA parameter covers the parameter
        itself (2 bytes BF16), gradients (2 bytes), and Adam
        states (8 bytes FP32). But 8.4M is so small that
        this only adds ~100 MB.
      </p>
      <p
        class="text-[hsl(var(--foreground))] font-medium"
      >
        Requires: 1x A100 40GB (or a high-end consumer GPU)
      </p>
      <button
        type="button"
        class="mt-2 px-3 py-1 bg-[hsl(var(--primary))] text-[hsl(var(--primary-foreground))] rounded text-sm"
        data-reveal-button="2"
      >
        Next: QLoRA →
      </button>
    </div>

    <div data-reveal-step>
      <h4>QLoRA (NF4 base + BF16 adapters)</h4>
      <p>
        The base model is compressed to 4-bit NF4 (with
        double quantization adding ~0.127 bits per weight,
        so ~0.52 bytes per parameter total). The LoRA
        adapters remain in BF16:
      </p>
      <MathBlock
        formula={"\\text{Base weights (NF4): } 6.74\\text{B} \\times 0.52 = 3.5\\text{ GB}"}
        display={true}
      />
      <MathBlock
        formula={"\\text{LoRA params + optimizer: } \\sim 0.1\\text{ GB}"}
        display={true}
      />
      <MathBlock
        formula={"\\text{Activations + overhead: } \\sim 2\\text{ GB}"}
        display={true}
      />
      <MathBlock
        formula={"\\textbf{Total: } \\sim 6\\text{ GB}"}
        display={true}
      />
      <p>
        The 4-bit base model is roughly 4x smaller than
        FP16. Combined with LoRA's minimal adapter overhead,
        the total is 18x less than full fine-tuning.
      </p>
      <p
        class="text-[hsl(var(--foreground))] font-medium"
      >
        Requires: 1x RTX 3090 / 4090 (consumer GPU)
      </p>
      <p
        class="mt-2 text-[hsl(var(--muted-foreground))]"
      >
        QLoRA reduces memory by <strong>18x</strong> compared
        to full fine-tuning, with less than 1% quality degradation
        on most benchmarks.
      </p>
    </div>
  </RevealSection>
</section>

<section>
  <h2>Quality: Does Quantization Hurt?</h2>

  <p>
    At this point you might be skeptical: crushing weights down to 4
    bits sounds aggressive. Surely quality must suffer? The QLoRA
    paper's key finding says otherwise: <strong>4-bit QLoRA matches
    16-bit full fine-tuning performance</strong> on a wide range of
    benchmarks. The authors fine-tuned LLaMA-65B using QLoRA on the
    OASST1 dataset and achieved state-of-the-art chatbot performance
    (at the time), surpassing all previous models on the Vicuna
    benchmark.
  </p>

  <p>
    Why does extreme quantization work so well? Two complementary
    reasons:
  </p>
  <ul>
    <li>
      <strong>LoRA compensates</strong>: The BF16 LoRA
      adapters can learn to correct any errors introduced by
      quantization. The adapters effectively act as a
      "quantization-aware" correction layer.
    </li>
    <li>
      <strong>NF4 is near-optimal</strong>: By matching the
      weight distribution, NF4 minimizes information loss
      per bit. The quantization error is as small as
      theoretically possible for 4 bits.
    </li>
  </ul>

  <Quiz
    question="Why does NF4 quantization outperform standard uniform INT4 quantization for neural network weights?"
    quizId="nf4-advantage"
    options={[
      {
        id: "a",
        text: "NF4 uses more total bits when you include the scale factors",
        correct: false,
        explanation:
          "NF4 actually uses fewer effective bits than standard approaches due to double quantization.",
      },
      {
        id: "b",
        text: "NF4 places quantization levels at normal distribution quantiles, matching the weight distribution for minimal error",
        correct: true,
        explanation:
          "Correct! Neural network weights are approximately normally distributed. NF4 places more quantization levels near zero (where most weights cluster) and fewer in the tails, achieving information-theoretically optimal quantization for this distribution.",
      },
      {
        id: "c",
        text: "NF4 uses a different rounding strategy that is more accurate",
        correct: false,
        explanation:
          "NF4 uses standard nearest-value rounding. The improvement comes from the placement of quantization levels, not the rounding method.",
      },
      {
        id: "d",
        text: "NF4 only quantizes unimportant weights and keeps important ones in full precision",
        correct: false,
        explanation:
          "NF4 quantizes all weights uniformly to 4 bits. It does not make importance-based decisions.",
      },
    ]}
  />
</section>

<KeyTakeaway>
  <ul>
    <li>
      <strong>Quantization</strong> compresses model weights from
      32/16 bits to 8/4 bits, with memory savings proportional
      to the compression ratio
    </li>
    <li>
      <strong>NF4</strong> places quantization levels at normal
      distribution quantiles, achieving optimal 4-bit quantization
      for the typically Gaussian weight distributions in transformers
    </li>
    <li>
      <strong>Double quantization</strong> further compresses
      the quantization scale factors themselves, reducing overhead
      from 0.5 to 0.127 bits per weight
    </li>
    <li>
      <strong>QLoRA</strong> combines NF4 quantization + double
      quantization + LoRA adapters to fine-tune a 7B model in
      ~6 GB (vs. ~108 GB for full fine-tuning)
    </li>
    <li>
      <strong>Quality preservation</strong>: 4-bit QLoRA
      matches 16-bit full fine-tuning on most benchmarks,
      because LoRA adapters compensate for quantization
      error
    </li>
  </ul>
</KeyTakeaway>

<section>
  <h2>References</h2>

  <PaperReference
    title="QLoRA: Efficient Finetuning of Quantized LLMs"
    authors="Dettmers, Pagnoni, Holtzman, Zettlemoyer"
    year="2023"
    url="https://arxiv.org/abs/2305.14314"
    type="paper"
  />

  <PaperReference
    title="LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale"
    authors="Dettmers, Lewis, Belkada, Zettlemoyer"
    year="2022"
    url="https://arxiv.org/abs/2208.07339"
    type="paper"
  />

  <PaperReference
    title="GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers"
    authors="Frantar, Ashkboos, Hoefler, Alistarh"
    year="2022"
    url="https://arxiv.org/abs/2210.17323"
    type="paper"
  />

  <PaperReference
    title="The case for 4-bit precision: k-bit Inference Scaling Laws"
    authors="Dettmers, Zettlemoyer"
    year="2022"
    url="https://arxiv.org/abs/2212.09720"
    type="paper"
  />
</section>
