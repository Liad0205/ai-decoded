---
// Module 7, Lesson 7.1: Foundations of Effective Prompting
import KeyTakeaway from '../../components/KeyTakeaway.astro';
import PaperReference from '../../components/PaperReference.astro';
import Diagram from '../../components/Diagram.astro';
import Quiz from '../../components/Quiz.astro';
import MathBlock from '../../components/MathBlock.astro';
import GlossaryTooltip from '../../components/GlossaryTooltip.astro';
---

<section>
  <h2>Learning Objectives</h2>
  <p>After completing this lesson, you will be able to:</p>
  <ul>
    <li>Explain how <GlossaryTooltip term="LLM" />s process and respond to prompts at the token level</li>
    <li>Distinguish between zero-shot, one-shot, and few-shot prompting and select the right approach</li>
    <li>Understand the mechanics of temperature, top-p, and top-k sampling and their effects on output</li>
    <li>Apply structured prompting techniques like role assignment and format specification</li>
    <li>Reason about why certain prompts succeed or fail based on the model's autoregressive nature</li>
  </ul>
</section>

<section>
  <h2>How LLMs Process Prompts</h2>

  <p>
    To write effective prompts, you need a mental model of what happens inside the LLM when it processes your input. Prompt engineering is not magic -- it is applied understanding of autoregressive language modeling.
  </p>

  <h3>The Autoregressive Generation Loop</h3>
  <p>
    An LLM generates text one token at a time. At each step, it takes the <em>entire</em> preceding context (your prompt plus any tokens already generated) and produces a probability distribution over the vocabulary for the next token. This means:
  </p>

  <ul>
    <li><strong>Order matters</strong>: The model processes tokens left-to-right. Information at the beginning of your prompt is "seen" by every subsequent token through the attention mechanism.</li>
    <li><strong>No backtracking</strong>: Once a token is generated, it cannot be revised. The model commits to each token and all subsequent tokens are conditioned on it. A mistake early in generation propagates.</li>
    <li><strong>Context is everything</strong>: The model has no memory beyond the current context window. Every piece of information it needs must be in the prompt or the generated prefix.</li>
  </ul>

  <h3>Tokenization: The Hidden Layer</h3>
  <p>
    Before the model sees your text, it is converted to tokens via a tokenizer (typically <GlossaryTooltip term="BPE" /> -- Byte Pair Encoding). This has practical consequences:
  </p>

  <ul>
    <li><strong>Words are not tokens</strong>: Common words like "the" are single tokens, but uncommon words may be split. "Anthropic" might become ["Anthrop", "ic"]. Code identifiers, non-English text, and numbers are often split unpredictably.</li>
    <li>
        <strong>Arithmetic is tokenization-dependent</strong>: The number "12345" might be tokenized as ["123", "45"] or ["1", "2345"], making digit-level reasoning unreliable. This partly explains why LLMs struggle with arithmetic.
        <div class="mt-2 bg-[hsl(var(--muted))] p-2 rounded text-xs font-mono">
          Input: 9999 (1 token) vs. 10000 (2 tokens: "10", "000")<br/>
          To the model, "10000" looks like two separate concepts, confusing arithmetic operations.
        </div>
    </li>
    <li><strong>Whitespace and formatting matter</strong>: Leading spaces, newlines, and punctuation affect tokenization, which can subtly change model behavior.</li>
  </ul>

  <h3>The Attention Pattern</h3>
  <p>
    Within the context window, the model uses attention to determine which parts of the prompt are most relevant to generating each new token. Key implications for prompt design:
  </p>

  <ul>
    <li><strong>Primacy and recency effects</strong>: Information at the very beginning and very end of the context tends to receive more attention. Critical instructions placed in the middle of a long prompt may be "lost" -- a phenomenon documented as the "lost in the middle" effect.</li>
    <li><strong>Explicit structure helps</strong>: Headers, numbered lists, and clear delimiters help the model's attention focus on relevant sections.</li>
    <li><strong>Context window capacity</strong>: Modern models support context windows from 128K to 1M+ tokens (e.g., <GlossaryTooltip term="GPT" />-4 Turbo at 128K, Claude 3.5 at 200K, Gemini 1.5 Pro at 1M), though very long contexts increase cost and can degrade performance on information retrieval ("lost in the middle" becomes more pronounced).</li>
  </ul>
</section>

<Diagram diagramId="prompt-processing" title="How an LLM Processes a Prompt" autoplay={true} animationDuration={5000}>
  <div class="bg-[hsl(var(--card))] p-4 rounded">
    <div class="flex flex-col gap-4">
      <!-- Step 1: Prompt text -->
      <div class="flex items-center gap-4" data-animate style="animation-delay: 0.3s">
        <div class="w-28 text-sm font-semibold text-[hsl(var(--muted-foreground))] flex-shrink-0">Input Text</div>
        <div class="flex-1 px-4 py-2 bg-[hsl(var(--muted))] rounded font-mono text-sm">"Explain quantum computing simply"</div>
      </div>

      <!-- Step 2: Tokenization -->
      <div class="flex items-center gap-4" data-animate style="animation-delay: 1s">
        <div class="w-28 text-sm font-semibold text-[hsl(var(--muted-foreground))] flex-shrink-0">Tokenize</div>
        <div class="flex-1 flex gap-1 flex-wrap">
          <span class="px-2 py-1 bg-[hsl(var(--diagram-indigo-bg))] rounded text-xs font-mono">Explain</span>
          <span class="px-2 py-1 bg-[hsl(var(--diagram-indigo-bg))] rounded text-xs font-mono">quantum</span>
          <span class="px-2 py-1 bg-[hsl(var(--diagram-indigo-bg))] rounded text-xs font-mono">computing</span>
          <span class="px-2 py-1 bg-[hsl(var(--diagram-indigo-bg))] rounded text-xs font-mono">simply</span>
        </div>
      </div>

      <!-- Step 3: Transformer -->
      <div class="flex items-center gap-4" data-animate style="animation-delay: 2s">
        <div class="w-28 text-sm font-semibold text-[hsl(var(--muted-foreground))] flex-shrink-0">Attention</div>
        <div class="flex-1 px-4 py-3 bg-[hsl(var(--diagram-purple-bg))] border border-[hsl(var(--diagram-purple-border))] rounded text-sm text-center">
          Transformer layers compute attention across all tokens
        </div>
      </div>

      <!-- Step 4: Next-token distribution -->
      <div class="flex items-center gap-4" data-animate style="animation-delay: 3s">
        <div class="w-28 text-sm font-semibold text-[hsl(var(--muted-foreground))] flex-shrink-0">Predict</div>
        <div class="flex-1 flex gap-2 items-end">
          <div class="flex flex-col items-center">
            <div class="text-xs text-[hsl(var(--muted-foreground))] mb-1">0.35</div>
            <div class="w-14 bg-[hsl(var(--diagram-indigo-solid))] rounded-t" style="height: 70px"></div>
            <div class="text-xs mt-1 font-mono">Quantum</div>
          </div>
          <div class="flex flex-col items-center">
            <div class="text-xs text-[hsl(var(--muted-foreground))] mb-1">0.25</div>
            <div class="w-14 bg-[hsl(var(--diagram-indigo-solid))] rounded-t" style="height: 50px"></div>
            <div class="text-xs mt-1 font-mono">At</div>
          </div>
          <div class="flex flex-col items-center">
            <div class="text-xs text-[hsl(var(--muted-foreground))] mb-1">0.15</div>
            <div class="w-14 bg-[hsl(var(--diagram-indigo-border))] rounded-t" style="height: 30px"></div>
            <div class="text-xs mt-1 font-mono">In</div>
          </div>
          <div class="flex flex-col items-center">
            <div class="text-xs text-[hsl(var(--muted-foreground))] mb-1">0.25</div>
            <div class="w-14 bg-[hsl(var(--diagram-indigo-border))] rounded-t" style="height: 50px"></div>
            <div class="text-xs mt-1 font-mono">...</div>
          </div>
        </div>
      </div>

      <!-- Step 5: Sample -->
      <div class="flex items-center gap-4" data-animate style="animation-delay: 4s">
        <div class="w-28 text-sm font-semibold text-[hsl(var(--muted-foreground))] flex-shrink-0">Sample</div>
        <div class="flex-1 px-4 py-2 bg-[hsl(var(--diagram-emerald-bg))] border border-[hsl(var(--diagram-emerald-border))] rounded text-sm">
          Select next token based on sampling parameters (temperature, top-p) and repeat
        </div>
      </div>
    </div>
  </div>
</Diagram>

<section>
  <h2>Zero-Shot, One-Shot, and Few-Shot Prompting</h2>

  <p>
    The number of examples you provide in a prompt fundamentally changes how the model approaches the task. This framework was formalized in the <GlossaryTooltip term="GPT" />-3 paper and remains central to prompt engineering.
  </p>

  <h3>Zero-Shot Prompting</h3>
  <p>
    In <strong>zero-shot prompting</strong>, you describe the task without providing any examples. You rely entirely on the model's pretrained knowledge and instruction-tuning to understand what you want.
  </p>

  <pre><code>Classify the sentiment of this review as positive, negative, or neutral:

"The battery life is incredible but the screen is too dim."

Sentiment:</code></pre>

  <p>
    Zero-shot works well for tasks the model has seen extensively during training: summarization, translation, simple classification, and general Q&A. It fails on tasks with unusual formats, domain-specific conventions, or ambiguous requirements.
  </p>

  <h3>One-Shot Prompting</h3>
  <p>
    Providing a <strong>single example</strong> disambiguates the task format and establishes the expected output structure. Even one example can dramatically improve performance:
  </p>

  <pre><code>Classify the sentiment and identify the key aspect:

Review: "Amazing camera quality but the phone overheats."
Sentiment: Mixed
Aspect: Camera (positive), Thermal (negative)

Review: "The battery life is incredible but the screen is too dim."
Sentiment:</code></pre>

  <h3>Few-Shot Prompting</h3>
  <p>
    <strong>Few-shot prompting</strong> provides 2-8 examples that demonstrate the desired input-output pattern. The model performs in-context learning -- adapting its behavior based on the pattern in the examples without any weight updates.
  </p>

  <p>
    <strong>Why does in-context learning work?</strong> This is an active area of research. Leading hypotheses include:
  </p>
  <ul>
    <li><strong>Implicit Bayesian inference</strong>: The model identifies which "task" the examples are drawn from and applies the corresponding learned mapping</li>
    <li><strong>Induction heads</strong>: Specific attention patterns that copy patterns from earlier in the context to predict continuations</li>
    <li><strong>Task vectors</strong>: Examples create a direction in activation space that steers the model toward the desired behavior</li>
  </ul>

  <h3>Best Practices for Few-Shot Examples</h3>
  <ul>
    <li><strong>Diversity</strong>: Cover edge cases and varied inputs, not just the easy cases</li>
    <li><strong>Consistency</strong>: Use identical formatting across all examples -- the model is extremely sensitive to format variations</li>
    <li><strong>Order effects</strong>: The order of examples matters. Recent research shows that placing harder examples last can improve performance</li>
    <li><strong>Label balance</strong>: For classification, balance positive and negative examples to avoid biasing the model toward one class</li>
    <li><strong>Diminishing returns</strong>: Performance typically plateaus at 4-8 examples. More examples consume context window without proportional benefit</li>
  </ul>
</section>

<Quiz
  question="A developer uses few-shot prompting with 5 examples that are all positive sentiment. When classifying a new ambiguous review, the model consistently labels it as positive. What is the most likely cause?"
  quizId="few-shot-bias"
  options={[
    {
      id: "a",
      text: "The model has a bug in its attention mechanism",
      correct: false,
      explanation: "This is not a model bug -- it is a prompting issue. The model's behavior is a rational response to the biased examples."
    },
    {
      id: "b",
      text: "The examples created a label distribution bias, making the model favor the majority class",
      correct: true,
      explanation: "Correct! Few-shot examples implicitly define a prior distribution over labels. With 5/5 positive examples, the model learns that 'positive' is the likely output in this context. Balancing examples across classes resolves this."
    },
    {
      id: "c",
      text: "The context window is too small to hold all examples",
      correct: false,
      explanation: "5 examples easily fit in any modern context window. The issue is the content of the examples, not the quantity."
    },
    {
      id: "d",
      text: "Few-shot prompting only works for generative tasks, not classification",
      correct: false,
      explanation: "Few-shot prompting works well for classification -- it was one of the first demonstrated applications in the GPT-3 paper."
    }
  ]}
/>

<section>
  <h2>Sampling Parameters: Controlling Randomness</h2>

  <p>
    After the model computes a probability distribution over the next token, <strong>sampling parameters</strong> determine how that distribution is used to select the actual output token. Understanding these parameters is critical for getting consistent, high-quality outputs.

  </p>

  <h3>Temperature</h3>
  <p>
    Temperature (<MathBlock formula={"\\tau"} />) scales the logits before softmax. Given logits <MathBlock formula="z_i" />, the sampling probability becomes:
  </p>

  <MathBlock formula={"P(token_i) = \\frac{\\exp(z_i / \\tau)}{\\sum_j \\exp(z_j / \\tau)}"} display={true} />

  <ul>
    <li><strong><MathBlock formula={"\\tau \\to 0"} /> (greedy/argmax)</strong>: The distribution collapses to a point mass on the highest-probability token, making generation deterministic. The model always picks the single most likely next token, producing identical output for identical input.</li>
    <li><strong><MathBlock formula={"\\tau = 1.0"} /> (neutral)</strong>: The raw distribution is used as-is. Moderate diversity.</li>
    <li><strong><MathBlock formula={"\\tau \\to \\infty"} /> (high temperature)</strong>: The distribution approaches uniform over the vocabulary, making all tokens equally likely regardless of the model's learned preferences. In practice, values above 1.0 (e.g., 1.5) substantially flatten the distribution, increasing diversity but raising the risk of incoherent output.</li>
  </ul>

  <p>
    <strong>Practical guidance</strong>: Use <MathBlock formula={"\\tau = 0"} /> for deterministic output (exact repeatability); <MathBlock formula={"\\tau = 0.1"} /> to 0.3 for slight diversity in factual Q&A, code generation, and structured output. Use <MathBlock formula={"\\tau = 0.7"} /> to 1.0 for creative writing, brainstorming, and exploration.
  </p>

  <h3>Top-p (Nucleus Sampling)</h3>
  <p>
    Instead of sampling from the full vocabulary, <strong>top-p sampling</strong> considers only the smallest set of tokens whose cumulative probability exceeds threshold p. For example, with top-p = 0.9:
  </p>
  <ol>
    <li>Sort tokens by probability (descending)</li>
    <li>Add tokens until cumulative probability reaches 0.9</li>
    <li>Renormalize and sample from only these tokens</li>
  </ol>

  <p>
    Top-p is <em>adaptive</em>: when the model is confident (one token dominates), few tokens are considered. When uncertain, more tokens are included. This makes it generally more robust than fixed top-k.
  </p>

  <h3>Top-k Sampling</h3>
  <p>
    <strong>Top-k</strong> restricts sampling to the k highest-probability tokens. Simple but not adaptive -- with k=50, the model always considers exactly 50 tokens regardless of whether 3 or 300 are reasonable.
  </p>

  <h3>Interaction Between Parameters</h3>
  <p>
    Temperature, top-p, and top-k can be combined. A common production configuration:
  </p>
  <ul>
    <li><strong>Factual/deterministic tasks</strong>: temperature=0 (effectively disables top-p and top-k)</li>
    <li><strong>General use</strong>: temperature=0.7, top-p=0.9</li>
    <li><strong>Creative tasks</strong>: temperature=1.0, top-p=0.95</li>
  </ul>

  <p>
    <strong>Important</strong>: Setting temperature=0 with the same prompt should produce the same output (modulo floating-point nondeterminism in some implementations). This is essential for reproducible testing.
  </p>
</section>

<Diagram diagramId="temperature-effect" title="Effect of Temperature on Token Distribution" autoplay={true} animationDuration={4000}>
  <div class="bg-[hsl(var(--card))] p-4 rounded">
    <div class="grid grid-cols-3 gap-6">
      <!-- Low temperature -->
      <div class="text-center" data-animate style="animation-delay: 0.5s">
        <div class="font-semibold text-sm mb-3">Low Temp (0.1)</div>
        <div class="flex justify-center items-end gap-1 h-32">
          <div class="w-8 bg-[hsl(var(--diagram-indigo-solid))] rounded-t" style="height: 120px">
            <div class="text-xs text-white pt-1">0.92</div>
          </div>
          <div class="w-8 bg-[hsl(var(--diagram-indigo-border))] rounded-t" style="height: 12px"></div>
          <div class="w-8 bg-[hsl(var(--diagram-indigo-border))] rounded-t" style="height: 4px"></div>
          <div class="w-8 bg-[hsl(var(--diagram-indigo-bg))] rounded-t" style="height: 2px"></div>
        </div>
        <div class="text-xs text-[hsl(var(--muted-foreground))] mt-2">Nearly deterministic</div>
      </div>

      <!-- Medium temperature -->
      <div class="text-center" data-animate style="animation-delay: 1.5s">
        <div class="font-semibold text-sm mb-3">Med Temp (0.7)</div>
        <div class="flex justify-center items-end gap-1 h-32">
          <div class="w-8 bg-[hsl(var(--diagram-indigo-solid))] rounded-t" style="height: 72px">
            <div class="text-xs text-white pt-1">0.55</div>
          </div>
          <div class="w-8 bg-[hsl(var(--diagram-indigo-solid))] rounded-t" style="height: 36px"></div>
          <div class="w-8 bg-[hsl(var(--diagram-indigo-border))] rounded-t" style="height: 16px"></div>
          <div class="w-8 bg-[hsl(var(--diagram-indigo-border))] rounded-t" style="height: 8px"></div>
        </div>
        <div class="text-xs text-[hsl(var(--muted-foreground))] mt-2">Balanced diversity</div>
      </div>

      <!-- High temperature -->
      <div class="text-center" data-animate style="animation-delay: 2.5s">
        <div class="font-semibold text-sm mb-3">High Temp (1.5)</div>
        <div class="flex justify-center items-end gap-1 h-32">
          <div class="w-8 bg-[hsl(var(--diagram-indigo-solid))] rounded-t" style="height: 42px">
            <div class="text-xs text-white pt-1">0.30</div>
          </div>
          <div class="w-8 bg-[hsl(var(--diagram-indigo-solid))] rounded-t" style="height: 36px"></div>
          <div class="w-8 bg-[hsl(var(--diagram-indigo-border))] rounded-t" style="height: 28px"></div>
          <div class="w-8 bg-[hsl(var(--diagram-indigo-border))] rounded-t" style="height: 22px"></div>
        </div>
        <div class="text-xs text-[hsl(var(--muted-foreground))] mt-2">Highly random</div>
      </div>
    </div>
  </div>
</Diagram>

<section>
  <h2>Fundamental Prompting Techniques</h2>

  <h3>Role Assignment (System Prompts)</h3>
  <p>
    Assigning a role activates relevant knowledge and behavioral patterns the model learned during training. This is not anthropomorphization -- it is a mechanism to steer which parts of the model's learned distribution are most active.
  </p>

  <pre><code>System: You are an experienced database administrator with deep expertise
in PostgreSQL performance optimization. You give specific, actionable advice
backed by explain-analyze output interpretation.

User: My query is running slowly on a table with 10M rows...</code></pre>

  <p>
    Roles work because the model's training data contains text written by people in specific roles. Activating "database administrator" steers outputs toward patterns seen in DBA-authored content. The more specific the role, the more focused the outputs.
  </p>

  <h3>Format Specification</h3>
  <p>
    Explicitly specifying the output format dramatically improves consistency. The model generates tokens that match patterns it has seen in training, so providing a template reduces ambiguity:
  </p>

  <pre><code>Analyze this code for security vulnerabilities. Respond in this exact format:

## Vulnerabilities Found
1. [SEVERITY: HIGH/MEDIUM/LOW] [Vulnerability name]
   - Location: [file:line]
   - Description: [one sentence]
   - Fix: [one sentence]

## Summary
- Total: [N] vulnerabilities
- Critical action needed: [yes/no]</code></pre>

  <h3>Delimiter-Based Structuring</h3>
  <p>
    Use clear delimiters to separate different parts of your prompt, especially when including user-provided data. This helps the model distinguish instructions from content:
  </p>

  <pre><code>Summarize the following article in 3 bullet points.

---ARTICLE START---
[article text here]
---ARTICLE END---

Provide your summary:</code></pre>

  <p>
    Delimiters serve a dual purpose: they improve model comprehension <em>and</em> provide a basic defense against prompt injection (more on this in Lesson 7.3).
  </p>

  <h3>Constraint Specification</h3>
  <p>
    Negative constraints ("do not") and positive constraints ("only") bound the output space. Be explicit about what you want and do not want:
  </p>

  <ul>
    <li><strong>Length constraints</strong>: "Respond in exactly 3 sentences" is more effective than "be brief"</li>
    <li><strong>Content constraints</strong>: "Only discuss the technical aspects, not the business implications"</li>
    <li><strong>Format constraints</strong>: "Output valid JSON with no markdown formatting or code blocks"</li>
    <li><strong>Behavioral constraints</strong>: "If you are unsure, say 'I don't have enough information' rather than guessing"</li>
  </ul>
</section>

<Quiz
  question="You are building a system that uses an LLM to extract structured data from invoices. You need consistent JSON output. Which approach is most reliable?"
  quizId="structured-output"
  options={[
    {
      id: "a",
      text: "Ask the model to 'return the data as JSON' with no further specification",
      correct: false,
      explanation: "This is too vague. The model may include markdown code blocks, use inconsistent field names, or omit fields. Always specify the exact schema."
    },
    {
      id: "b",
      text: "Provide a few-shot example of the exact JSON schema, use temperature=0, and include the schema specification in the prompt",
      correct: true,
      explanation: "Correct! Combining a schema specification, concrete examples, and deterministic sampling (temperature=0) maximizes output consistency. The schema tells the model what fields to extract, examples disambiguate formatting, and low temperature reduces variation."
    },
    {
      id: "c",
      text: "Use high temperature (1.5) so the model explores more possible interpretations of the invoice",
      correct: false,
      explanation: "High temperature increases randomness, which is the opposite of what you want for consistent structured output. You want deterministic, repeatable extraction."
    },
    {
      id: "d",
      text: "Simply provide a very long, detailed system prompt describing JSON format in prose",
      correct: false,
      explanation: "Prose descriptions alone are less effective than concrete examples. Models learn patterns from examples far more reliably than from prose instructions."
    }
  ]}
/>

<section>
  <h2>Common Prompting Pitfalls</h2>

  <h3>The Sycophancy Trap</h3>
  <p>
    Models fine-tuned with <GlossaryTooltip term="RLHF" /> tend toward agreeableness. If your prompt implies an answer, the model is likely to confirm it rather than challenge it. Compare:
  </p>
  <ul>
    <li><strong>Leading</strong>: "This approach using microservices is better than a monolith, right?"</li>
    <li><strong>Neutral</strong>: "Compare microservices and monolith architectures for this use case. What are the tradeoffs?"</li>
  </ul>

  <h3>Instruction-Following vs. Completion</h3>
  <p>
    Base models (not instruction-tuned) complete text. Instruction-tuned models follow instructions. Writing prompts that work for both requires understanding which mode you are targeting. Most production use cases involve instruction-tuned models, but the completion behavior still leaks through -- the model always wants to continue the pattern it sees.
  </p>

  <h3>The "Do Not" Problem</h3>
  <p>
    Negative instructions are less reliable than positive ones. "Do not mention pricing" draws attention to pricing in the model's processing. When possible, rephrase negatives as positives: "Focus exclusively on technical features" rather than "Do not discuss pricing."
  </p>
</section>

<KeyTakeaway>
  <ul>
    <li><strong>LLMs generate tokens autoregressively</strong> -- each token is conditioned on the entire preceding context, with no backtracking. This means prompt structure and order directly affect output quality.</li>
    <li><strong>Few-shot prompting</strong> enables in-context learning: providing examples teaches the model your desired pattern without any weight updates. Balance examples across classes and keep formatting consistent.</li>
    <li><strong>Temperature controls randomness</strong>: low values (0-0.3) for factual/structured tasks, higher values (0.7-1.0) for creative tasks. Top-p adaptively limits the sampling vocabulary.</li>
    <li><strong>Structure your prompts</strong> with roles, delimiters, format specifications, and explicit constraints. The model cannot read your mind -- clarity in the prompt produces clarity in the output.</li>
    <li><strong>Avoid leading questions and vague negatives</strong>. Sycophancy bias and the unreliability of negative instructions are predictable failure modes you can design around.</li>
  </ul>
</KeyTakeaway>

<section>
  <h2>References</h2>

  <PaperReference
    title="Language Models are Few-Shot Learners (GPT-3)"
    authors="Brown, Mann, Ryder, et al."
    year="2020"
    url="https://arxiv.org/abs/2005.14165"
    type="paper"
  />

  <PaperReference
    title="The Nucleus Sampling Paper: The Curious Case of Neural Text Degeneration"
    authors="Holtzman, Buys, Du, Forbes, Choi"
    year="2020"
    url="https://arxiv.org/abs/1904.09751"
    type="paper"
  />

  <PaperReference
    title="Lost in the Middle: How Language Models Use Long Contexts"
    authors="Liu, Lin, Hewitt, Paranjape, Bevilacqua, Petroni, Liang"
    year="2023"
    url="https://arxiv.org/abs/2307.03172"
    type="paper"
  />

  <PaperReference
    title="In-context Learning and Induction Heads"
    authors="Olsson, Elhage, Nanda, et al."
    year="2022"
    url="https://arxiv.org/abs/2209.11895"
    type="paper"
  />

  <PaperReference
    title="Anthropic's Prompt Engineering Guide"
    authors="Anthropic"
    year="2024"
    url="https://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering/overview"
    type="docs"
  />
</section>
