---
// Module 1, Lesson 1.3: The Representation Learning Revolution
import KeyTakeaway from "../../components/KeyTakeaway.astro";
import PaperReference from "../../components/PaperReference.astro";
import MathBlock from "../../components/MathBlock.astro";
import Diagram from "../../components/Diagram.astro";
import Quiz from "../../components/Quiz.astro";
import GlossaryTooltip from "../../components/GlossaryTooltip.astro";
---

<section>
  <h2>Learning Objectives</h2>
  <p>After completing this lesson, you will be able to:</p>
  <ul>
    <li>
      Understand the progression from hand-crafted features
      to learned embeddings
    </li>
    <li>
      Explain how Word2Vec and GloVe differ from contextual
      embeddings
    </li>
    <li>
      Analyze why representation quality determines
      downstream task performance
    </li>
    <li>
      Understand transfer learning from a loss landscape
      perspective
    </li>
  </ul>
</section>

<section>
  <h2>From Hand-Crafted Features to Learned Embeddings</h2>

  <p>
    The core problem in machine learning is <em
      >representation</em
    >: how do we convert raw data (pixels, words, audio
    waveforms) into a format that machines can reason about?
  </p>

  <h3>The Limitation of Manual Features</h3>
  <p>
    Traditional <GlossaryTooltip term="ML" /> required domain experts to design
    features. For images, this meant <GlossaryTooltip
      term="SIFT"
    /> (Scale-Invariant Feature Transform), HOG (Histogram
    of Oriented Gradients), or color histograms. For text,
    bag-of-words or TF-IDF vectors. For audio, MFCCs.
  </p>

  <p>
    These features captured specific patterns experts deemed
    important, but had fundamental limitations:
  </p>
  <ul>
    <li>
      <strong>Labor-intensive</strong>: Designing good
      features required deep domain knowledge and
      experimentation
    </li>
    <li>
      <strong>Task-specific</strong>: Features optimized for
      one task often failed on related tasks
    </li>
    <li>
      <strong>Fixed complexity</strong>: No mechanism to
      learn more sophisticated features from more data
    </li>
    <li>
      <strong>Information loss</strong>: Dimensionality
      reduction often discarded useful signal
    </li>
  </ul>

  <h3>
    The Deep Learning Breakthrough: Learned Representations
  </h3>
  <p>
    Deep neural networks learn hierarchical representations
    automatically. Early layers detect simple patterns
    (edges, textures in vision; n-grams in text), while
    deeper layers compose these into higher-level concepts
    (object parts, semantic meaning).
  </p>

  <p>
    <strong>Key insight</strong>: The internal activations
    of a neural network trained on task A often transfer to
    task B, sometimes outperforming task-specific
    handcrafted features. This suggests the network has
    learned generally useful representations.
  </p>

  <p>
    This realization sparked the <strong
      >representation learning paradigm</strong
    >: the primary challenge is learning good
    representations; once you have them, simple models
    (linear classifiers, nearest neighbors) often suffice
    for downstream tasks.
  </p>
</section>

<section>
  <h2>Word Embeddings: Capturing Semantic Similarity</h2>

  <p>
    In <GlossaryTooltip term="NLP" />, words were historically represented as one-hot
    vectors: a vocabulary of 50,000 words yields
    50,000-dimensional vectors, each with a single 1 and
    zeros elsewhere. This encoding has no notion of
    similarity; "king" and "queen" are as different as
    "king" and "banana."
  </p>

  <h3>Word2Vec: Distributional Semantics</h3>
  <p>
    <strong>Word2Vec</strong> (2013) learns dense, low-dimensional
    embeddings by predicting words from context. Two architectures:
  </p>

  <ul>
    <li>
      <strong>CBOW (Continuous Bag of Words)</strong>:
      Predict target word from surrounding context
    </li>
    <li>
      <strong>Skip-gram</strong>: Predict context words from
      target word
    </li>
  </ul>

  <p>
    For skip-gram, given a word <MathBlock formula="w" /> and
    context word <MathBlock formula="c" />, we maximize:
  </p>

  <MathBlock
    formula={"\\mathcal{L} = \\sum_{(w,c) \\in D} \\log p(c \\mid w)"}
    display={true}
  />

  <p>
    Intuition: maximize the total log-probability of
    observing each context word given its target word,
    across all (word, context) pairs in the corpus.
  </p>

  <p>
    The probability of seeing context word <em>c</em> given target word <em>w</em> is computed using a softmax over all words in the vocabulary. In short: the more similar two word vectors are (high dot product), the higher the predicted probability:
  </p>

  <MathBlock
    formula={"p(c \\mid w) = \\frac{\\exp(\\mathbf{v}_w \\cdot \\mathbf{v}_c)}{\\sum_{c' \\in V} \\exp(\\mathbf{v}_w \\cdot \\mathbf{v}_{c'})}"}
    display={true}
  />

  <p>
    The problem: the denominator sums over the entire vocabulary (often 50,000+ words), which is far too expensive to compute for every training step. <strong>Negative sampling</strong> sidesteps this by turning the problem into a simpler binary task: "is this word a real context word, or a random one?"
  </p>

  <MathBlock
    formula={"\\mathcal{L} = \\log \\sigma(\\mathbf{v}_w \\cdot \\mathbf{v}_c) + \\sum_{i=1}^k \\mathbb{E}_{c_i \\sim P_n} [\\log \\sigma(-\\mathbf{v}_w \\cdot \\mathbf{v}_{c_i})]"}
    display={true}
  />

  <p>
    The first term pushes the target word and its true context word closer together. The second term pushes the target word away from k randomly sampled "noise" words. This is much cheaper to compute and works surprisingly well in practice.
  </p>

  <h3>GloVe: Global Vectors</h3>
  <p>
    <strong>GloVe</strong> (2014) takes a different approach. Instead of predicting words from context one sentence at a time, it first counts how often every pair of words appears near each other across the entire corpus, then learns vectors that reproduce those counts:
  </p>

  <MathBlock
    formula={"\\mathcal{L} = \\sum_{i,j} f(X_{ij}) (\\mathbf{v}_i^T \\mathbf{v}_j + b_i + b_j - \\log X_{ij})^2"}
    display={true}
  />

  <p>
    The idea: if "ice" and "cold" co-occur frequently, their word vectors should have a high dot product. The weighting function <MathBlock formula="f" /> prevents extremely common pairs (like "the" + "of") from dominating. <MathBlock formula="X_{ij}" /> is simply the co-occurrence count of words <em>i</em> and <em>j</em>.
  </p>

  <h3>Remarkable Properties</h3>
  <p>
    These embeddings capture semantic and syntactic
    relationships through vector arithmetic:
  </p>

  <MathBlock
    formula={"\\mathbf{v}_{\\text{king}} - \\mathbf{v}_{\\text{man}} + \\mathbf{v}_{\\text{woman}} \\approx \\mathbf{v}_{\\text{queen}}"}
    display={true}
  />
  <MathBlock
    formula={"\\mathbf{v}_{\\text{Paris}} - \\mathbf{v}_{\\text{France}} + \\mathbf{v}_{\\text{Italy}} \\approx \\mathbf{v}_{\\text{Rome}}"}
    display={true}
  />

  <p>
    Intuition: semantic relationships are encoded as vector
    offsets. Subtracting "man" and adding "woman" moves the
    embedding in the "gender" direction; subtracting
    "France" and adding "Italy" moves in the "country"
    direction.
  </p>

  <p>
    This linear structure emerges from the training
    objective without explicit supervision, a striking
    example of self-supervised (distributional)
    representation learning.
  </p>

  <h3>Limitations of Static Embeddings</h3>
  <p>
    Word2Vec and GloVe assign each word a single fixed
    vector, ignoring context. The word "bank" has the same
    embedding in "river bank" and "bank account." This
    limits their expressiveness.
  </p>
</section>

<section>
  <h2>Contextual Embeddings: A New Paradigm</h2>

  <p>
    <strong>Contextual embeddings</strong> solve the polysemy
    problem (words having multiple meanings depending on context)
    by computing word representations dynamically based on context.
    The same word gets different embeddings in different sentences.
  </p>

  <h3>ELMo: Bidirectional LSTMs</h3>
  <p>
    ELMo (Embeddings from Language Models, 2018) uses
    bidirectional LSTMs trained on language modeling. For a
    sentence, ELMo computes representations at multiple
    layers and combines them.
  </p>

  <p>
    Unlike Word2Vec, ELMo embeddings depend on the entire
    sentence, capturing nuanced meaning. Replacing static
    embeddings with ELMo immediately improved performance
    across NLP tasks, a clear signal that representation
    quality matters more than model architecture for many
    problems.
  </p>

  <h3>BERT and the Transformer Revolution</h3>
  <p>
    <strong>BERT</strong> (2018) uses transformers (not LSTMs) and bidirectional context via masked language modeling:
    randomly mask 15% of tokens and predict them from surrounding
    context.
  </p>

  <p>
    BERT's contextualized embeddings are even richer than
    ELMo's, and the transformer architecture scales better.
    BERT representations became the new foundation for NLP, analogous to ImageNet-pretrained <GlossaryTooltip
      term="CNN"
    />s in vision.
  </p>

  <h3>GPT: Unidirectional but Scalable</h3>
  <p>
    GPT (Generative Pretrained Transformer) uses
    unidirectional (left-to-right) language modeling instead
    of masked prediction. While this limits bidirectional
    context, it enables autoregressive generation and scales
    to much larger models (GPT-3: 175B parameters).
  </p>

  <p>
    GPT's representations prove highly effective when
    fine-tuned or few-shot prompted, demonstrating that
    scale can compensate for training objective differences.
  </p>
</section>

<Diagram
  diagramId="embedding-space"
  title="2D Embedding Space Visualization"
  autoplay={false}
  animationDuration={4000}
>
  <div class="w-full">
    <div class="relative w-full bg-[hsl(var(--card))] rounded-lg overflow-hidden">
    <svg viewBox="0 0 500 360" preserveAspectRatio="xMidYMid meet" class="w-full h-auto" style="max-height: 400px;">
      <!-- Axes -->
      <line x1="50" y1="310" x2="470" y2="310" stroke="hsl(var(--border))" stroke-width="1.5"></line>
      <line x1="50" y1="310" x2="50" y2="30" stroke="hsl(var(--border))" stroke-width="1.5"></line>
      <!-- Axis arrows -->
      <polygon points="470,310 462,306 462,314" fill="hsl(var(--border))"></polygon>
      <polygon points="50,30 46,38 54,38" fill="hsl(var(--border))"></polygon>

      <!-- Vector arithmetic arrow: king - man + woman = queen -->
      <defs>
        <marker id="arrowhead" markerWidth="8" markerHeight="6" refX="8" refY="3" orient="auto">
          <polygon points="0 0, 8 3, 0 6" fill="hsl(var(--primary))" opacity="0.5"></polygon>
        </marker>
      </defs>
      <line x1="155" y1="100" x2="255" y2="80" stroke="hsl(var(--primary))" stroke-width="1.5" stroke-dasharray="6 4" opacity="0.5" marker-end="url(#arrowhead)" data-animate style="animation-delay: 2.2s"></line>
      <text x="195" y="75" text-anchor="middle" font-size="9" fill="hsl(var(--primary))" opacity="0.6" data-animate style="animation-delay: 2.2s">- man + woman</text>

      <!-- Cluster 1: Animals (top-left) -->
      <g data-animate style="animation-delay: 0.2s">
        <circle cx="120" cy="160" r="7" fill="#6366f1"></circle>
        <text x="120" y="148" text-anchor="middle" font-size="12" fill="hsl(var(--foreground))">dog</text>
      </g>
      <g data-animate style="animation-delay: 0.4s">
        <circle cx="160" cy="180" r="7" fill="#6366f1"></circle>
        <text x="160" y="168" text-anchor="middle" font-size="12" fill="hsl(var(--foreground))">cat</text>
      </g>
      <g data-animate style="animation-delay: 0.6s">
        <circle cx="135" cy="210" r="7" fill="#6366f1"></circle>
        <text x="135" y="228" text-anchor="middle" font-size="12" fill="hsl(var(--foreground))">horse</text>
      </g>

      <!-- Cluster 2: Food (bottom-right) -->
      <g data-animate style="animation-delay: 0.8s">
        <circle cx="350" cy="220" r="7" fill="#10b981"></circle>
        <text x="350" y="208" text-anchor="middle" font-size="12" fill="hsl(var(--foreground))">apple</text>
      </g>
      <g data-animate style="animation-delay: 1s">
        <circle cx="390" cy="245" r="7" fill="#10b981"></circle>
        <text x="390" y="263" text-anchor="middle" font-size="12" fill="hsl(var(--foreground))">banana</text>
      </g>
      <g data-animate style="animation-delay: 1.2s">
        <circle cx="355" cy="260" r="7" fill="#10b981"></circle>
        <text x="355" y="278" text-anchor="middle" font-size="12" fill="hsl(var(--foreground))">orange</text>
      </g>

      <!-- Cluster 3: Royalty (top-center) -->
      <g data-animate style="animation-delay: 1.4s">
        <circle cx="155" cy="100" r="7" fill="#8b5cf6"></circle>
        <text x="140" y="93" text-anchor="end" font-size="12" fill="hsl(var(--foreground))">king</text>
      </g>
      <g data-animate style="animation-delay: 1.6s">
        <circle cx="265" cy="80" r="7" fill="#8b5cf6"></circle>
        <text x="280" y="73" text-anchor="start" font-size="12" fill="hsl(var(--foreground))">queen</text>
      </g>
      <g data-animate style="animation-delay: 1.8s">
        <circle cx="210" cy="110" r="7" fill="#8b5cf6"></circle>
        <text x="210" y="130" text-anchor="middle" font-size="12" fill="hsl(var(--foreground))">prince</text>
      </g>
      <g data-animate style="animation-delay: 2s">
        <circle cx="100" cy="120" r="7" fill="#8b5cf6"></circle>
        <text x="85" y="115" text-anchor="end" font-size="12" fill="hsl(var(--foreground))">man</text>
      </g>
      <g data-animate style="animation-delay: 2s">
        <circle cx="210" cy="60" r="7" fill="#8b5cf6"></circle>
        <text x="210" y="50" text-anchor="middle" font-size="12" fill="hsl(var(--foreground))">woman</text>
      </g>

      <!-- Legend -->
      <g transform="translate(270, 310)">
        <circle cx="0" cy="15" r="4" fill="#6366f1"></circle>
        <text x="10" y="19" font-size="11" fill="hsl(var(--muted-foreground))">Animals</text>
        <circle cx="75" cy="15" r="4" fill="#10b981"></circle>
        <text x="85" y="19" font-size="11" fill="hsl(var(--muted-foreground))">Food</text>
        <circle cx="130" cy="15" r="4" fill="#8b5cf6"></circle>
        <text x="140" y="19" font-size="11" fill="hsl(var(--muted-foreground))">Royalty</text>
      </g>
    </svg>
    </div>
    <p class="text-sm text-[hsl(var(--muted-foreground))] mt-3">
      Similar words cluster together, and relationships are encoded as directions. The dashed arrow shows the famous "king - man + woman = queen" analogy. Real embeddings range from hundreds to several thousand dimensions, but the same clustering principle holds.
    </p>
  </div>
</Diagram>

<section>
  <h2>
    Why Representation Quality Determines Downstream
    Performance
  </h2>

  <p>
    A central insight: <strong
      >the representation bottleneck is often more severe
      than the model capacity bottleneck</strong
    >.
  </p>

  <h3>Linear Probing as a Diagnostic</h3>
  <p>
    <em>Linear probing</em> tests representation quality: freeze
    a pretrained model's representations and train only a linear
    classifier on top. If linear probing achieves high accuracy,
    the representations already separate classes well, a sign
    of good features.
  </p>

  <p>
    Surprisingly, modern foundation models (<GlossaryTooltip
      term="CLIP"
    /> for vision, and frozen language models with simple
    heads in NLP) can achieve strong performance with linear
    probing alone. This suggests:
  </p>
  <ul>
    <li>
      Pretraining has solved much of the "hard part"
      (learning features)
    </li>
    <li>
      Task-specific fine-tuning primarily adapts these
      features rather than learning new ones
    </li>
  </ul>

  <h3>The Information-Theoretic View</h3>
  <p>
    Good representations compress data: they throw away what doesn't matter (lighting conditions, background noise, irrelevant words) while keeping what does (object shape, semantic meaning, sentiment). Think of it like summarizing a book. A perfect summary preserves every important idea while discarding filler.
  </p>

  <p>
    This is formalized as the <strong>Information Bottleneck</strong> principle: representations should keep as much information as possible about the task you care about, while discarding as much raw input detail as possible. Self-supervised pretraining implicitly learns this trade-off, which is part of why pretrained representations transfer so well to new tasks.
  </p>
</section>

<section>
  <h2>Transfer Learning: The Loss Landscape Perspective</h2>

  <p>
    <strong>Transfer learning</strong>, pretraining on a
    large dataset then fine-tuning on a target task, is the
    dominant paradigm in modern ML. But why does it work?
  </p>

  <h3>Initialization Matters</h3>
  <p>
    Imagine the loss landscape as a hilly terrain, where lower elevation means better performance. Training a neural network means searching this terrain for the lowest valley. The catch: there are many valleys (local minima), and random initialization drops you at an arbitrary location. You might end up in a shallow dip instead of the deepest valley. Pretrained initialization, on the other hand, starts you near a known "good" region of the landscape.
  </p>

  <p>
    <strong>Key insight</strong>: Pretrained weights are not
    just good feature extractors; they also provide a
    beneficial <em>inductive bias</em> (a built-in preference
    toward certain solutions) for optimization. Fine-tuning from
    pretrained weights converges faster and to better generalization
    than training from scratch in many practical settings,
    especially when labeled data is limited.
  </p>

  <h3>Mode Connectivity and Basins</h3>
  <p>
    Recent research shows that pretrained models and their fine-tuned variants often sit in the same "basin" of the loss landscape: a broad, low-loss valley. You can walk from one to the other without climbing over any hills.
  </p>

  <p>
    This explains why fine-tuning is stable and effective: you're exploring within a good valley that pretraining already discovered, rather than searching the entire terrain from scratch.
  </p>

  <h3>The Data Efficiency of Transfer</h3>
  <p>
    Pretraining on a large, diverse dataset (ImageNet,
    Common Crawl) exposes the model to a wide range of
    patterns. Fine-tuning on a smaller, specialized dataset
    then requires far fewer examples to achieve good
    performance.
  </p>

  <p>
    In practice, transfer learning often substantially reduces
    the labeled data needed for a new task. This has democratized
    ML: many problems that previously required massive
    labeled datasets can now be solved with thousands, or
    even hundreds, of examples via fine-tuning.
  </p>
</section>

<Quiz
  quizId="contextual-embeddings"
  question="What fundamental limitation of Word2Vec/GloVe is addressed by contextual embeddings like BERT?"
  options={[
    {
      id: "a",
      text: "They produce embeddings that are too high-dimensional",
      correct: false,
      explanation:
        "Dimensionality isn't the core issue; the bigger limitation is that static embeddings ignore context.",
    },
    {
      id: "b",
      text: "They cannot capture any semantic meaning from text",
      correct: false,
      explanation:
        "Word2Vec/GloVe do capture semantic meaning, they famously encode relationships like king - man + woman = queen.",
    },
    {
      id: "c",
      text: "Each word gets a single fixed vector regardless of context",
      correct: true,
      explanation:
        "Correct! Word2Vec/GloVe assign one static embedding per word. 'Bank' gets the same vector whether it means a river bank or financial bank. BERT produces different embeddings based on surrounding context.",
    },
    {
      id: "d",
      text: "They require too much training data to learn useful representations",
      correct: false,
      explanation:
        "Word2Vec/GloVe are actually quite data-efficient compared to large contextual models like BERT.",
    },
  ]}
/>

<KeyTakeaway>
  <ul>
    <li>
      <strong>Learned representations</strong> have replaced hand-crafted
      features, learning hierarchical patterns automatically from
      data
    </li>
    <li>
      <strong>Word embeddings</strong> (Word2Vec, GloVe) capture
      semantic similarity but assign each word a fixed vector,
      ignoring context
    </li>
    <li>
      <strong>Contextual embeddings</strong> (ELMo, <GlossaryTooltip
        term="BERT"
      />, GPT) compute word representations
      dynamically based on surrounding text, capturing polysemy
      and nuance
    </li>
    <li>
      <strong>Representation quality</strong> is often more important
      than model architecture; good representations enable simple
      classifiers to achieve strong performance
    </li>
    <li>
      <strong>Transfer learning works</strong> because pretrained
      weights initialize optimization in a favorable basin of
      the loss landscape, improving both convergence speed and
      final generalization
    </li>
    <li>
      <strong>The paradigm has shifted</strong>: instead of
      task-specific models, we pretrain general-purpose
      representations and adapt them, achieving dramatic
      data efficiency gains
    </li>
  </ul>
</KeyTakeaway>

<section>
  <h2>References</h2>

  <PaperReference
    title="Efficient Estimation of Word Representations in Vector Space (Word2Vec)"
    authors="Mikolov et al."
    year="2013"
    url="https://arxiv.org/abs/1301.3781"
    type="paper"
  />

  <PaperReference
    title="GloVe: Global Vectors for Word Representation"
    authors="Pennington, Socher, Manning"
    year="2014"
    url="https://aclanthology.org/D14-1162/"
    type="paper"
  />

  <PaperReference
    title="Deep contextualized word representations (ELMo)"
    authors="Peters et al."
    year="2018"
    url="https://arxiv.org/abs/1802.05365"
    type="paper"
  />

  <PaperReference
    title="BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"
    authors="Devlin et al."
    year="2018"
    url="https://arxiv.org/abs/1810.04805"
    type="paper"
  />

  <PaperReference
    title="Improving Language Understanding by Generative Pre-Training (GPT)"
    authors="Radford et al."
    year="2018"
    url="https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf"
    type="paper"
  />

  <PaperReference
    title="Visualizing the Loss Landscape of Neural Nets"
    authors="Li et al."
    year="2018"
    url="https://arxiv.org/abs/1712.09913"
    type="paper"
  />
</section>
