---
// Module 2, Lesson 2.3: Encoder-only, Decoder-only, and Encoder-Decoder
import KeyTakeaway from "../../components/KeyTakeaway.astro";
import PaperReference from "../../components/PaperReference.astro";
import MathBlock from "../../components/MathBlock.astro";
import Quiz from "../../components/Quiz.astro";
import Diagram from "../../components/Diagram.astro";
import GlossaryTooltip from "../../components/GlossaryTooltip.astro";
---

<h2>Learning Objectives</h2>
<p>After completing this lesson, you will be able to:</p>
<ul>
  <li>
    Distinguish <GlossaryTooltip term="BERT" />-style, <GlossaryTooltip
      term="GPT"
    />-style, and <GlossaryTooltip term="T5" />-style
    transformer architectures
  </li>
  <li>
    Understand causal masking and its role in autoregressive
    generation
  </li>
  <li>
    Explain the KV-cache optimization and its memory
    implications
  </li>
  <li>Compare approaches to the context window problem</li>
</ul>

<h2>Three Transformer Architectures</h2>

<p>
  The original "Attention Is All You Need" transformer had both an encoder and a decoder working together. But researchers quickly discovered that you don't always need both halves. Depending on your task, you can strip the transformer down to just one side, or keep both. This led to three main variants, each with different strengths.
</p>

<h3>1. Encoder-Only (BERT-Style)</h3>

<p>
  <GlossaryTooltip term="BERT" />-style encoder-only models use just the encoder stack
  (self-attention without causal masking). Think of them as "reading comprehension" models: they see the
  entire input sequence at once (bidirectional context),
  which makes them excellent at understanding tasks like
  classification and sentiment analysis. They read the whole sentence before forming an opinion, rather than processing it word by word.
</p>
<p>
  <strong>Architecture</strong>: A stack of transformer blocks
  with bidirectional attention. Each token can attend to all
  other tokens, both past and future. This is a big advantage for understanding, because meaning often depends on what comes later in a sentence.
</p>

<p>
  <strong>Training</strong>: These models learn through Masked Language Modeling (MLM), a clever "fill in the blank" game.
  You randomly mask 15% of tokens, and the model predicts the missing words using the surrounding context:
</p>

<MathBlock
  formula={"p(x_i \\mid x_{<i}, x_{>i}) = \\text{softmax}(W_{\\text{vocab}} \\cdot h_i)"}
  display={true}
/>

<p>
  Read this as: "the probability of the masked token x_i, given all the tokens before it and after it, equals the softmax over vocabulary scores computed from its hidden representation h_i." It's essentially the model saying, "given the surrounding words, which word most likely goes here?"
</p>

<p>
  <strong>The <GlossaryTooltip term="CLS" /> token</strong>: A special token
  prepended to every input sequence. Its final hidden state acts as a summary of the entire sequence, which you can feed into a classifier for tasks like sentiment analysis.
</p>

<p>
  <strong>Use cases</strong>: Sentence classification, named
  entity recognition, question answering (extractive). Not
  designed for generation, since these models have no mechanism to produce text one token at a time.
</p>

<p>
  <strong>Key models</strong>: BERT, RoBERTa, ELECTRA
</p>

<h3>2. Decoder-Only (GPT-Style)</h3>

<p>
  <GlossaryTooltip term="GPT" />-style decoder-only models use just the decoder stack (masked
  self-attention). They process tokens left-to-right,
  predicting the next token at each step. If encoder-only models are "readers," decoder-only models are "writers": their entire design revolves around generating text one token at a time.
</p>
<p>
  <strong>Architecture</strong>: A stack of transformer blocks
  with causal (unidirectional) attention. The key constraint is that a token at position
  i can only attend to positions at or before i, never peeking ahead. This is what makes generation possible: the model can produce tokens sequentially without needing to see what comes next.
</p>

<p>
  <strong>Causal masking</strong> enforces this "no peeking" rule. Here's how it works: you add a mask
  to the attention scores before the softmax step. The mask is a simple rule:
</p>

<MathBlock
  formula={"\\text{mask}_{ij} = \\begin{cases} 0 & \\text{if } i \\geq j \\\\ -\\infty & \\text{if } i < j \\end{cases}"}
  display={true}
/>

<p>
  In plain English: if token i is looking at token j, and j comes after i (a future token), the mask adds negative infinity. If j is at or before position i, the mask adds 0 (no effect). The -infinity is a trick: when softmax sees -infinity, it outputs 0, effectively blocking that future token from influencing the current one. You then fold this mask into the standard attention formula:
</p>

<MathBlock
  formula={"\\alpha_{ij} = \\text{softmax}(\\frac{q_i^T k_j}{\\sqrt{d_k}} + \\text{mask}_{ij})"}
  display={true}
/>

<p>
  Read this as: "the attention weight from token i to token j is the softmax of their scaled dot-product, plus the mask." For allowed tokens, the mask is 0 and attention works normally. For future tokens, the -infinity drives the attention weight to 0, preventing information leakage.
</p>

<p>
  <strong>Training</strong>: The training objective is beautifully simple: next-token prediction. Given a sequence of tokens, the model learns to predict what comes next at every position:
</p>

<MathBlock
  formula={"\\mathcal{L} = -\\sum_t \\log p(x_t \\mid x_{<t})"}
  display={true}
/>

<p>
  In plain English: "sum up the negative log-probabilities of each token given all preceding tokens." The model is rewarded for assigning high probability to the actual next token. This single objective turns out to be surprisingly powerful, because to predict the next word well, you need to understand grammar, facts, reasoning, and more.
</p>

<p><strong>Why GPT-style won for <GlossaryTooltip term="LLM" />s</strong>:</p>
<ul>
  <li>The pretraining objective (next-token prediction) is the same as the generation task, so there's no mismatch</li>
  <li>Scales naturally with more compute and data</li>
  <li>Simple to implement and train</li>
  <li>Enables autoregressive sampling for open-ended generation</li>
</ul>

<p>
  <strong>Key models</strong>: GPT-2, GPT-3, GPT-4, GPT-5.2, Llama 4, Claude Opus 4.6, Gemini
  3 Pro
</p>

<h3>3. Encoder-Decoder (T5-Style)</h3>
<p>
  What if you need both deep understanding of an input <em>and</em> the ability to generate output? <GlossaryTooltip term="T5" />-style models keep both stacks. The encoder reads the full input bidirectionally (like BERT), and the decoder generates output one token at a time (like GPT). This "best of both worlds" approach is versatile for
  sequence-to-sequence tasks like translation and
  summarization.
</p>
<p>
  <strong>Architecture</strong>: The encoder (bidirectional)
  processes the input and builds rich representations; the decoder (causal) generates output while
  cross-attending to those encoder representations. The cross-attention is the bridge between the two halves.
</p>

<p>
  <strong>Cross-attention</strong> is how the decoder "looks back" at the encoder's understanding of the input. The decoder's queries attend
  to the encoder's keys and values:
</p>

<MathBlock
  formula={"\\text{CrossAttn} = \\text{Attention}(Q_{\\text{decoder}}, K_{\\text{encoder}}, V_{\\text{encoder}})"}
  display={true}
/>

<p>
  Read this as: "the decoder asks questions (queries) and finds answers in the encoder's stored information (keys and values)." For example, when translating "The cat sat" to French, the decoder generating "Le" would attend to the encoder's representation of "The" to determine the correct article.
</p>

<p>
  <strong>Use cases</strong>: Tasks with distinct
  input and output (translation, summarization). Encoder-decoder
  architectures shine when the input and output have different structures or even different modalities.
</p>

<p><strong>Key models</strong>: T5, BART, mT5</p>

<div class="my-8 p-6 bg-[hsl(var(--card))] rounded-lg border border-[hsl(var(--border))]">
  <h3 class="text-lg font-semibold text-[hsl(var(--foreground))] mb-2">Attention Mask Patterns</h3>
  <p class="text-sm text-[hsl(var(--muted-foreground))] mb-6">
    Each grid shows which tokens can attend to which. Rows = the token computing attention (query), columns = tokens it can look at (keys). A colored cell means "allowed," a gray cell means "blocked."
  </p>

  <div class="grid grid-cols-1 sm:grid-cols-3 gap-8">
    <!-- Encoder-Only (BERT) -->
    <div>
      <div class="font-semibold text-sm mb-1 text-center">Encoder-Only (BERT)</div>
      <div class="text-xs text-[hsl(var(--muted-foreground))] mb-3 text-center">Every token sees every other token</div>
      <!-- Axis labels -->
      <div class="flex items-start gap-1">
        <div class="flex flex-col items-center justify-between w-6 pt-1" style="height: 148px;">
          <span class="text-[10px] text-[hsl(var(--muted-foreground))] -rotate-90 whitespace-nowrap">query →</span>
        </div>
        <div>
          <div class="grid grid-cols-4 gap-1">
            {
              [...Array(16)].map(() => (
                <div class="w-8 h-8 bg-[hsl(var(--diagram-indigo-solid))] rounded opacity-85" />
              ))
            }
          </div>
          <div class="text-[10px] text-[hsl(var(--muted-foreground))] text-center mt-1">key →</div>
        </div>
      </div>
    </div>

    <!-- Decoder-Only (GPT) -->
    <div>
      <div class="font-semibold text-sm mb-1 text-center">Decoder-Only (GPT)</div>
      <div class="text-xs text-[hsl(var(--muted-foreground))] mb-3 text-center">Each token only sees past + itself</div>
      <div class="flex items-start gap-1">
        <div class="flex flex-col items-center justify-between w-6 pt-1" style="height: 148px;">
          <span class="text-[10px] text-[hsl(var(--muted-foreground))] -rotate-90 whitespace-nowrap">query →</span>
        </div>
        <div>
          <div class="grid grid-cols-4 gap-1">
            {
              [...Array(16)].map((_, i) => {
                const row = Math.floor(i / 4);
                const col = i % 4;
                const masked = col > row;
                return (
                  <div class={`w-8 h-8 rounded ${masked ? "bg-[hsl(var(--muted))] border border-dashed border-[hsl(var(--border))]" : "bg-[hsl(var(--diagram-emerald-solid))] opacity-85"}`} />
                );
              })
            }
          </div>
          <div class="text-[10px] text-[hsl(var(--muted-foreground))] text-center mt-1">key →</div>
        </div>
      </div>
    </div>

    <!-- Encoder-Decoder (T5) -->
    <div>
      <div class="font-semibold text-sm mb-1 text-center">Encoder-Decoder (T5)</div>
      <div class="text-xs text-[hsl(var(--muted-foreground))] mb-3 text-center">Encoder: full, Decoder: causal</div>
      <div class="flex items-start gap-1">
        <div class="flex flex-col items-center w-6 pt-1" style="height: 148px;">
          <span class="text-[10px] text-[hsl(var(--muted-foreground))] -rotate-90 whitespace-nowrap">query →</span>
        </div>
        <div>
          <!-- Top half: encoder (bidirectional) -->
          <div class="grid grid-cols-4 gap-1">
            {
              [...Array(8)].map(() => (
                <div class="w-8 h-[15px] bg-[hsl(var(--diagram-indigo-solid))] rounded-sm opacity-85" />
              ))
            }
          </div>
          <div class="h-px bg-[hsl(var(--border))] my-1"></div>
          <!-- Bottom half: decoder (causal) -->
          <div class="grid grid-cols-4 gap-1">
            {
              [...Array(8)].map((_, i) => {
                const row = Math.floor(i / 4);
                const col = i % 4;
                const masked = col > row;
                return (
                  <div class={`w-8 h-[15px] rounded-sm ${masked ? "bg-[hsl(var(--muted))] border border-dashed border-[hsl(var(--border))]" : "bg-[hsl(var(--diagram-emerald-solid))] opacity-85"}`} />
                );
              })
            }
          </div>
          <div class="text-[10px] text-[hsl(var(--muted-foreground))] text-center mt-1">key →</div>
        </div>
      </div>
    </div>
  </div>

  <div class="flex flex-wrap gap-4 mt-5 pt-4 border-t border-[hsl(var(--border))] text-xs text-[hsl(var(--muted-foreground))]">
    <span class="flex items-center gap-1.5"><span class="inline-block w-3 h-3 bg-[hsl(var(--diagram-indigo-solid))] rounded-sm"></span> Bidirectional attention</span>
    <span class="flex items-center gap-1.5"><span class="inline-block w-3 h-3 bg-[hsl(var(--diagram-emerald-solid))] rounded-sm"></span> Causal attention</span>
    <span class="flex items-center gap-1.5"><span class="inline-block w-3 h-3 bg-[hsl(var(--muted))] rounded-sm border border-dashed border-[hsl(var(--border))]"></span> Masked (blocked)</span>
  </div>
</div>

<h2>KV-Cache: Making Generation Fast</h2>

<p>
  Here's a practical problem you might not expect. Autoregressive generation is sequential: generate token 1, append it to the context, generate token 2, and so on. Without any optimization, every time you generate a new token, you'd have to recompute attention over all previous tokens from scratch. That's O(n<sup>2</sup>) operations total, and it gets painfully slow as sequences grow.
</p>

<p>
  The insight behind the <strong>KV-cache</strong> is simple: the key and value vectors for previous tokens don't change as you generate new tokens. So why recompute them? Instead, you cache the K and V vectors for every token you've already processed. When generating the next token, you only compute the new token's query (Q<sub>t</sub>), then look it up against the cached keys and values:
</p>

<MathBlock
  formula={"\\text{Attention}_t(Q_t, [K_{1:t-1}; K_t], [V_{1:t-1}; V_t])"}
  display={true}
/>

<p>
  Read this as: "at step t, compute attention using the new token's query against all previously cached keys and values, plus the current token's own K and V." The semicolons mean "concatenate," so you're just appending the new token's vectors to the cache.
</p>

<p>
  This reduces per-token generation cost from O(n<sup>2</sup>) to O(n), which is the difference between a chatbot that takes seconds to respond and one that takes minutes.
</p>

<h3>Memory Implications</h3>

<p>
  The KV-cache makes generation fast, but it eats memory. For a model with L layers and hidden dimension d,
  storing the KV-cache for n tokens requires:
</p>

<MathBlock
  formula={"2 \\times L \\times n \\times d \\times \\text{bytes\\_per\\_param}"}
  display={true}
/>

<p>
  The factor of 2 is because you store both K and V. To make this concrete: for GPT-3 (96 layers, d=12288) using float16, caching 2048 tokens requires roughly 4.7 GB. That's a significant chunk of GPU memory just for one request, which is why KV-cache size is often the bottleneck that limits how many users a model can serve simultaneously.
</p>

<h2>The Context Window Problem (and Solutions)</h2>

<p>
  You might be wondering: if self-attention lets every token look at every other token, what's the catch? The catch is cost. Self-attention scales as O(n<sup>2</sup>) with sequence length, meaning doubling your context length quadruples the computation. Early models were limited to 2k-8k tokens, but by 2026, context windows have expanded dramatically through clever engineering.
</p>

<h3>Modern Context Windows (2026)</h3>
<p>
  Current frontier models support context windows of <strong>up to 1-2M tokens</strong> (Gemini 3 Pro at 1M+, Claude Opus 4.6 at 200K), with context lengths continuing to grow. How did we get here? Three key innovations:
</p>
<ul>
  <li>
    <strong>FlashAttention-2</strong>: A memory-efficient
    implementation of attention that avoids materializing the full n-by-n attention matrix, achieving 230 TFLOPs/s on A100 GPUs
  </li>
  <li>
    <strong>Sub-quadratic alternatives</strong>: State-space
    models (Mamba, RWKV) achieve O(n) complexity, offering
    an alternative to full attention for very long sequences
  </li>
  <li>
    <strong>Improved positional encodings</strong>: RoPE
    (Rotary Position Embedding) and ALiBi enable models to generalize to sequence lengths longer than what they were trained on
  </li>
</ul>

<h3>Earlier Approaches (Historical)</h3>
<p>
  Before these breakthroughs, researchers tried to work around the O(n<sup>2</sup>) barrier more directly.
  <strong>Sparse Attention</strong> (Longformer, BigBird)
  restricted attention to local windows or predefined
  patterns, reducing complexity to O(n) but sacrificing full
  attention flexibility.
</p>

<p>
  <strong>Sliding Window Attention</strong> took a simpler approach: each token
  attends only to a fixed window (e.g., 512 tokens
  before and after). Used in models like MPT and CodeGen.
</p>

<h3>Alternatives: Retrieval-Augmented Generation</h3>
<p>
  There's also a completely different strategy: instead of making the context window bigger,
  retrieve only the relevant chunks from a database and pass them as
  context. This <GlossaryTooltip term="RAG" /> approach remains valuable for knowledge-intensive tasks even
  with 1M+ token windows (covered in the RAG module).
</p>

<Quiz
  question="Why did decoder-only (GPT-style) become dominant for LLMs, despite encoder-only (BERT-style) achieving state-of-the-art on many NLU benchmarks?"
  quizId="decoder-dominance"
  options={[
    {
      id: "a",
      text: "Decoder-only models are faster to train",
      correct: false,
      explanation:
        "Training speed is similar. The key difference is in the training objective and generation capabilities.",
    },
    {
      id: "b",
      text: "Decoder-only unifies pretraining (next-token prediction) with generation, and scales better with compute",
      correct: true,
      explanation:
        "Correct! Decoder-only models use the same objective for pretraining and generation (next-token prediction). This simplifies the pipeline and scales naturally. As models grew larger, generation capabilities became more valuable than bidirectional encoding.",
    },
    {
      id: "c",
      text: "BERT-style models can't be fine-tuned for downstream tasks",
      correct: false,
      explanation:
        "BERT-style models fine-tune very well for classification and extraction tasks.",
    },
    {
      id: "d",
      text: "Causal attention is mathematically superior to bidirectional attention",
      correct: false,
      explanation:
        "Bidirectional attention can capture richer context. The advantage of causal attention is enabling generation.",
    },
  ]}
/>

<KeyTakeaway>
  <ul>
    <li>
      <strong>BERT (encoder-only)</strong> uses bidirectional
      attention and MLM training; excellent for understanding
      tasks but not generation
    </li>
    <li>
      <strong>GPT (decoder-only)</strong> uses causal attention
      and next-token prediction; unified pretraining/generation
      objective that scales well
    </li>
    <li>
      <strong>T5 (encoder-decoder)</strong> combines both; best
      for tasks with distinct input/output like translation
    </li>
    <li>
      <strong>KV-cache</strong> caches key/value vectors to reduce
      generation from O(n²) to O(n) per token, critical for real-time
      inference
    </li>
    <li>
      <strong>Context window limits</strong> arise from O(n²)
      attention; solutions include sparse attention, sliding windows,
      and retrieval augmentation
    </li>
  </ul>
</KeyTakeaway>

<h2>References</h2>

<PaperReference
  title="BERT: Pre-training of Deep Bidirectional Transformers"
  authors="Devlin et al."
  year="2018"
  url="https://arxiv.org/abs/1810.04805"
  type="paper"
/>

<PaperReference
  title="Improving Language Understanding by Generative Pre-Training (GPT)"
  authors="Radford et al."
  year="2018"
  url="https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf"
  type="paper"
/>

<PaperReference
  title="Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer (T5)"
  authors="Raffel et al."
  year="2019"
  url="https://arxiv.org/abs/1910.10683"
  type="paper"
/>

<PaperReference
  title="Longformer: The Long-Document Transformer"
  authors="Beltagy et al."
  year="2020"
  url="https://arxiv.org/abs/2004.05150"
  type="paper"
/>
