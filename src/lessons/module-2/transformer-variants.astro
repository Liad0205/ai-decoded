---
// Module 2, Lesson 2.3: Encoder-only, Decoder-only, and Encoder-Decoder
import KeyTakeaway from "../../components/KeyTakeaway.astro";
import PaperReference from "../../components/PaperReference.astro";
import MathBlock from "../../components/MathBlock.astro";
import Quiz from "../../components/Quiz.astro";
import Diagram from "../../components/Diagram.astro";
import GlossaryTooltip from "../../components/GlossaryTooltip.astro";
---

<h2>Learning Objectives</h2>
<ul>
  <li>
    Distinguish <GlossaryTooltip term="BERT" />-style, <GlossaryTooltip
      term="GPT"
    />-style, and <GlossaryTooltip term="T5" />-style
    transformer architectures
  </li>
  <li>
    Understand causal masking and its role in autoregressive
    generation
  </li>
  <li>
    Explain the KV-cache optimization and its memory
    implications
  </li>
  <li>Compare approaches to the context window problem</li>
</ul>

<h2>Three Transformer Architectures</h2>

<p>
  The original transformer had an encoder-decoder structure.
  Modern models specialize into three main variants:
</p>

<h3>
  1. Encoder-Only (<GlossaryTooltip term="BERT" />-Style)
</h3>

<p>
  Encoder-only models use just the encoder stack
  (self-attention without causal masking). They see the
  entire input sequence at once (bidirectional context),
  making them ideal for understanding tasks like
  classification and sentiment analysis.
</p>
<p>
  <strong>Architecture</strong>: Stack of transformer blocks
  with bidirectional attention. Each token can attend to all
  other tokens (past and future).
</p>

<p>
  <strong>Training</strong>: Masked Language Modeling (MLM).
  Randomly mask 15% of tokens, predict them from
  bidirectional context:
</p>

<MathBlock
  formula={"p(x_i \\mid x_{<i}, x_{>i}) = \\text{softmax}(W_{\\text{vocab}} \\cdot h_i)"}
  display={true}
/>

<p>
  <strong>The <GlossaryTooltip term="CLS" /> token</strong>: A special token
  prepended to sequences. Its final representation
  aggregates sequence-level information, used for
  classification tasks.
</p>

<p>
  <strong>Use cases</strong>: Sentence classification, named
  entity recognition, question answering (extractive). Not
  designed for generation.
</p>

<p>
  <strong>Key models</strong>: BERT, RoBERTa, ELECTRA
</p>

<h3>
  2. Decoder-Only (<GlossaryTooltip term="GPT" />-Style)
</h3>

<p>
  Decoder-only models use just the decoder stack (masked
  self-attention). They process tokens left-to-right,
  predicting the next token. This causal structure makes
  them perfect for text generation.
</p>
<p>
  <strong>Architecture</strong>: Stack of transformer blocks
  with causal (unidirectional) attention. Token at position
  i can only attend to positions ≤ i.
</p>

<p>
  <strong>Causal masking</strong> is implemented by adding a mask
  to attention scores before softmax:
</p>

<MathBlock
  formula={"\\text{mask}_{ij} = \\begin{cases} 0 & \\text{if } i \\geq j \\\\ -\\infty & \\text{if } i < j \\end{cases}"}
  display={true}
/>

<p>
  The mask is 0 for positions the token is allowed to see
  (current and past) and negative infinity for future
  positions. Adding this to the attention scores before
  softmax gives:
</p>

<MathBlock
  formula={"\\alpha_{ij} = \\text{softmax}(\\frac{q_i^T k_j}{\\sqrt{d_k}} + \\text{mask}_{ij})"}
  display={true}
/>

<p>
  The -∞ makes softmax output 0 for future tokens
  (preventing information leakage).
</p>

<p>
  <strong>Training</strong>: Next-token prediction (language
  modeling). Given prefix x<sub>1:t</sub>, predict x<sub
    >t+1</sub
  >:
</p>

<MathBlock
  formula={"\\mathcal{L} = -\\sum_t \\log p(x_t \\mid x_{<t})"}
  display={true}
/>

<p><strong>Why GPT-style won for LLMs</strong>:</p>
<ul>
  <li>Unified pretraining and generation objective</li>
  <li>Scales naturally with compute and data</li>
  <li>Simple to implement and train</li>
  <li>Enables autoregressive sampling for generation</li>
</ul>

<p>
  <strong>Key models</strong>: GPT-2, GPT-3, GPT-4, GPT-4o, Llama 3, Claude 3.5 Sonnet, Gemini
  1.5 Pro
</p>

<h3>
  3. Encoder-Decoder (<GlossaryTooltip term="T5" />-Style)
</h3>
<p>
  These models keep both stacks. The encoder processes the
  input bidirectionally, and the decoder generates output
  autoregressively. This is versatile for
  sequence-to-sequence tasks like translation and
  summarization.
</p>
<p>
  <strong>Architecture</strong>: Encoder (bidirectional)
  processes input; decoder (causal) generates output while
  cross-attending to encoder states.
</p>

<p>
  <strong>Cross-attention</strong>: Decoder queries attend
  to encoder keys/values:
</p>

<MathBlock
  formula={"\\text{CrossAttn} = \\text{Attention}(Q_{\\text{decoder}}, K_{\\text{encoder}}, V_{\\text{encoder}})"}
  display={true}
/>

<p>
  <strong>Use cases</strong>: Tasks with distinct
  input/output (translation, summarization). Encoder-decoder
  architectures shine when input ≠ output modality or
  structure.
</p>

<p><strong>Key models</strong>: T5, BART, mT5</p>

<div class="my-8 p-6 bg-[hsl(var(--card))] rounded-lg border border-[hsl(var(--border))]">
  <h3 class="text-lg font-semibold text-[hsl(var(--foreground))] mb-2">Attention Mask Patterns</h3>
  <p class="text-sm text-[hsl(var(--muted-foreground))] mb-6">
    Each grid shows which tokens can attend to which. Rows = the token computing attention (query), columns = tokens it can look at (keys). A colored cell means "allowed," a gray cell means "blocked."
  </p>

  <div class="grid grid-cols-1 sm:grid-cols-3 gap-8">
    <!-- Encoder-Only (BERT) -->
    <div>
      <div class="font-semibold text-sm mb-1 text-center">Encoder-Only (BERT)</div>
      <div class="text-xs text-[hsl(var(--muted-foreground))] mb-3 text-center">Every token sees every other token</div>
      <!-- Axis labels -->
      <div class="flex items-start gap-1">
        <div class="flex flex-col items-center justify-between w-6 pt-1" style="height: 148px;">
          <span class="text-[10px] text-[hsl(var(--muted-foreground))] -rotate-90 whitespace-nowrap">query →</span>
        </div>
        <div>
          <div class="grid grid-cols-4 gap-1">
            {
              [...Array(16)].map(() => (
                <div class="w-8 h-8 bg-[hsl(var(--diagram-indigo-solid))] rounded opacity-85" />
              ))
            }
          </div>
          <div class="text-[10px] text-[hsl(var(--muted-foreground))] text-center mt-1">key →</div>
        </div>
      </div>
    </div>

    <!-- Decoder-Only (GPT) -->
    <div>
      <div class="font-semibold text-sm mb-1 text-center">Decoder-Only (GPT)</div>
      <div class="text-xs text-[hsl(var(--muted-foreground))] mb-3 text-center">Each token only sees past + itself</div>
      <div class="flex items-start gap-1">
        <div class="flex flex-col items-center justify-between w-6 pt-1" style="height: 148px;">
          <span class="text-[10px] text-[hsl(var(--muted-foreground))] -rotate-90 whitespace-nowrap">query →</span>
        </div>
        <div>
          <div class="grid grid-cols-4 gap-1">
            {
              [...Array(16)].map((_, i) => {
                const row = Math.floor(i / 4);
                const col = i % 4;
                const masked = col > row;
                return (
                  <div class={`w-8 h-8 rounded ${masked ? "bg-[hsl(var(--muted))] border border-dashed border-[hsl(var(--border))]" : "bg-[hsl(var(--diagram-emerald-solid))] opacity-85"}`} />
                );
              })
            }
          </div>
          <div class="text-[10px] text-[hsl(var(--muted-foreground))] text-center mt-1">key →</div>
        </div>
      </div>
    </div>

    <!-- Encoder-Decoder (T5) -->
    <div>
      <div class="font-semibold text-sm mb-1 text-center">Encoder-Decoder (T5)</div>
      <div class="text-xs text-[hsl(var(--muted-foreground))] mb-3 text-center">Encoder: full, Decoder: causal</div>
      <div class="flex items-start gap-1">
        <div class="flex flex-col items-center w-6 pt-1" style="height: 148px;">
          <span class="text-[10px] text-[hsl(var(--muted-foreground))] -rotate-90 whitespace-nowrap">query →</span>
        </div>
        <div>
          <!-- Top half: encoder (bidirectional) -->
          <div class="grid grid-cols-4 gap-1">
            {
              [...Array(8)].map(() => (
                <div class="w-8 h-[15px] bg-[hsl(var(--diagram-indigo-solid))] rounded-sm opacity-85" />
              ))
            }
          </div>
          <div class="h-px bg-[hsl(var(--border))] my-1"></div>
          <!-- Bottom half: decoder (causal) -->
          <div class="grid grid-cols-4 gap-1">
            {
              [...Array(8)].map((_, i) => {
                const row = Math.floor(i / 4);
                const col = i % 4;
                const masked = col > row;
                return (
                  <div class={`w-8 h-[15px] rounded-sm ${masked ? "bg-[hsl(var(--muted))] border border-dashed border-[hsl(var(--border))]" : "bg-[hsl(var(--diagram-emerald-solid))] opacity-85"}`} />
                );
              })
            }
          </div>
          <div class="text-[10px] text-[hsl(var(--muted-foreground))] text-center mt-1">key →</div>
        </div>
      </div>
    </div>
  </div>

  <div class="flex flex-wrap gap-4 mt-5 pt-4 border-t border-[hsl(var(--border))] text-xs text-[hsl(var(--muted-foreground))]">
    <span class="flex items-center gap-1.5"><span class="inline-block w-3 h-3 bg-[hsl(var(--diagram-indigo-solid))] rounded-sm"></span> Bidirectional attention</span>
    <span class="flex items-center gap-1.5"><span class="inline-block w-3 h-3 bg-[hsl(var(--diagram-emerald-solid))] rounded-sm"></span> Causal attention</span>
    <span class="flex items-center gap-1.5"><span class="inline-block w-3 h-3 bg-[hsl(var(--muted))] rounded-sm border border-dashed border-[hsl(var(--border))]"></span> Masked (blocked)</span>
  </div>
</div>

<h2>KV-Cache: Making Generation Fast</h2>

<p>
  Autoregressive generation is sequential: generate token t,
  append to context, generate token t+1, etc. Naively, this
  requires recomputing attention for all previous tokens at
  each step: O(n²) operations.
</p>

<p>
  <strong>KV-cache optimization</strong>: Cache the key and
  value vectors for all previous tokens. At step t, only
  compute Q<sub>t</sub>, then reuse cached K<sub>1:t-1</sub
  >, V<sub>1:t-1</sub>:
</p>

<MathBlock
  formula={"\\text{Attention}_t(Q_t, [K_{1:t-1}; K_t], [V_{1:t-1}; V_t])"}
  display={true}
/>

<p>
  This reduces per-token generation from O(n²) to O(n),
  making real-time generation feasible.
</p>

<h3>Memory Implications</h3>

<p>
  For a model with h heads, d<sub>k</sub> = d/h, and L layers,
  storing KV-cache for n tokens requires:
</p>

<MathBlock
  formula={"2 \\times L \\times n \\times d \\times \\text{bytes\\_per\\_param}"}
  display={true}
/>

<p>
  For GPT-3 (96 layers, d=12288, float16), caching 2048
  tokens ≈ 4.7 GB. This limits batch size during generation.
</p>

<h2>The Context Window Problem (and Solutions)</h2>

<p>
  Self-attention is O(n²) in sequence length. Early models
  trained on 2k-8k tokens, but by 2026, context windows have
  expanded dramatically through architectural innovations.
</p>

<h3>Modern Context Windows (2026)</h3>
<p>
  Current frontier models support context windows of <strong
    >up to 1-2M tokens</strong
  > (Gemini 1.5 Pro at 1M, Claude 3.5 Sonnet at 200K) with context
  lengths continuing to grow. This was achieved through:
</p>
<ul>
  <li>
    <strong>FlashAttention-2</strong>: Memory-efficient
    attention achieving 230 TFLOPs/s on A100 GPUs
  </li>
  <li>
    <strong>Sub-quadratic alternatives</strong>: State-space
    models (Mamba, RWKV) achieve O(n) complexity, offering
    an alternative to full attention for long sequences
  </li>
  <li>
    <strong>Improved positional encodings</strong>: RoPE
    (Rotary Position Embedding) and ALiBi enable better
    length generalization
  </li>
</ul>

<h3>Earlier Approaches (Historical)</h3>
<p>
  <strong>Sparse Attention</strong>: Longformer and BigBird
  restricted attention to local windows or predefined
  patterns, reducing complexity to O(n) but losing full
  attention flexibility.
</p>

<p>
  <strong>Sliding Window Attention</strong>: Each token
  attends only to a fixed window (e.g., 512 tokens
  before/after). Used in models like MPT, CodeGen.
</p>

<h3>Alternatives: Retrieval-Augmented Generation</h3>
<p>
  Instead of extending context windows indefinitely,
  retrieve relevant chunks from a database and pass them as
  context. Still valuable for knowledge-intensive tasks even
  with 1M+ token windows (covered in RAG module).
</p>

<Quiz
  question="Why did decoder-only (GPT-style) become dominant for LLMs, despite encoder-only (BERT-style) achieving state-of-the-art on many NLU benchmarks?"
  quizId="decoder-dominance"
  options={[
    {
      id: "a",
      text: "Decoder-only models are faster to train",
      correct: false,
      explanation:
        "Training speed is similar. The key difference is in the training objective and generation capabilities.",
    },
    {
      id: "b",
      text: "Decoder-only unifies pretraining (next-token prediction) with generation, and scales better with compute",
      correct: true,
      explanation:
        "Correct! Decoder-only models use the same objective for pretraining and generation (next-token prediction). This simplifies the pipeline and scales naturally. As models grew larger, generation capabilities became more valuable than bidirectional encoding.",
    },
    {
      id: "c",
      text: "BERT-style models can't be fine-tuned for downstream tasks",
      correct: false,
      explanation:
        "BERT-style models fine-tune very well for classification and extraction tasks.",
    },
    {
      id: "d",
      text: "Causal attention is mathematically superior to bidirectional attention",
      correct: false,
      explanation:
        "Bidirectional attention can capture richer context. The advantage of causal attention is enabling generation.",
    },
  ]}
/>

<KeyTakeaway>
  <ul>
    <li>
      <strong>BERT (encoder-only)</strong> uses bidirectional
      attention and MLM training; excellent for understanding
      tasks but not generation
    </li>
    <li>
      <strong>GPT (decoder-only)</strong> uses causal attention
      and next-token prediction; unified pretraining/generation
      objective that scales well
    </li>
    <li>
      <strong>T5 (encoder-decoder)</strong> combines both; best
      for tasks with distinct input/output like translation
    </li>
    <li>
      <strong>KV-cache</strong> caches key/value vectors to reduce
      generation from O(n²) to O(n) per token, critical for real-time
      inference
    </li>
    <li>
      <strong>Context window limits</strong> arise from O(n²)
      attention; solutions include sparse attention, sliding windows,
      and retrieval augmentation
    </li>
  </ul>
</KeyTakeaway>

<h2>References</h2>

<PaperReference
  title="BERT: Pre-training of Deep Bidirectional Transformers"
  authors="Devlin et al."
  year="2018"
  url="https://arxiv.org/abs/1810.04805"
  type="paper"
/>

<PaperReference
  title="Improving Language Understanding by Generative Pre-Training (GPT)"
  authors="Radford et al."
  year="2018"
  url="https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf"
  type="paper"
/>

<PaperReference
  title="Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer (T5)"
  authors="Raffel et al."
  year="2019"
  url="https://arxiv.org/abs/1910.10683"
  type="paper"
/>

<PaperReference
  title="Longformer: The Long-Document Transformer"
  authors="Beltagy et al."
  year="2020"
  url="https://arxiv.org/abs/2004.05150"
  type="paper"
/>
