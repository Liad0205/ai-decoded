---
// Module 2, Lesson 2.4: Scaling Laws and Emergent Abilities
import KeyTakeaway from "../../components/KeyTakeaway.astro";
import PaperReference from "../../components/PaperReference.astro";
import MathBlock from "../../components/MathBlock.astro";
import Quiz from "../../components/Quiz.astro";
import Diagram from "../../components/Diagram.astro";
import RevealSection from "../../components/RevealSection.astro";
import GlossaryTooltip from "../../components/GlossaryTooltip.astro";
---

<h2>Learning Objectives</h2>
<p>After completing this lesson, you will be able to:</p>
<ul>
  <li>
    Understand Chinchilla scaling laws and compute-optimal
    training
  </li>
  <li>
    Analyze power law relationships between loss and
    compute/data/parameters
  </li>
  <li>
    Evaluate the debate around emergent abilities in large
    models
  </li>
  <li>
    Explain Mixture of Experts (<GlossaryTooltip
      term="MoE"
    />) and how it enables efficient scaling
  </li>
</ul>

<h2>Scaling Laws: The Predictable Side of AI Progress</h2>

<p>
  Here's something surprising: even though individual AI breakthroughs can feel unpredictable, the overall relationship between "how much you invest" and "how good the model gets" follows remarkably clean mathematical patterns. If you double your compute budget, you can predict almost exactly how much better the model will be. This discovery transformed AI from "train it and hope" into something closer to engineering, where companies can forecast capabilities before spending millions on a training run.
</p>

<h3>The Original Scaling Laws (Kaplan et al., 2020)</h3>

<p>
  OpenAI found that test loss (how wrong the model is) decreases in a predictable pattern as you increase three things: model size, training data, and compute. Specifically, loss follows a <strong>power law</strong> with each:
</p>

<div class="equation-sequence">
  <MathBlock
    formula={"L(N) \\propto N^{-\\alpha_N}"}
    display={true}
  />
  <MathBlock
    formula={"L(D) \\propto D^{-\\alpha_D}"}
    display={true}
  />
  <MathBlock
    formula={"L(C) \\propto C^{-\\alpha_C}"}
    display={true}
  />
</div>

<p>
  where N = number of parameters (model size), D = dataset size (tokens seen during training), and C = compute (total floating-point operations). The symbol ∝ means "proportional to," and the negative exponent means loss goes <em>down</em> as the quantity goes <em>up</em>.
</p>

<p>
  What makes power laws special: if you plot these on a log-log scale (both axes logarithmic), they appear as straight lines. That means the relationship holds across huge ranges. A pattern observed going from 1M to 100M parameters still predicts what happens at 100B. This predictability is what turned billion-dollar training runs from a gamble into a calculated investment.
</p>

<p>
  <strong>Key finding</strong>: When not bottlenecked by the others, scaling any single factor (N, D, or C) improves loss predictably. But there's an optimal way to balance them, which brings us to Chinchilla.
</p>

<h3>Chinchilla Scaling Laws (Hoffmann et al., 2022)</h3>

<p>
  DeepMind revisited scaling laws and discovered something important: most existing models were <em>undertrained</em>. They were too big for the amount of data they saw. It's like building a massive factory but only running it for a day. The key insight: for a given compute budget, you get the best results when model size and training data scale together in balance.
</p>

<p>
  <strong>Chinchilla's rule of thumb</strong>: For every doubling of model size, double the training data too.
</p>

<p>Expressed mathematically, for a compute budget C (in FLOPs):</p>

<MathBlock
  formula={"N_{\\text{opt}} \\propto C^{0.5}, \\quad D_{\\text{opt}} \\propto C^{0.5}"}
  display={true}
/>

<p>
  In plain English: the optimal model size and the optimal amount of training data both grow as the square root of your compute budget. If you 4x your compute, you should 2x both model size and data (since √4 = 2). Spend it all on a bigger model? You'll get a big model that hasn't learned enough. Spend it all on more data with a small model? The model won't have the capacity to absorb it all.
</p>

<p>
  <strong>Impact</strong>: <GlossaryTooltip term="GPT" />-3 (175B params, 300B tokens)
  was overparameterized and undertrained. Chinchilla (70B
  params, 1.4T tokens) outperformed it despite being
  smaller, because it was compute-optimal.
</p>

<p>
  Many modern <GlossaryTooltip term="LLM" /> training runs follow Chinchilla-style
  guidance: more data, not just more parameters.
</p>

<h3>Practical Implications</h3>
<ul>
  <li>
    <strong>Data is the bottleneck</strong>: High-quality
    training data is scarce. We're approaching exhaustion of
    internet text.
  </li>
  <li>
    <strong>Smaller models, longer training</strong>:
    Inference costs favor smaller models trained longer
    (cheaper to serve).
  </li>
  <li>
    <strong>Predictable improvement</strong>: Companies can
    forecast ROI on compute investment with reasonable
    accuracy.
  </li>
</ul>

<h2>Power Law Exponents: What Do They Mean?</h2>

<p>
  The exponents in those power laws (typically α ≈ 0.05-0.10) tell you how steep the improvement curve is. A small exponent means you need a <em>lot</em> more resources for a modest improvement. For example:
</p>

<MathBlock
  formula={"L \\propto N^{-0.076} \\Rightarrow \\text{10× parameters} \\Rightarrow \\text{~40\\% loss reduction}"}
  display={true}
/>

<p>
  Read this as: if you make the model 10 times larger, you reduce loss by roughly 40%. That's a real improvement, but you needed 10x the parameters to get it. And the next 40% reduction? Another 10x on top of that. This is the nature of diminishing returns with power laws. But the returns are <em>predictable</em>, which is why frontier labs invest billions: they know exactly what they'll get.
</p>

<h2>Emergent Abilities: Magic or Measurement?</h2>

<p>
  As models got bigger, researchers noticed something curious: certain abilities seemed to appear out of nowhere. A model with 1B parameters can't do arithmetic, a model with 10B is shaky, and then a model with 100B just... does it. These sudden jumps are called <strong>emergent abilities</strong>: capabilities absent in smaller models that appear at larger scales. Other examples include few-shot learning (learning from just a few examples in the prompt) and chain-of-thought reasoning (solving problems step by step).
</p>

<h3>The Original Claim (Wei et al., 2022)</h3>

<p>
  GPT-3 exhibited few-shot learning: given a few examples in
  the prompt, it could generalize to new instances. This
  behavior was not seen in smaller models, suggesting
  qualitative phase transitions with scale.
</p>

<h3>The Debate: Are They Real?</h3>

<p>
  Schaeffer et al. (2023) made a compelling counterargument: emergent abilities might be <em>measurement artifacts</em>, not real phase transitions. The key insight is about how we measure:
</p>
<ul>
  <li>
    <strong>Accuracy is binary</strong>: a math problem is either right or wrong. If a model's internal probability of getting the answer right rises smoothly from 5% to 95% as it scales, accuracy will look flat at 0 for a while and then jump to 1. The improvement was gradual all along; the metric made it look sudden.
  </li>
  <li>
    <strong>Continuous metrics tell a different story</strong>: when researchers switched from accuracy to smoother metrics like token edit distance (how close the answer was), the "emergence" disappeared and scaling looked gradual.
  </li>
</ul>

<p>
  <strong>Counterargument</strong>: Some abilities (e.g.,
  in-context learning, instruction following) do require
  minimum model capacity. Below ~1B parameters, models
  simply can't learn these skills regardless of data.
</p>

<h3>Current Consensus View</h3>
<ul>
  <li>
    Scaling improves capabilities <em>predictably</em> on smooth
    metrics (loss, perplexity)
  </li>
  <li>
    On task-specific metrics (pass@1 on HumanEval,
    SWE-bench), improvements may appear sharp due to metric
    choice
  </li>
  <li>
    Some capabilities require minimum scale (architectural
    inductive biases + sufficient capacity)
  </li>
  <li>
    "Emergent" often means "we didn't measure finely enough
    at smaller scales"
  </li>
  <li>
    Modern focus has shifted to data quality and synthetic
    data generation as high-quality web text becomes scarce
  </li>
</ul>

<h2>
  Mixture of Experts (MoE): Scaling Without Proportional Compute
</h2>

<p>
  So far, scaling means bigger models with more compute. But there's a practical problem: a dense model with 1 trillion parameters needs to run <em>all</em> of those parameters for every single token. That's enormously expensive at inference time. <GlossaryTooltip term="MoE" /> offers an elegant solution: what if the model had a trillion parameters but only used a small fraction of them for each input?
</p>

<h3>How MoE Works</h3>

<p>
  Recall from the previous lesson that each transformer layer has a feed-forward network (FFN) that processes each token independently. In a standard (dense) model, every token passes through the same FFN. MoE replaces that single FFN with multiple smaller "expert" FFNs, and a gating network decides which experts each token should use:
</p>

<MathBlock
  formula={"\\text{FFN}_{\\text{MoE}}(x) = \\sum_{i=1}^N w_i(x) \\cdot \\text{Expert}_i(x)"}
  display={true}
/>

<p>
  In plain English: for each token x, compute a weight w<sub>i</sub> for each expert (how relevant is this expert for this input?), then combine the outputs. Think of it like a company with specialist teams: instead of sending every task to one giant department, you route each task to the two or three specialists best suited to handle it.
</p>

<p>
  In practice, the gating network selects only the top-K experts (typically K=2) and zeros out the rest. This means:
</p>
<ul>
  <li>
    <strong>Total capacity</strong>: N experts → N×
    parameters
  </li>
  <li>
    <strong>Activated capacity</strong>: Only K experts →
    K/N of total FLOPs
  </li>
</ul>

<h3>Example: Mixtral 8x7B</h3>

<p>
  Mixtral has 8 experts per layer, but only activates the top 2 for each token. The total parameter count is ~47B (all 8 experts combined), but for any given token, only ~13B parameters are active. So it has the <em>knowledge</em> of a 47B model with the <em>inference cost</em> of a 13B model. In practice, it matched the performance of much larger dense models while being significantly cheaper to run.
</p>

<h3>Challenges</h3>
<ul>
  <li>
    <strong>Load balancing</strong>: Left to its own devices, the gating network tends to send most tokens to just a few favorite experts while the rest sit idle. Training adds a special "auxiliary loss" that penalizes uneven expert usage, encouraging the model to spread the load.
  </li>
  <li>
    <strong>Communication overhead</strong>: In large-scale training, different experts live on different GPUs. Routing tokens to the right expert means sending data between devices, which adds latency.
  </li>
  <li>
    <strong>Training instability</strong>: The combination of sparse routing and load-balancing losses makes MoE models trickier to train than dense models, requiring careful hyperparameter tuning.
  </li>
</ul>

<h3>Key MoE Models</h3>
<ul>
  <li>
    <strong>Switch Transformer</strong> (Google, 2021): 1.6T params,
    activates 1 expert per token
  </li>
  <li>
    <strong>GLaM</strong> (Google, 2021): 1.2T params, outperforms
    GPT-3 with 1/3 the compute
  </li>
  <li>
    <strong>Mixtral 8x7B</strong> (Mistral, 2023): Open-weight
    model, SOTA performance at 13B active params
  </li>
</ul>

<Quiz
  quizId="scaling-laws-chinchilla"
  question="According to the Chinchilla scaling laws, if you double your compute budget, how should you optimally split the increase between model parameters and training data?"
  options={[
    {
      id: "a",
      text: "Put all the extra compute into a larger model",
      correct: false,
      explanation:
        "Chinchilla showed that just scaling model size is suboptimal. GPT-3 followed this approach and was undertrained relative to its size.",
    },
    {
      id: "b",
      text: "Scale both model size and training data equally",
      correct: true,
      explanation:
        "Correct! Hoffmann et al. (2022) showed that compute-optimal training requires scaling both model parameters and training tokens at roughly equal rates.",
    },
    {
      id: "c",
      text: "Put all the extra compute into more training data",
      correct: false,
      explanation:
        "While more data helps, only scaling data without increasing model capacity is also suboptimal.",
    },
    {
      id: "d",
      text: "The split doesn't matter as long as total compute increases",
      correct: false,
      explanation:
        "The split matters significantly! The Chinchilla paper showed that many existing models were significantly undertrained.",
    },
  ]}
/>

<Diagram
  diagramId="scaling-laws-viz"
  title="Scaling Law: Loss vs Compute (Log-Log Scale)"
  autoplay={true}
  animationDuration={4000}
>
  <div class="w-full">
    <svg viewBox="0 0 500 300" preserveAspectRatio="xMidYMid meet" class="w-full h-auto">
      <!-- Axes -->
      <line
        x1="60"
        y1="250"
        x2="460"
        y2="250"
        stroke="hsl(var(--muted-foreground))"
        stroke-width="2"></line>
      <line
        x1="60"
        y1="250"
        x2="60"
        y2="30"
        stroke="hsl(var(--muted-foreground))"
        stroke-width="2"></line>
      <!-- Axis labels -->
      <text
        x="260"
        y="290"
        text-anchor="middle"
        font-size="14"
        fill="hsl(var(--foreground))">Log(Compute)</text
      >
      <text
        x="20"
        y="140"
        text-anchor="middle"
        font-size="14"
        fill="hsl(var(--foreground))"
        transform="rotate(-90, 20, 140)">Log(Loss)</text
      >
      <!-- Subtitle -->
      <text
        x="260"
        y="20"
        text-anchor="middle"
        font-size="13"
        fill="hsl(var(--muted-foreground))"
        font-weight="bold"
        >Loss decreases as a power law with compute</text
      >
      <!-- Power law curve -->
      <path
        d="M 80 60 Q 150 100 200 150 Q 280 210 440 235"
        fill="none"
        stroke="hsl(var(--primary))"
        stroke-width="3"
        data-animate
        style="animation-delay: 0.3s"></path>
      <!-- Data points -->
      <circle
        cx="100"
        cy="80"
        r="5"
        fill="hsl(var(--primary))"
        data-animate
        style="animation-delay: 0.6s"></circle>
      <circle
        cx="150"
        cy="115"
        r="5"
        fill="hsl(var(--primary))"
        data-animate
        style="animation-delay: 0.9s"></circle>
      <circle
        cx="200"
        cy="150"
        r="5"
        fill="hsl(var(--primary))"
        data-animate
        style="animation-delay: 1.2s"></circle>
      <circle
        cx="280"
        cy="195"
        r="5"
        fill="hsl(var(--primary))"
        data-animate
        style="animation-delay: 1.5s"></circle>
      <circle
        cx="360"
        cy="220"
        r="5"
        fill="hsl(var(--primary))"
        data-animate
        style="animation-delay: 1.8s"></circle>
      <circle
        cx="440"
        cy="235"
        r="5"
        fill="hsl(var(--primary))"
        data-animate
        style="animation-delay: 2.1s"></circle>
      <!-- Annotations -->
      <text
        x="100"
        y="70"
        text-anchor="middle"
        font-size="11"
        fill="hsl(var(--muted-foreground))"
        data-animate
        style="animation-delay: 0.6s">Small</text
      >
      <text
        x="280"
        y="185"
        text-anchor="middle"
        font-size="11"
        fill="hsl(var(--muted-foreground))"
        data-animate
        style="animation-delay: 1.5s">GPT-3</text
      >
      <text
        x="440"
        y="225"
        text-anchor="end"
        font-size="11"
        fill="hsl(var(--muted-foreground))"
        data-animate
        style="animation-delay: 2.1s">Chinchilla</text
      >
    </svg>
  </div>
</Diagram>

<RevealSection
  revealId="gpt3-optimal"
  title="Was GPT-3 Trained Optimally?"
>
  <div data-reveal-step>
    <p class="font-semibold text-[hsl(var(--foreground))]">
      Step 1: GPT-3's actual allocation
    </p>
    <p class="text-[hsl(var(--muted-foreground))]">
      GPT-3 has 175B parameters and was trained on ~300B
      tokens. That's a ratio of roughly 1.7 tokens per
      parameter.
    </p>
  </div>
  <div data-reveal-step class="hidden">
    <p class="font-semibold text-[hsl(var(--foreground))]">
      Step 2: Chinchilla's recommendation
    </p>
    <p class="text-[hsl(var(--muted-foreground))]">
      The Chinchilla paper suggests the optimal ratio is
      approximately 20 tokens per parameter. For 175B
      parameters, that would mean ~3.5T tokens of training
      data.
    </p>
  </div>
  <div data-reveal-step class="hidden">
    <p class="font-semibold text-[hsl(var(--foreground))]">
      Step 3: The verdict
    </p>
    <p class="text-[hsl(var(--muted-foreground))]">
      GPT-3 was significantly undertrained: it saw roughly
      10x fewer tokens than optimal. The Chinchilla model
      (70B params, 1.4T tokens) matched GPT-3's performance
      with less than half the parameters by training on more
      data.
    </p>
  </div>
  <div data-reveal-step class="hidden">
    <p class="font-semibold text-[hsl(var(--foreground))]">
      Step 4: Impact on the field
    </p>
    <p class="text-[hsl(var(--muted-foreground))]">
      This insight reshaped <GlossaryTooltip term="LLM" /> training: LLaMA (65B, 1.4T
      tokens) and subsequent models adopted
      Chinchilla-optimal ratios, achieving better
      performance per parameter.
    </p>
  </div>
</RevealSection>

<KeyTakeaway>
  <ul>
    <li>
      <strong>Scaling laws</strong> show loss scales predictably
      with compute, data, and parameters via power laws
    </li>
    <li>
      <strong>Chinchilla's insight</strong>: compute-optimal
      training requires scaling parameters and data equally
      (N ∝ D ∝ C<sup>0.5</sup>)
    </li>
    <li>
      <strong>Emergent abilities</strong> debate: smooth improvements
      on continuous metrics vs sharp transitions on discrete metrics;
      minimum scale required for some capabilities
    </li>
    <li>
      <strong>MoE (Mixture of Experts)</strong> enables scaling to massive
      parameter counts while keeping per-token compute low
    </li>
    <li>
      <strong>The future</strong>: data quality and
      availability become primary bottlenecks as we approach
      compute-optimal frontier
    </li>
  </ul>
</KeyTakeaway>

<h2>References</h2>

<PaperReference
  title="Scaling Laws for Neural Language Models"
  authors="Kaplan et al."
  year="2020"
  url="https://arxiv.org/abs/2001.08361"
  type="paper"
/>

<PaperReference
  title="Training Compute-Optimal Large Language Models (Chinchilla)"
  authors="Hoffmann et al."
  year="2022"
  url="https://arxiv.org/abs/2203.15556"
  type="paper"
/>

<PaperReference
  title="Emergent Abilities of Large Language Models"
  authors="Wei et al."
  year="2022"
  url="https://arxiv.org/abs/2206.07682"
  type="paper"
/>

<PaperReference
  title="Are Emergent Abilities of Large Language Models a Mirage?"
  authors="Schaeffer et al."
  year="2023"
  url="https://arxiv.org/abs/2304.15004"
  type="paper"
/>

<PaperReference
  title="Switch Transformers: Scaling to Trillion Parameter Models"
  authors="Fedus et al."
  year="2021"
  url="https://arxiv.org/abs/2101.03961"
  type="paper"
/>

<PaperReference
  title="Mixtral of Experts"
  authors="Jiang et al."
  year="2024"
  url="https://arxiv.org/abs/2401.04088"
  type="paper"
/>
