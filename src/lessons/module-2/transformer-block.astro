---
// Module 2, Lesson 2.2: The Full Transformer Block
import KeyTakeaway from "../../components/KeyTakeaway.astro";
import PaperReference from "../../components/PaperReference.astro";
import MathBlock from "../../components/MathBlock.astro";
import RevealSection from "../../components/RevealSection.astro";
import Quiz from "../../components/Quiz.astro";
import Diagram from "../../components/Diagram.astro";
import GlossaryTooltip from "../../components/GlossaryTooltip.astro";
---

<h2>Learning Objectives</h2>
<p>After completing this lesson, you will be able to:</p>
<ul>
  <li>
    Understand layer normalization and why Pre-LN
    outperforms Post-LN
  </li>
  <li>
    Explain the role of feed-forward networks as transformer
    "memory"
  </li>
  <li>
    Analyze residual connections from gradient flow and
    ensemble perspectives
  </li>
  <li>
    Compare positional encoding schemes (sinusoidal,
    learned, RoPE, ALiBi)
  </li>
</ul>

<h2>Layer Normalization: Pre-LN vs Post-LN</h2>

<p>
  As data flows through a deep network, the numbers at each layer can drift: some neurons output very large values, others very small. This makes the optimizer's job harder because the "terrain" it's navigating keeps shifting. <strong>Layer normalization</strong> fixes this by recentering and rescaling the activations at each layer, keeping them in a well-behaved range.
</p>

<p>
  The operation is straightforward: take the mean and standard deviation across the features of each token, normalize to zero mean and unit variance, then let the model learn a per-feature scale and shift:
</p>

<MathBlock
  formula={"\\text{LayerNorm}(x) = \\gamma \\odot \\frac{x - \\mu}{\\sigma + \\epsilon} + \\beta"}
  display={true}
/>

<p>
  Here γ (scale) and β (shift) are learned parameters that let the model undo the normalization for features where large or small values are actually useful. The ε is a tiny constant to prevent division by zero.
</p>

<h3>Post-LN (Original Transformer)</h3>
<p>
  The original "Attention Is All You Need" paper placed normalization <em>after</em> the sublayer and residual addition:
</p>

<MathBlock
  formula={"x' = \\text{LayerNorm}(x + \\text{Sublayer}(x))"}
  display={true}
/>

<p>
  In this setup, "Sublayer" means either the attention or the FFN (the two main operations inside a transformer block). First compute the sublayer output, add the residual skip, then normalize.
</p>

<h3>Pre-LN (Modern Standard)</h3>
<p>
  Modern transformers flip the order: normalize <em>before</em> each sublayer.
</p>

<MathBlock
  formula={"x' = x + \\text{Sublayer}(\\text{LayerNorm}(x))"}
  display={true}
/>

<p>
  The difference is subtle but important. In Pre-LN, the residual path carries raw activations with nothing in the way. During backpropagation, gradients can flow straight through the residual connection without being distorted by normalization. Think of it as a clean highway for gradient flow.
</p>

<p>
  <strong>Why Pre-LN won</strong>: Post-LN suffers from gradient instability in very deep networks and requires careful learning rate warmup to avoid divergence. Pre-LN eliminates these issues, making it practical to train models with 100+ layers. Nearly all modern transformers use Pre-LN.
</p>

<h2>Feed-Forward Network: The Transformer's Memory</h2>

<p>
  Attention lets tokens talk to each other, but it doesn't do much "thinking" about the information it gathers. That's the feed-forward network's job. After self-attention, each token independently passes through a small two-layer network:
</p>

<MathBlock
  formula={"\\text{FFN}(x) = \\text{GELU}(xW_1 + b_1)W_2 + b_2"}
  display={true}
/>

<p>
  The first layer expands the token's representation to a wider dimension (typically 4x), applies a <GlossaryTooltip term="GELU" /> nonlinearity, and the second layer projects it back down. Each token gets the same network applied independently. This is where the bulk of the transformer's parameters live, and research suggests these layers function as a kind of knowledge store (more on that below).
</p>

<p>
  Typically, <MathBlock
    formula={"W_1 \\in \\mathbb{R}^{d \\times 4d}"}
  /> (expands by 4x), and <MathBlock
    formula={"W_2 \\in \\mathbb{R}^{4d \\times d}"}
  /> (projects back).
</p>

<h3>Why 4× Expansion?</h3>
<ul>
  <li>
    <strong>Expressive capacity</strong>: The wider hidden
    layer provides model capacity. Most parameters in a
    transformer are in FFN weights, not attention.
  </li>
  <li>
    <strong>Interpretation as memory</strong>: Research
    suggests FFN layers store factual knowledge. The first
    layer <MathBlock formula="W_1" /> acts as a key-value lookup:
    if input matches a "key" pattern, activate corresponding neurons.
    Second layer <MathBlock formula="W_2" /> retrieves the "value."
  </li>
  <li>
    <strong>Empirical sweet spot</strong>: 4× expansion
    balances compute/memory vs performance. Larger ratios
    (8×, 16×) can help but hit diminishing returns.
  </li>
</ul>

<h2>Residual Connections: Why They're Critical</h2>

<p>
  Every sublayer (attention, FFN) has a residual connection:
</p>

<MathBlock
  formula={"x' = x + \\text{Sublayer}(x)"}
  display={true}
/>

<p>
  Intuition: the output is the sum of the input and whatever
  the sublayer computes. Even if the sublayer learns nothing
  useful, the input passes through unchanged.
</p>

<h3>Gradient Flow Perspective</h3>
<p>
  During backpropagation, gradients flow through both the
  sublayer and the residual path. The residual provides a
  "gradient highway", a direct path for gradients to flow
  backward without being multiplied through many weight
  matrices.
</p>

<p>
  For a network with L layers, gradients can travel directly
  from layer L to layer 1 via residuals, preventing
  vanishing gradients.
</p>

<h3>Ensemble Interpretation</h3>
<p>
  Here's a surprising way to think about residuals: at each layer, the data can either go through the sublayer <em>or</em> skip it via the residual. With L layers, that creates <MathBlock formula="2^L" /> possible paths through the network (each layer is either "used" or "skipped"). The network effectively acts like an ensemble of many shallow networks running simultaneously, which improves robustness.
</p>

<RevealSection
  revealId="transformer-block-walkthrough"
  title="Complete Forward Pass Through One Transformer Block"
>
  <div data-reveal-step>
    <h4>Step 1: Input</h4>
    <p>
      Start with input <MathBlock
        formula={"x \\in \\mathbb{R}^{n \\times d}"}
      /> (n tokens, d dimensions)
    </p>
    <button
      type="button"
      class="mt-2 px-3 py-1 bg-[hsl(var(--primary))] text-[hsl(var(--primary-foreground))] rounded text-sm"
      data-reveal-button="1">Next Step →</button
    >
  </div>

  <div data-reveal-step>
    <h4>Step 2: Pre-norm + Self-Attention</h4>
    <MathBlock
      formula={"x_{\\text{norm}} = \\text{LayerNorm}(x)"}
      display={true}
    />
    <MathBlock
      formula={"x_{\\text{attn}} = \\text{MultiHeadAttention}(x_{\\text{norm}}, x_{\\text{norm}}, x_{\\text{norm}})"}
      display={true}
    />
    <button
      type="button"
      class="mt-2 px-3 py-1 bg-[hsl(var(--primary))] text-[hsl(var(--primary-foreground))] rounded text-sm"
      data-reveal-button="2">Next Step →</button
    >
  </div>

  <div data-reveal-step>
    <h4>Step 3: Residual Connection</h4>
    <MathBlock
      formula={"x' = x + x_{\\text{attn}}"}
      display={true}
    />
    <p>
      Add attention output back to original input (residual
      skip connection)
    </p>
    <button
      type="button"
      class="mt-2 px-3 py-1 bg-[hsl(var(--primary))] text-[hsl(var(--primary-foreground))] rounded text-sm"
      data-reveal-button="3">Next Step →</button
    >
  </div>

  <div data-reveal-step>
    <h4>Step 4: Pre-norm + Feed-Forward</h4>
    <MathBlock
      formula={"x'_{\\text{norm}} = \\text{LayerNorm}(x')"}
      display={true}
    />
    <MathBlock
      formula={"x_{\\text{ffn}} = \\text{FFN}(x'_{\\text{norm}})"}
      display={true}
    />
    <button
      type="button"
      class="mt-2 px-3 py-1 bg-[hsl(var(--primary))] text-[hsl(var(--primary-foreground))] rounded text-sm"
      data-reveal-button="4">Next Step →</button
    >
  </div>

  <div data-reveal-step>
    <h4>Step 5: Final Residual</h4>
    <MathBlock
      formula={"x'' = x' + x_{\\text{ffn}}"}
      display={true}
    />
    <p class="text-[hsl(var(--diagram-emerald-fg))] font-medium">
      ✓ Output x'' passes to the next transformer block!
    </p>
  </div>
</RevealSection>

<h2>Positional Encoding: Injecting Sequence Order</h2>

<p>
  Transformers process all tokens in parallel, which means
  they have no inherent notion of word order. Without
  positional information, "the cat sat on the mat" and "the
  mat sat on the cat" would produce identical
  representations. Positional encoding injects position
  information into the model so that attention can
  distinguish token order.
</p>

<h3>Sinusoidal Positional Encoding (Original)</h3>
<p>
  The original transformer used sine and cosine waves at different frequencies. The idea is similar to how a clock encodes time: the seconds hand moves fast, the minutes hand moves slower, the hours hand slower still. By combining waves at many frequencies, each position gets a unique "fingerprint" that the model can learn to interpret:
</p>

<div class="equation-sequence">
  <MathBlock
    formula={"PE_{(pos, 2i)} = \\sin(pos / 10000^{2i/d})"}
    display={true}
  />
  <MathBlock
    formula={"PE_{(pos, 2i+1)} = \\cos(pos / 10000^{2i/d})"}
    display={true}
  />
</div>

<p>
  Each pair of dimensions uses a sine and cosine at the same frequency, with frequencies decreasing across dimensions. These vectors are added directly to the token embeddings before entering the transformer. Because the encoding is deterministic (no learned parameters), it can generalize to sequence lengths not seen during training, though in practice this extrapolation is limited.
</p>

<h3>Learned Positional Embeddings</h3>
<p>
  The simplest alternative: learn a separate embedding vector for each position, just like word embeddings but for positions. Used in <GlossaryTooltip term="BERT" /> and early <GlossaryTooltip term="GPT" /> models. The tradeoff is straightforward: more flexible than sinusoids, but the model has never seen positions beyond its maximum training length and can't generalize to them.
</p>

<h3>RoPE (Rotary Position Embedding)</h3>
<p>
  RoPE takes a different approach: instead of adding position information to the input, it rotates the query and key vectors by an angle proportional to their position. Two tokens that are 5 positions apart will always have the same relative rotation regardless of where they appear in the sequence. This naturally encodes relative distance and extrapolates better to longer sequences. Used in LLaMA and PaLM.
</p>

<h3>ALiBi (Attention with Linear Biases)</h3>
<p>
  ALiBi is the simplest scheme: don't encode positions in the embeddings at all. Instead, subtract a penalty from attention scores based on how far apart two tokens are. Nearby tokens get high scores, distant tokens get penalized. This requires no extra parameters and extrapolates well to longer sequences. Used in BLOOM and MPT.
</p>

<Quiz
  quizId="pre-ln-post-ln"
  question="Why did modern transformers switch from Post-LN (original) to Pre-LN architecture?"
  options={[
    {
      id: "a",
      text: "Pre-LN reduces the total number of parameters",
      correct: false,
      explanation:
        "Both architectures have the same number of parameters, the only difference is where LayerNorm is applied.",
    },
    {
      id: "b",
      text: "Pre-LN produces better final model quality",
      correct: false,
      explanation:
        "Post-LN can actually achieve slightly better quality when trained successfully. The issue is training stability, not final quality.",
    },
    {
      id: "c",
      text: "Pre-LN enables more stable gradient flow during training",
      correct: true,
      explanation:
        "Correct! In Pre-LN, the residual path carries unnormalized activations directly, creating a clean gradient highway. This prevents gradient explosion/vanishing and eliminates the need for careful learning rate warmup.",
    },
    {
      id: "d",
      text: "Pre-LN was required for multi-head attention to work correctly",
      correct: false,
      explanation:
        "Multi-head attention works identically in both configurations, the difference is purely about where normalization occurs relative to the sublayer.",
    },
  ]}
/>

<Diagram
  diagramId="transformer-block-arch"
  title="Pre-LN Transformer Block Architecture"
>
  <div class="w-full">
    <svg
      viewBox="0 0 400 520"
      class="w-full h-auto max-w-md mx-auto"
    >
      <!-- Input -->
      <rect
        x="130"
        y="10"
        width="140"
        height="35"
        rx="6"
        fill="#e0e7ff"
        stroke="#6366f1"
        stroke-width="1.5"></rect>
      <text
        x="200"
        y="32"
        text-anchor="middle"
        font-size="13"
        fill="#312e81"
        font-weight="bold">Input (B, T, d)</text
      >

      <!-- Arrow -->
      <line
        x1="200"
        y1="45"
        x2="200"
        y2="65"
        stroke="#94a3b8"
        stroke-width="1.5"
        marker-end="url(#arrow)"></line>

      <!-- LayerNorm 1 -->
      <rect
        x="130"
        y="65"
        width="140"
        height="30"
        rx="6"
        fill="#fef3c7"
        stroke="#f59e0b"
        stroke-width="1.5"></rect>
      <text
        x="200"
        y="85"
        text-anchor="middle"
        font-size="12"
        fill="#92400e">LayerNorm</text
      >

      <line
        x1="200"
        y1="95"
        x2="200"
        y2="115"
        stroke="#94a3b8"
        stroke-width="1.5"
        marker-end="url(#arrow)"></line>

      <!-- Multi-Head Attention -->
      <rect
        x="110"
        y="115"
        width="180"
        height="35"
        rx="6"
        fill="#dbeafe"
        stroke="#3b82f6"
        stroke-width="1.5"></rect>
      <text
        x="200"
        y="137"
        text-anchor="middle"
        font-size="12"
        fill="#1e40af"
        font-weight="bold">Multi-Head Attention</text
      >

      <line
        x1="200"
        y1="150"
        x2="200"
        y2="175"
        stroke="#94a3b8"
        stroke-width="1.5"
        marker-end="url(#arrow)"></line>

      <!-- Add (residual) -->
      <circle
        cx="200"
        cy="190"
        r="15"
        fill="#f0fdf4"
        stroke="#22c55e"
        stroke-width="1.5"></circle>
      <text
        x="200"
        y="195"
        text-anchor="middle"
        font-size="16"
        fill="#166534"
        font-weight="bold">+</text
      >
      <!-- Residual connection line -->
      <path
        d="M 80 30 L 80 190 L 185 190"
        fill="none"
        stroke="#22c55e"
        stroke-width="1.5"
        stroke-dasharray="5,3"></path>

      <line
        x1="200"
        y1="205"
        x2="200"
        y2="230"
        stroke="#94a3b8"
        stroke-width="1.5"
        marker-end="url(#arrow)"></line>

      <!-- LayerNorm 2 -->
      <rect
        x="130"
        y="230"
        width="140"
        height="30"
        rx="6"
        fill="#fef3c7"
        stroke="#f59e0b"
        stroke-width="1.5"></rect>
      <text
        x="200"
        y="250"
        text-anchor="middle"
        font-size="12"
        fill="#92400e">LayerNorm</text
      >

      <line
        x1="200"
        y1="260"
        x2="200"
        y2="280"
        stroke="#94a3b8"
        stroke-width="1.5"
        marker-end="url(#arrow)"></line>

      <!-- FFN -->
      <rect
        x="110"
        y="280"
        width="180"
        height="35"
        rx="6"
        fill="#fce7f3"
        stroke="#ec4899"
        stroke-width="1.5"></rect>
      <text
        x="200"
        y="302"
        text-anchor="middle"
        font-size="12"
        fill="#9d174d"
        font-weight="bold">Feed-Forward Network</text
      >

      <!-- FFN detail -->
      <text
        x="200"
        y="335"
        text-anchor="middle"
        font-size="10"
        fill="#64748b"
        >Linear(d→4d) → GELU → Linear(4d→d)</text
      >

      <line
        x1="200"
        y1="345"
        x2="200"
        y2="365"
        stroke="#94a3b8"
        stroke-width="1.5"
        marker-end="url(#arrow)"></line>

      <!-- Add (residual 2) -->
      <circle
        cx="200"
        cy="380"
        r="15"
        fill="#f0fdf4"
        stroke="#22c55e"
        stroke-width="1.5"></circle>
      <text
        x="200"
        y="385"
        text-anchor="middle"
        font-size="16"
        fill="#166534"
        font-weight="bold">+</text
      >
      <!-- Residual connection line 2 -->
      <path
        d="M 80 190 L 80 380 L 185 380"
        fill="none"
        stroke="#22c55e"
        stroke-width="1.5"
        stroke-dasharray="5,3"></path>

      <line
        x1="200"
        y1="395"
        x2="200"
        y2="420"
        stroke="#94a3b8"
        stroke-width="1.5"
        marker-end="url(#arrow)"></line>

      <!-- Output -->
      <rect
        x="130"
        y="420"
        width="140"
        height="35"
        rx="6"
        fill="#e0e7ff"
        stroke="#6366f1"
        stroke-width="1.5"></rect>
      <text
        x="200"
        y="442"
        text-anchor="middle"
        font-size="13"
        fill="#312e81"
        font-weight="bold">Output (B, T, d)</text
      >

      <!-- Tensor shape annotations -->
      <text x="310" y="137" font-size="10" fill="#64748b"
        >(B, T, d) → (B, T, d)</text
      >
      <text x="310" y="302" font-size="10" fill="#64748b"
        >(B, T, d) → (B, T, d)</text
      >

      <!-- Arrow marker definition -->
      <defs>
        <marker
          id="arrow"
          viewBox="0 0 10 10"
          refX="5"
          refY="5"
          markerWidth="6"
          markerHeight="6"
          orient="auto-start-auto"
        >
          <path d="M 0 0 L 10 5 L 0 10 z" fill="#94a3b8"
          ></path>
        </marker>
      </defs>
    </svg>
  </div>
</Diagram>

<KeyTakeaway>
  <ul>
    <li>
      <strong>Pre-LN</strong> (normalize before sublayers) is
      the modern standard, offering more stable training than
      Post-LN at scale
    </li>
    <li>
      <strong>Feed-forward networks</strong> typically expand
      by 4× and act as the transformer's "memory," storing factual
      knowledge
    </li>
    <li>
      <strong>Residual connections</strong> provide gradient highways
      and enable ensemble-like behavior across exponentially many
      paths
    </li>
    <li>
      <strong>Positional encoding</strong> injects sequence order;
      modern choices (RoPE, ALiBi) offer better length extrapolation
      than fixed sinusoids
    </li>
    <li>
      <strong>A complete transformer block</strong> combines:
      LayerNorm → Attention → Residual → LayerNorm → FFN → Residual
    </li>
  </ul>
</KeyTakeaway>

<h2>References</h2>

<PaperReference
  title="On Layer Normalization in the Transformer Architecture"
  authors="Xiong et al."
  year="2020"
  url="https://arxiv.org/abs/2002.04745"
  type="paper"
/>

<PaperReference
  title="RoFormer: Enhanced Transformer with Rotary Position Embedding"
  authors="Su et al."
  year="2021"
  url="https://arxiv.org/abs/2104.09864"
  type="paper"
/>

<PaperReference
  title="Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation"
  authors="Press et al."
  year="2022"
  url="https://arxiv.org/abs/2108.12409"
  type="paper"
/>
